# @references:
#   - .windsurfrules
#   - CODE_OF_CONDUCT.md
#   - MQP.md
#   - README.md
#   - ROADMAP.md
#   - CROSSREF_STANDARD.md

# scripts/maintenance/code_health/docstring_autofixer.py
"""
Automated Docstring Fixer for EGOS Project.

Applies high-confidence, simple fixes to docstring issues based on a
JSON report generated by docstring_checker.py.

Fixes Implemented:
- Moves existing module docstrings to line 0 if misplaced.
- Adds placeholder module docstrings if missing entirely.
- Adds placeholder 'Attributes:' and 'Methods:' sections to class docstrings
  if reported as missing by the checker.

Usage:
  python scripts/maintenance/code_health/docstring_autofixer.py --report-path <path_to_report.json> [--no-backup] [--dry-run]
"""

import argparse
import json
import logging
import sys
import shutil
import re
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional

# Basic logger setup (KoiosLogger integration can be added later if needed)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger("DocstringAutofixer")

# --- Helper Functions (Implemented) ---

def find_docstring(lines: List[str]) -> Tuple[Optional[int], Optional[int]]:
    """
    Finds the start and end lines (inclusive, 0-based) of the first 
    triple-quoted string block in the file, suitable for module/class/func docstrings.
    Returns (None, None) if no triple-quoted string is found before the first
    significant code line.
    """
    start_line, end_line = None, None
    in_docstring = False
    docstring_quotes = None

    for i, line in enumerate(lines):
        stripped_line = line.strip()

        # Check for comments or empty lines *before* the first potential docstring
        if start_line is None and (not stripped_line or stripped_line.startswith('#')):
            continue # Skip initial comments/empty lines

        if not in_docstring:
            # Check for start of triple-quoted string
            if stripped_line.startswith('"""') or stripped_line.startswith("'''"):
                # Make sure it's not part of a multiline f-string or similar edge case (basic check)
                if '=' not in lines[i].split('#')[0].split('"""')[0].split("'''")[0]:
                     start_line = i
                     docstring_quotes = stripped_line[:3]
                     # Check for single-line docstring
                     if len(stripped_line) > 3 and stripped_line.endswith(docstring_quotes):
                         end_line = i
                         return start_line, end_line
                     else:
                         in_docstring = True
            elif start_line is None and stripped_line:
                 # Found significant code/import before any docstring
                 return None, None
        else:
            # Check for end of the current docstring
            if stripped_line.endswith(docstring_quotes):
                end_line = i
                return start_line, end_line

    # If loop finishes, docstring might be unterminated or not found
    return start_line, end_line

def has_section(docstring_lines: List[str], section_name: str) -> bool:
    """
    Checks if a list of docstring lines already contains a specific section header.
    Example section_name: "Attributes", "Methods", "Args"
    Looks for pattern like '  Attributes:' (case-insensitive, ignores spaces).
    """
    # Pattern: Optional leading whitespace, section_name, optional whitespace, colon, optional trailing whitespace, end-of-line
    pattern = re.compile(r"^\s*" + re.escape(section_name) + r"\s*:\s*$", re.IGNORECASE)
    for line in docstring_lines:
        if pattern.match(line):
            return True
    return False

# --- Core Fixing Logic (Partially Implemented) ---

def apply_fixes_to_file(
    file_path: str,
    issues: List[Dict[str, Any]],
    dry_run: bool = False,
    backup: bool = True,
) -> bool:
    """
    Reads a file, applies fixes based on issues, and writes back changes.
    Returns True if the file was modified (or would be in dry run), False otherwise.
    """
    logger.info(f"Processing file: {file_path}")
    modified = False
    original_lines = []
    try:
        # Read with universal newline handling, keep ends for writelines
        with open(file_path, "r", encoding="utf-8", newline='') as f:
            original_lines = f.readlines()
    except Exception as e:
        logger.error(f"Error reading file {file_path}: {e}")
        return False

    # Use a list to build the new content to handle insertions/deletions easily
    new_lines = original_lines[:]
    lines_modified_flag = False # Track if changes actually happened
    line_offset_for_subsequent_fixes = 0 # Track shifts for *later* fixes in the same file

    # --- 1. Fix Module Docstring ---
    module_docstring_issue = next((iss for iss in issues if iss.get("element_name") == "module" and iss.get("issue_type") == "missing_docstring"), None)

    if module_docstring_issue:
        logger.debug(f"  Checking for module docstring issue...")
        start, end = find_docstring(new_lines) # Check current state of potentially modified lines

        if start is not None and start > 0 and end is not None:
            # Found a docstring, but it's not at the top. Move it.
            logger.info(f"  Moving misplaced module docstring (lines {start+1}-{end+1}) to top.")
            docstring_block = new_lines[start : end + 1]

            # Remove from original position
            del new_lines[start : end + 1]

            # Insert at the beginning
            # Preserve shebang or encoding declarations if they exist
            insert_pos = 0
            if new_lines and new_lines[0].startswith('#!'):
                insert_pos = 1
                if insert_pos < len(new_lines) and new_lines[insert_pos].strip().startswith('# -*- coding:'):
                    insert_pos = 2
            elif new_lines and new_lines[0].strip().startswith('# -*- coding:'):
                insert_pos = 1

            # Insert the docstring block
            new_lines[insert_pos:insert_pos] = docstring_block
            lines_modified_flag = True
            # Offset calculation is complex due to moving blocks, 
            # better to re-evaluate positions for later fixes if needed.

        elif start is None:
            # No existing docstring found anywhere suitable, add placeholder at top.
            logger.info(f"  Adding placeholder module docstring.")
            filename = Path(file_path).name
            placeholder = f'"""TODO: Module docstring for {filename}"""\n'

            # Preserve shebang or encoding declarations if they exist
            insert_pos = 0
            if new_lines and new_lines[0].startswith('#!'):
                insert_pos = 1
                if insert_pos < len(new_lines) and new_lines[insert_pos].strip().startswith('# -*- coding:'):
                    insert_pos = 2
            elif new_lines and new_lines[0].strip().startswith('# -*- coding:'):
                insert_pos = 1

            # Add extra newline if inserting before existing code/imports that are not comments/empty
            if insert_pos < len(new_lines) and new_lines[insert_pos].strip() and not new_lines[insert_pos].strip().startswith('#'):
                 placeholder += '\n'
            elif insert_pos == len(new_lines) and insert_pos > 0: # Handle adding to end of file with only comments
                placeholder = '\n' + placeholder

            new_lines.insert(insert_pos, placeholder)
            lines_modified_flag = True
            line_offset_for_subsequent_fixes += 1 # Added one line
            if '\n' in placeholder.strip(): # Added two lines effectively if placeholder had newline
                line_offset_for_subsequent_fixes +=1
        else:
             # start == 0, docstring exists at top
             logger.debug(f"  Module docstring already exists at the top (lines {start+1}-{end+1}). Skipping fix.")

    # --- 2. Fix Incomplete Class Docstrings ---
    class_issues = [iss for iss in issues if iss.get("element_name", "").startswith("class ") and iss.get("issue_type") == "incomplete_docstring"]

    if class_issues:
         logger.debug(f"  Found {len(class_issues)} incomplete class docstring issues.")
         # Sort by line number to process top-down, applying offset cumulatively
         class_issues.sort(key=lambda x: x.get("line_number", 0))

         current_offset = line_offset_for_subsequent_fixes # Start with offset from module fix

         for issue in class_issues:
            reported_line_num = issue.get("line_number", 1) - 1 # 0-indexed
            # Apply the offset accumulated *so far* from previous fixes in this file
            actual_line_num = reported_line_num + current_offset 
            element_name = issue.get("element_name", "") # e.g., "class MyClass"
            description = issue.get("description", "") # e.g., "Class docstring is missing an Attributes section"

            logger.debug(f"    Processing issue: '{description}' for {element_name} near line {actual_line_num + 1}")

            # Find the class definition line (sanity check)
            if actual_line_num >= len(new_lines) or not new_lines[actual_line_num].strip().startswith("class "):
                logger.warning(f"    Could not find class definition near adjusted line {actual_line_num + 1}. Skipping fix. Line content: '{new_lines[actual_line_num].strip() if actual_line_num < len(new_lines) else 'OUT OF BOUNDS'}'")
                continue

            # Find the docstring for this class, searching *after* the class definition line
            class_doc_start_rel, class_doc_end_rel = find_docstring(new_lines[actual_line_num + 1:]) 

            if class_doc_start_rel is None or class_doc_end_rel is None:
                 logger.warning(f"    Could not find docstring immediately following class {element_name} definition at line {actual_line_num + 1}. Skipping fix.")
                 continue

            # Adjust docstring lines relative to the start of the file
            doc_start_abs = actual_line_num + 1 + class_doc_start_rel
            doc_end_abs = actual_line_num + 1 + class_doc_end_rel

            # Extract docstring lines including quotes
            docstring_lines_with_quotes = new_lines[doc_start_abs : doc_end_abs + 1]

            # Determine which section is missing
            section_to_add = None
            if "missing an Attributes section" in description and not has_section(docstring_lines_with_quotes, "Attributes"):
                section_to_add = "Attributes"
            elif "missing a Methods section" in description and not has_section(docstring_lines_with_quotes, "Methods"):
                 section_to_add = "Methods"
            # Add other sections like Args, Returns if needed later

            if section_to_add:
                logger.info(f"    Adding placeholder '{section_to_add}:' section to {element_name} docstring.")

                # Determine indentation (use indent of the line *before* closing quotes if possible)
                indent = "    " # Default indent
                if doc_end_abs > doc_start_abs: # Multiline docstring
                    # Try indent of line before closing quotes
                    prev_line_index = doc_end_abs - 1
                    match = re.match(r"^(\s+)", new_lines[prev_line_index])
                    if match:
                         indent = match.group(1)
                    else: # If line before is empty/no indent, try first line of docstring
                        match = re.match(r"^(\s+)", new_lines[doc_start_abs+1])
                        if match: indent = match.group(1)

                else: # Single line docstring - use indent of opening line + 4 spaces?
                     match = re.match(r"^(\s+)", new_lines[doc_start_abs])
                     if match:
                         indent = match.group(1) + "    "
                     # else: default '    ' is fine


                placeholder_section = f"\n{indent}{section_to_add}:\n{indent}    None\n" # Includes leading newline

                # Insert the placeholder *before* the closing triple quotes
                # Find the content of the line with closing quotes
                closing_quote_line = new_lines[doc_end_abs]
                closing_quote_marker = '"""' if '"""' in closing_quote_line else "'''"

                # Split the line at the closing quotes
                parts = closing_quote_line.split(closing_quote_marker, 1)
                if len(parts) == 2:
                     before_quotes = parts[0]
                     # Ensure placeholder starts on a new line relative to existing content
                     if before_quotes.strip(): # Content exists before quotes on the last line
                        placeholder_to_insert = placeholder_section
                     else: # Closing quotes are likely on their own line or just whitespace before
                         placeholder_to_insert = placeholder_section.lstrip('\n') # Remove leading newline if inserting on almost empty line

                     # Insert the placeholder before the quotes on that line
                     new_lines[doc_end_abs] = before_quotes + placeholder_to_insert + closing_quote_marker + parts[1]
                     lines_modified_flag = True

                     # Update offset for subsequent fixes in *this* file
                     # Count lines added by the placeholder
                     added_lines = placeholder_to_insert.strip('\n').count('\n') + 1
                     current_offset += added_lines
                     logger.debug(f"      Added {added_lines} lines. New offset: {current_offset}")

                else:
                     # This shouldn't happen if find_docstring worked correctly
                     logger.error(f"    Could not properly find closing quotes on line {doc_end_abs + 1} to insert section. Skipping fix for {element_name}.")

            else:
                 logger.debug(f"    Section mentioned in issue description ('{description}') seems to already exist or is not recognized. Skipping.")


    # Check if modifications were made by comparing lists
    if lines_modified_flag and new_lines != original_lines:
        modified = True
        if dry_run:
            logger.info(f"[DRY RUN] Would modify {file_path}")
            # Optionally print diff here for dry run (using difflib)
            # import difflib
            # diff = difflib.unified_diff(original_lines, new_lines, fromfile='original', tofile='modified')
            # logger.debug("--- DIFF START ---")
            # for line in diff:
            #     logger.debug(line.rstrip())
            # logger.debug("--- DIFF END ---")
        else:
            if backup:
                try:
                    backup_path = Path(f"{file_path}.bak")
                    if backup_path.exists(): # Avoid overwriting previous backups easily
                        import datetime
                        timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
                        backup_path = Path(f"{file_path}.{timestamp}.bak")
                    shutil.copy2(file_path, backup_path)
                    logger.info(f"  Created backup: {backup_path}")
                except Exception as e:
                    logger.error(f"  Error creating backup for {file_path}: {e}")
                    return False # Safer to stop if backup fails

            try:
                with open(file_path, "w", encoding="utf-8", newline='') as f:
                     f.writelines(new_lines)
                logger.info(f"  Successfully modified {file_path}")
            except Exception as e:
                logger.error(f"  Error writing changes to {file_path}: {e}")
                # Consider restoring from backup if write fails
                return False
    elif lines_modified_flag:
         logger.warning(f"  Internal modification flag set, but lists are identical for {file_path}. Check logic.")
         modified = False # Reset if lists are same
    else:
         logger.info(f"  No changes applied to {file_path}")


    return modified

# --- Main Execution ---

def main():
    parser = argparse.ArgumentParser(description="Automated Docstring Fixer for EGOS.")
    parser.add_argument(
        "--report-path",
        required=True,
        help="Path to the JSON report from docstring_checker.py",
    )
    parser.add_argument(
        "--no-backup",
        action="store_true",
        help="Do not create backup files before modifying.",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Report changes without modifying files.",
    )
    args = parser.parse_args()

    report_path = Path(args.report_path)
    do_backup = not args.no_backup
    dry_run = args.dry_run

    if not report_path.is_file():
        logger.error(f"Report file not found: {report_path}")
        sys.exit(1)

    logger.info(f"Loading report from: {report_path}")
    logger.info(f"Dry run: {dry_run}, Backup enabled: {do_backup}")

    try:
        with open(report_path, "r", encoding="utf-8") as f:
            report_data = json.load(f)
    except Exception as e:
        logger.error(f"Error loading report JSON: {e}")
        sys.exit(1)

    issues = report_data.get("issues", [])
    if not issues:
        logger.info("No issues found in the report.")
        sys.exit(0)

    logger.info(f"Found {len(issues)} total issues in the report.")

    # Group issues by file
    issues_by_file: Dict[str, List[Dict[str, Any]]] = {}
    for issue in issues:
        file_path = issue.get("file_path")
        if file_path:
            if file_path not in issues_by_file:
                issues_by_file[file_path] = []
            issues_by_file[file_path].append(issue)

    logger.info(f"Grouped issues for {len(issues_by_file)} unique files.")

    files_processed = 0
    files_modified = 0
    for file_path, file_issues in issues_by_file.items():
        files_processed += 1
        if apply_fixes_to_file(file_path, file_issues, dry_run, do_backup):
            files_modified += 1

    logger.info(f"\n--- Autofixer Summary ---")
    logger.info(f"Files Processed: {files_processed}")
    logger.info(f"Files Modified {'' if dry_run else '(or would be modified)'}: {files_modified}")
    logger.info("Script finished.")

if __name__ == "__main__":
    main()