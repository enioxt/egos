# @references:
#   - .windsurfrules
#   - CODE_OF_CONDUCT.md
#   - MQP.md
#   - README.md
#   - ROADMAP.md
#   - CROSSREF_STANDARD.md

[
  {
    "File": "C:\\EGOS\\WORK_2025_05_21.md",
    "LineNumber": 157,
    "LineContent": "            *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, `Related:`, `Doc:`, `Link to:`)."
  },
  {
    "File": "C:\\EGOS\\docs\\core_materials\\historical_changelogs\\PLANO DE NEGOCIO EVA (1).txt",
    "LineNumber": 714,
    "LineContent": "- Backend ReDoc: http://localhost:8000/redoc"
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\cross_reference_inventory_20250521.md",
    "LineNumber": 76,
    "LineContent": "  - Backend ReDoc: http://localhost:8000/redoc"
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_Doc_results.json",
    "LineNumber": 5,
    "LineContent": "    \"LineContent\": \"*   **[MVP-DEFINE-SPARC-SVC]** **Define MVP - SPARC Service:** Define MVP for a 'SPARC-based Project Analysis & Refactoring Service', outlining core features, target outcomes, required subsystems, and potential delivery model. (`HIGH`) `Status: Planned`, `linked_doc: docs_egos/strategy/MVP_Definition.md`\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_EGOS_ID_Start_results.json",
    "LineNumber": 145,
    "LineContent": "    \"LineContent\": \"    \\\"LineContent\\\": \\\"      \\\\\\\"diff\\\\\\\": \\\\\\\"--- a/generate_documentation_graph.py\\\\\\\\n+++ b/generate_documentation_graph.py\\\\\\\\n@@ -1,10 +1,10 @@\\\\\\\\n-<!-- \\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n @references:\\\\\\\\n - Core References:\\\\\\\\n-  - [MQP.md](mdc:../../MQP.md) - Master Quantum Prompt defining EGOS principles\\\\\\\\n-  - [ROADMAP.md](mdc:../../ROADMAP.md) - Project roadmap and planning\\\\\\\\n+- [MQP.md](mdc:../../MQP.md) - Master Quantum Prompt defining EGOS principles\\\\\\\\n+- [ROADMAP.md](mdc:../../ROADMAP.md) - Project roadmap and planning\\\\\\\\n - Process Documentation:\\\\\\\\n-  - [system_maintenance.md](mdc:../../docs_egos/process/system_maintenance.md)\\\\\\\\n+- [system_maintenance.md](mdc:../../docs_egos/process/system_maintenance.md)\\\\\\\\n \\\\\\\\n \\\\\\\\n \\\\\\\\n@@ -42,809 +42,810 @@\\\\\\\\n \\\\\\\\n # Import Rich components for progress bars\\\\\\\\n from rich.progress import (\\\\\\\\n-    Progress,\\\\\\\\n-    SpinnerColumn,\\\\\\\\n-    TextColumn,\\\\\\\\n-    BarColumn,\\\\\\\\n-    TimeRemainingColumn,\\\\\\\\n-    TaskID,\\\\\\\\n+Progress,\\\\\\\\n+SpinnerColumn,\\\\\\\\n+TextColumn,\\\\\\\\n+BarColumn,\\\\\\\\n+TimeRemainingColumn,\\\\\\\\n+TaskID,\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Setup logging\\\\\\\\n logging.basicConfig(\\\\\\\\n-    level=logging.INFO,\\\\\\\\n-    format=\\\\\\\\\\\\\\\"%(message)s\\\\\\\\\\\\\\\", # Rich handler handles time/level\\\\\\\\n-    datefmt=\\\\\\\\\\\\\\\"[%X]\\\\\\\\\\\\\\\",\\\\\\\\n-    handlers=[logging.StreamHandler()]\\\\\\\\n+level=logging.INFO,\\\\\\\\n+format=\\\\\\\\\\\\\\\"%(message)s\\\\\\\\\\\\\\\", # Rich handler handles time/level\\\\\\\\n+datefmt=\\\\\\\\\\\\\\\"[%X]\\\\\\\\\\\\\\\",\\\\\\\\n+handlers=[logging.StreamHandler()]\\\\\\\\n )\\\\\\\\n \\\\\\\\n logger = logging.getLogger(__name__)\\\\\\\\n \\\\\\\\n # Optional imports with fallbacks\\\\\\\\n try:\\\\\\\\n-    from pyvis.network import Network\\\\\\\\n-    PYVIS_AVAILABLE = True\\\\\\\\n+from pyvis.network import Network\\\\\\\\n+PYVIS_AVAILABLE = True\\\\\\\\n except ImportError:\\\\\\\\n-    logger.warning(\\\\\\\\\\\\\\\"Pyvis not installed. Will use simplified visualization.\\\\\\\\\\\\\\\")\\\\\\\\n-    PYVIS_AVAILABLE = False\\\\\\\\n+logger.warning(\\\\\\\\\\\\\\\"Pyvis not installed. Will use simplified visualization.\\\\\\\\\\\\\\\")\\\\\\\\n+PYVIS_AVAILABLE = False\\\\\\\\n \\\\\\\\n # NetworkX for basic/static visualization\\\\\\\\n try:\\\\\\\\n-    import networkx as nx\\\\\\\\n-    import matplotlib.pyplot as plt\\\\\\\\n-    NETWORKX_AVAILABLE = True\\\\\\\\n+import networkx as nx\\\\\\\\n+import matplotlib.pyplot as plt\\\\\\\\n+NETWORKX_AVAILABLE = True\\\\\\\\n except ImportError:\\\\\\\\n-    logger.warning(\\\\\\\\\\\\\\\"NetworkX not installed. Only JSON graph data will be generated.\\\\\\\\\\\\\\\")\\\\\\\\n-    NETWORKX_AVAILABLE = False\\\\\\\\n+logger.warning(\\\\\\\\\\\\\\\"NetworkX not installed. Only JSON graph data will be generated.\\\\\\\\\\\\\\\")\\\\\\\\n+NETWORKX_AVAILABLE = False\\\\\\\\n \\\\\\\\n \\\\\\\\n class DocumentationGraphGenerator:\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generates interactive visualizations of EGOS documentation connections.\\\\\\\\n-    \\\\\\\\n-    This class creates visual representations of the documentation cross-reference\\\\\\\\n-    network, helping identify key documents, clusters, and connection patterns.\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    \\\\\\\\n-    def __init__(self, base_path: str = \\\\\\\\\\\\\\\".\\\\\\\\\\\\\\\"):\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Initialize the graph generator.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            base_path: Base path of the EGOS project\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        self.base_path = Path(base_path)\\\\\\\\n-        self.report_path = self.base_path / \\\\\\\\\\\\\\\"reports\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\"\\\\\\\\n-        self.visualization_path = self.report_path / \\\\\\\\\\\\\\\"visualizations\\\\\\\\\\\\\\\"\\\\\\\\n-        \\\\\\\\n-        # Ensure output directories exist\\\\\\\\n-        self.visualization_path.mkdir(parents=True, exist_ok=True)\\\\\\\\n-        \\\\\\\\n-        # Graph data\\\\\\\\n-        self.nodes = []\\\\\\\\n-        self.edges = []\\\\\\\\n-        self.subsystems = set()\\\\\\\\n-        \\\\\\\\n-        # Node attributes\\\\\\\\n-        self.node_sizes = {}\\\\\\\\n-        self.node_colors = {}\\\\\\\\n-        self.node_clusters = {}\\\\\\\\n-        \\\\\\\\n-        # Color palette for subsystems with EGOS-aligned color scheme\\\\\\\\n-        self.color_palette = {\\\\\\\\n-            \\\\\\\\\\\\\\\"koios\\\\\\\\\\\\\\\": theme.color.koios,    # Blue - Knowledge\\\\\\\\n-            \\\\\\\\\\\\\\\"cronos\\\\\\\\\\\\\\\": theme.color.cronos,   # Purple - Time\\\\\\\\n-            \\\\\\\\\\\\\\\"nexus\\\\\\\\\\\\\\\": theme.color.nexus,    # Green - Connections\\\\\\\\n-            \\\\\\\\\\\\\\\"ethik\\\\\\\\\\\\\\\": theme.color.ethik,    # Red - Ethics\\\\\\\\n-            \\\\\\\\\\\\\\\"atlas\\\\\\\\\\\\\\\": theme.color.atlas,    # Orange - Mapping\\\\\\\\n-            \\\\\\\\\\\\\\\"mycelium\\\\\\\\\\\\\\\": theme.color.mycelium, # Teal - Integration\\\\\\\\n-            \\\\\\\\\\\\\\\"harmony\\\\\\\\\\\\\\\": theme.color.harmony,  # Dark Blue - Harmony\\\\\\\\n-            \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\": theme.color.default   # Gray - Default\\\\\\\\n-        }\\\\\\\\n-    \\\\\\\\n-    def load_report_data(self, report_path: Optional[str] = None) -> bool:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Load cross-reference report data from a JSON file.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            report_path: Path to the JSON report file (optional)\\\\\\\\n-            \\\\\\\\n-        Returns:\\\\\\\\n-            True if data was loaded successfully, False otherwise\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if report_path:\\\\\\\\n-            json_path = Path(report_path)\\\\\\\\n-        else:\\\\\\\\n-            # Look for the latest 'analysis' stage checkpoint file\\\\\\\\n-            checkpoint_dir = self.base_path / \\\\\\\\\\\\\\\"reports\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"checkpoints\\\\\\\\\\\\\\\"\\\\\\\\n-            analysis_checkpoints = list(checkpoint_dir.glob(\\\\\\\\\\\\\\\"xref_checkpoint_analysis_*.json\\\\\\\\\\\\\\\"))\\\\\\\\n-            \\\\\\\\n-            if not analysis_checkpoints:\\\\\\\\n-                # Fallback: Check for older format without timestamp\\\\\\\\n-                old_format_checkpoint = checkpoint_dir / \\\\\\\\\\\\\\\"xref_checkpoint_analysis.json\\\\\\\\\\\\\\\"\\\\\\\\n-                if old_format_checkpoint.exists():\\\\\\\\n-                    analysis_checkpoints.append(old_format_checkpoint)\\\\\\\\n-                else:\\\\\\\\n-                    logger.error(\\\\\\\\n-                        \\\\\\\\\\\\\\\"No analysis checkpoint found in reports/documentation/checkpoints/. \\\\\\\\\\\\\\\"\\\\\\\\n-                        \\\\\\\\\\\\\\\"Run add_cross_references.py --stage analysis first.\\\\\\\\\\\\\\\"\\\\\\\\n-                    )\\\\\\\\n-                    return False\\\\\\\\n-                    \\\\\\\\n-            # Get the most recent analysis checkpoint\\\\\\\\n-            json_path = max(analysis_checkpoints, key=os.path.getmtime)\\\\\\\\n-            logger.info(f\\\\\\\\\\\\\\\"Using latest analysis checkpoint: {json_path.name}\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Loading and processing graph data from: {json_path}\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        try:\\\\\\\\n-            with open(json_path, 'r', encoding='utf-8') as f:\\\\\\\\n-                checkpoint_content = json.load(f)\\\\\\\\n-            \\\\\\\\n-            # Extract the core data list\\\\\\\\n-            raw_data = None\\\\\\\\n-            if isinstance(checkpoint_content, dict) and \\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\" in checkpoint_content:\\\\\\\\n-                if isinstance(checkpoint_content[\\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\"], list):\\\\\\\\n-                     raw_data = checkpoint_content[\\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\"]\\\\\\\\n-            elif isinstance(checkpoint_content, list):\\\\\\\\n-                 raw_data = checkpoint_content # Assume direct list data\\\\\\\\n-                 \\\\\\\\n-            if raw_data is None:\\\\\\\\n-                logger.error(f\\\\\\\\\\\\\\\"Could not extract valid list data from checkpoint {json_path.name}\\\\\\\\\\\\\\\")\\\\\\\\n-                return False\\\\\\\\n-                \\\\\\\\n-            # Process the list data to build the required dictionary structure\\\\\\\\n-            processed_data = {\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\": {}, \\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\": {}}\\\\\\\\n-            all_docs = set()\\\\\\\\n-\\\\\\\\n-            # First pass: Build referenced_by and collect all docs\\\\\\\\n-            for item in raw_data:\\\\\\\\n-                # Expecting [doc_path, some_int, list_of_referencing_docs]\\\\\\\\n-                if len(item) == 3 and isinstance(item[0], str) and isinstance(item[2], list):\\\\\\\\n-                    doc = item[0]\\\\\\\\n-                    referencing_docs = item[2]\\\\\\\\n-                    all_docs.add(doc)\\\\\\\\n-                    processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][doc] = referencing_docs\\\\\\\\n-                    all_docs.update(referencing_docs) # Add referencing docs too\\\\\\\\n-                else:\\\\\\\\n-                     logger.warning(f\\\\\\\\\\\\\\\"Skipping malformed item in checkpoint data: {item}\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-            # Initialize all docs in references dict\\\\\\\\n-            for doc in all_docs:\\\\\\\\n-                # Ensure doc path is cleaned/normalized if needed (optional)\\\\\\\\n-                # doc_clean = Path(doc).as_posix() # Example normalization\\\\\\\\n-                processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][doc] = [] \\\\\\\\n-                # Ensure referenced_by also has all docs initialized, even if empty initially\\\\\\\\n-                if doc not in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"]:\\\\\\\\n-                    processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][doc] = []\\\\\\\\n-                \\\\\\\\n-            # Second pass: Build references from referenced_by\\\\\\\\n-            for doc, referencing_docs in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"].items():\\\\\\\\n-                for ref_doc in referencing_docs:\\\\\\\\n-                    if ref_doc in processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"]:\\\\\\\\n-                        processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][ref_doc].append(doc)\\\\\\\\n-                    else:\\\\\\\\n-                        # This case handles if a referencing doc wasn't in the initial list's first element\\\\\\\\n-                        # It means ref_doc references 'doc', but ref_doc itself might not have been a primary entry\\\\\\\\n-                        processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][ref_doc] = [doc]\\\\\\\\n-                        if ref_doc not in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"]:\\\\\\\\n-                           processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][ref_doc] = [] # Initialize if missing\\\\\\\\n-                        logger.debug(f\\\\\\\\\\\\\\\"Added '{ref_doc}' to structures during second pass.\\\\\\\\\\\\\\\")\\\\\\\\n-                        \\\\\\\\n-            self.report_data = processed_data\\\\\\\\n-            logger.info(f\\\\\\\\\\\\\\\"Successfully processed {len(self.report_data['references'])} documents from checkpoint.\\\\\\\\\\\\\\\")\\\\\\\\n-            return True\\\\\\\\n-                \\\\\\\\n-        except Exception as e:\\\\\\\\n-            logger.error(f\\\\\\\\\\\\\\\"Error loading and processing graph data: {str(e)}\\\\\\\\\\\\\\\")\\\\\\\\n-            import traceback # Added for better debugging\\\\\\\\n-            logger.error(traceback.format_exc()) # Added for better debugging\\\\\\\\n-            return False\\\\\\\\n-    \\\\\\\\n-    def build_graph(self) -> None:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Build the document graph from loaded report data.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not hasattr(self, 'report_data'):\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No report data loaded. Call load_report_data() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return\\\\\\\\n-            \\\\\\\\n-        references = self.report_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"]\\\\\\\\n-        referenced_by = self.report_data.get(\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\", {})\\\\\\\\n-        \\\\\\\\n-        logger.info(\\\\\\\\\\\\\\\"Building documentation graph...\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        # Create nodes for all documents\\\\\\\\n-        all_docs = set(references.keys())\\\\\\\\n-        for refs in references.values():\\\\\\\\n-            all_docs.update(refs)\\\\\\\\n-        \\\\\\\\n-        # Show progress during graph building\\\\\\\\n-        total_docs = len(all_docs)\\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Processing {total_docs} documents...\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        if console:\\\\\\\\n-            with Progress(\\\\\\\\n-                SpinnerColumn(),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"[progress.description]{task.description}\\\\\\\\\\\\\\\"),\\\\\\\\n-                BarColumn(),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"[progress.percentage]{task.percentage:>3.0f}%\\\\\\\\\\\\\\\"),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"({task.completed}/{task.total})\\\\\\\\\\\\\\\"),\\\\\\\\n-                TimeRemainingColumn(),\\\\\\\\n-                console=console,\\\\\\\\n-                transient=False,  # Keep progress visible after completion\\\\\\\\n-            ) as progress:\\\\\\\\n-                build_task = progress.add_task(\\\\\\\\\\\\\\\"Building graph nodes and edges...\\\\\\\\\\\\\\\", total=total_docs)\\\\\\\\n-                \\\\\\\\n-                # Process each document as a node\\\\\\\\n-                for i, doc in enumerate(all_docs):\\\\\\\\n-                    try: \\\\\\\\n-                        # Basic filtering for non-markdown files or complex paths\\\\\\\\n-                        if not doc.endswith(\\\\\\\\\\\\\\\".md\\\\\\\\\\\\\\\") or \\\\\\\\\\\\\\\"/\\\\\\\\\\\\\\\" in doc or r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\.\\\\\\\\\\\\\\\" in doc or \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\" in doc:\\\\\\\\n-                            logger.debug(f\\\\\\\\\\\\\\\"Skipping non-standard doc path: {doc}\\\\\\\\\\\\\\\")\\\\\\\\n-                            progress.update(build_task, advance=1)\\\\\\\\n-                            continue\\\\\\\\n-        \\\\\\\\n-                        # Extract subsystem from path\\\\\\\\n-                        subsystem = self._get_subsystem(doc)\\\\\\\\n-                        self.subsystems.add(subsystem)\\\\\\\\n-        \\\\\\\\n-                        # Add node with attributes\\\\\\\\n-                        node_id = doc\\\\\\\\n-                        node_label = os.path.basename(doc)\\\\\\\\n-                        # Ensure references and referenced_by are accessed safely\\\\\\\\n-                        doc_refs = references.get(doc, [])\\\\\\\\n-                        doc_referenced_by = referenced_by.get(doc, [])\\\\\\\\n-        \\\\\\\\n-                        node_title = self._generate_node_title(doc, doc_refs, doc_referenced_by) \\\\\\\\n-                        node_size = self._calculate_node_size(len(doc_refs), len(doc_referenced_by))\\\\\\\\n-                        node_color = self.color_palette.get(subsystem, self.color_palette[\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\"])\\\\\\\\n-        \\\\\\\\n-                        self.nodes.append({\\\\\\\\n-                            \\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\": node_id,\\\\\\\\n-                            \\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\": node_label,\\\\\\\\n-                            \\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\": node_title,\\\\\\\\n-                            \\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\": node_size,\\\\\\\\n-                            \\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\": node_color,\\\\\\\\n-                            \\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\": subsystem\\\\\\\\n-                        })\\\\\\\\n-        \\\\\\\\n-                        # Add edges (references from this doc to others)\\\\\\\\n-                        for target_doc in doc_refs:\\\\\\\\n-                            # Skip self-references or invalid targets\\\\\\\\n-                            if target_doc == doc or not target_doc.endswith(\\\\\\\\\\\\\\\".md\\\\\\\\\\\\\\\") or \\\\\\\\\\\\\\\"/\\\\\\\\\\\\\\\" in target_doc or r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\.\\\\\\\\\\\\\\\" in target_doc or \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\" in target_doc:\\\\\\\\n-                                continue\\\\\\\\n-                            self.edges.append({\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\": node_id, \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\": target_doc, \\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"})\\\\\\\\n-        \\\\\\\\n-                        # Update progress\\\\\\\\n-                        progress.update(build_task, advance=1)\\\\\\\\n-        \\\\\\\\n-                    except Exception as node_error:\\\\\\\\n-                        logger.error(f\\\\\\\\\\\\\\\"Error processing node '{doc}': {node_error}\\\\\\\\\\\\\\\")\\\\\\\\n-                        # Still advance progress even if one node fails\\\\\\\\n-                        progress.update(build_task, advance=1)\\\\\\\\n-                        continue\\\\\\\\n-        else:\\\\\\\\n-            # Fallback for when rich is not available\\\\\\\\n-            logger.info(\\\\\\\\\\\\\\\"Building graph nodes and edges (basic logging)...\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generates interactive visualizations of EGOS documentation connections.\\\\\\\\n+\\\\\\\\n+This class creates visual representations of the documentation cross-reference\\\\\\\\n+network, helping identify key documents, clusters, and connection patterns.\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+def __init__(self, base_path: str = \\\\\\\\\\\\\\\".\\\\\\\\\\\\\\\"):\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Initialize the graph generator.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+base_path: Base path of the EGOS project\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+self.base_path = Path(base_path)\\\\\\\\n+self.report_path = self.base_path / \\\\\\\\\\\\\\\"reports\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\"\\\\\\\\n+self.visualization_path = self.report_path / \\\\\\\\\\\\\\\"visualizations\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Ensure output directories exist\\\\\\\\n+self.visualization_path.mkdir(parents=True, exist_ok=True)\\\\\\\\n+\\\\\\\\n+# Graph data\\\\\\\\n+self.nodes = []\\\\\\\\n+self.edges = []\\\\\\\\n+self.subsystems = set()\\\\\\\\n+\\\\\\\\n+# Node attributes\\\\\\\\n+self.node_sizes = {}\\\\\\\\n+self.node_colors = {}\\\\\\\\n+self.node_clusters = {}\\\\\\\\n+\\\\\\\\n+# Color palette for subsystems with EGOS-aligned color scheme\\\\\\\\n+self.color_palette = {\\\\\\\\n+\\\\\\\\\\\\\\\"koios\\\\\\\\\\\\\\\": theme.color.koios,    # Blue - Knowledge\\\\\\\\n+\\\\\\\\\\\\\\\"cronos\\\\\\\\\\\\\\\": theme.color.cronos,   # Purple - Time\\\\\\\\n+\\\\\\\\\\\\\\\"nexus\\\\\\\\\\\\\\\": theme.color.nexus,    # Green - Connections\\\\\\\\n+\\\\\\\\\\\\\\\"ethik\\\\\\\\\\\\\\\": theme.color.ethik,    # Red - Ethics\\\\\\\\n+\\\\\\\\\\\\\\\"atlas\\\\\\\\\\\\\\\": theme.color.atlas,    # Orange - Mapping\\\\\\\\n+\\\\\\\\\\\\\\\"mycelium\\\\\\\\\\\\\\\": theme.color.mycelium, # Teal - Integration\\\\\\\\n+\\\\\\\\\\\\\\\"harmony\\\\\\\\\\\\\\\": theme.color.harmony,  # Dark Blue - Harmony\\\\\\\\n+\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\": theme.color.default   # Gray - Default\\\\\\\\n+}\\\\\\\\n+\\\\\\\\n+def load_report_data(self, report_path: Optional[str] = None) -> bool:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Load cross-reference report data from a JSON file.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+report_path: Path to the JSON report file (optional)\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+True if data was loaded successfully, False otherwise\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if report_path:\\\\\\\\n+json_path = Path(report_path)\\\\\\\\n+else:\\\\\\\\n+# Look for the latest 'analysis' stage checkpoint file\\\\\\\\n+checkpoint_dir = self.base_path / \\\\\\\\\\\\\\\"reports\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"checkpoints\\\\\\\\\\\\\\\"\\\\\\\\n+analysis_checkpoints = list(checkpoint_dir.glob(\\\\\\\\\\\\\\\"xref_checkpoint_analysis_*.json\\\\\\\\\\\\\\\"))\\\\\\\\n+\\\\\\\\n+if not analysis_checkpoints:\\\\\\\\n+# Fallback: Check for older format without timestamp\\\\\\\\n+old_format_checkpoint = checkpoint_dir / \\\\\\\\\\\\\\\"xref_checkpoint_analysis.json\\\\\\\\\\\\\\\"\\\\\\\\n+if old_format_checkpoint.exists():\\\\\\\\n+analysis_checkpoints.append(old_format_checkpoint)\\\\\\\\n+else:\\\\\\\\n+logger.error(\\\\\\\\n+\\\\\\\\\\\\\\\"No analysis checkpoint found in reports/documentation/checkpoints/. \\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\\\\\\\\"Run add_cross_references.py --stage analysis first.\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+return False\\\\\\\\n+\\\\\\\\n+# Get the most recent analysis checkpoint\\\\\\\\n+json_path = max(analysis_checkpoints, key=os.path.getmtime)\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Using latest analysis checkpoint: {json_path.name}\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Loading and processing graph data from: {json_path}\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+try:\\\\\\\\n+with open(json_path, 'r', encoding='utf-8') as f:\\\\\\\\n+checkpoint_content = json.load(f)\\\\\\\\n+\\\\\\\\n+# Extract the core data list\\\\\\\\n+raw_data = None\\\\\\\\n+if isinstance(checkpoint_content, dict) and \\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\" in checkpoint_content:\\\\\\\\n+if isinstance(checkpoint_content[\\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\"], list):\\\\\\\\n+raw_data = checkpoint_content[\\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\"]\\\\\\\\n+elif isinstance(checkpoint_content, list):\\\\\\\\n+raw_data = checkpoint_content # Assume direct list data\\\\\\\\n+\\\\\\\\n+if raw_data is None:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"Could not extract valid list data from checkpoint {json_path.name}\\\\\\\\\\\\\\\")\\\\\\\\n+return False\\\\\\\\n+\\\\\\\\n+# Process the list data to build the required dictionary structure\\\\\\\\n+processed_data = {\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\": {}, \\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\": {}}\\\\\\\\n+all_docs = set()\\\\\\\\n+\\\\\\\\n+# First pass: Build referenced_by and collect all docs\\\\\\\\n+for item in raw_data:\\\\\\\\n+# Expecting [doc_path, some_int, list_of_referencing_docs]\\\\\\\\n+if len(item) == 3 and isinstance(item[0], str) and isinstance(item[2], list):\\\\\\\\n+doc = item[0]\\\\\\\\n+referencing_docs = item[2]\\\\\\\\n+all_docs.add(doc)\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][doc] = referencing_docs\\\\\\\\n+all_docs.update(referencing_docs) # Add referencing docs too\\\\\\\\n+else:\\\\\\\\n+logger.warning(f\\\\\\\\\\\\\\\"Skipping malformed item in checkpoint data: {item}\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Initialize all docs in references dict\\\\\\\\n+for doc in all_docs:\\\\\\\\n+# Ensure doc path is cleaned/normalized if needed (optional)\\\\\\\\n+# doc_clean = Path(doc).as_posix() # Example normalization\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][doc] = []\\\\\\\\n+# Ensure referenced_by also has all docs initialized, even if empty initially\\\\\\\\n+if doc not in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"]:\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][doc] = []\\\\\\\\n+\\\\\\\\n+# Second pass: Build references from referenced_by\\\\\\\\n+for doc, referencing_docs in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"].items():\\\\\\\\n+for ref_doc in referencing_docs:\\\\\\\\n+if ref_doc in processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"]:\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][ref_doc].append(doc)\\\\\\\\n+else:\\\\\\\\n+# This case handles if a referencing doc wasn't in the initial list's first element\\\\\\\\n+# It means ref_doc references 'doc', but ref_doc itself might not have been a primary entry\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][ref_doc] = [doc]\\\\\\\\n+if ref_doc not in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"]:\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][ref_doc] = [] # Initialize if missing\\\\\\\\n+logger.debug(f\\\\\\\\\\\\\\\"Added '{ref_doc}' to structures during second pass.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+self.report_data = processed_data\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Successfully processed {len(self.report_data['references'])} documents from checkpoint.\\\\\\\\\\\\\\\")\\\\\\\\n+return True\\\\\\\\n+\\\\\\\\n+except Exception as e:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"Error loading and processing graph data: {str(e)}\\\\\\\\\\\\\\\")\\\\\\\\n+import traceback # Added for better debugging\\\\\\\\n+logger.error(traceback.format_exc()) # Added for better debugging\\\\\\\\n+return False\\\\\\\\n+\\\\\\\\n+def build_graph(self) -> None:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Build the document graph from loaded report data.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not hasattr(self, 'report_data'):\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No report data loaded. Call load_report_data() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return\\\\\\\\n+\\\\\\\\n+references = self.report_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"]\\\\\\\\n+referenced_by = self.report_data.get(\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\", {})\\\\\\\\n+\\\\\\\\n+logger.info(\\\\\\\\\\\\\\\"Building documentation graph...\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Create nodes for all documents\\\\\\\\n+all_docs = set(references.keys())\\\\\\\\n+for refs in references.values():\\\\\\\\n+all_docs.update(refs)\\\\\\\\n+\\\\\\\\n+# Show progress during graph building\\\\\\\\n+total_docs = len(all_docs)\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Processing {total_docs} documents...\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+if console:\\\\\\\\n+with Progress(\\\\\\\\n+SpinnerColumn(),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"[progress.description]{task.description}\\\\\\\\\\\\\\\"),\\\\\\\\n+BarColumn(),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"[progress.percentage]{task.percentage:>3.0f}%\\\\\\\\\\\\\\\"),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"({task.completed}/{task.total})\\\\\\\\\\\\\\\"),\\\\\\\\n+TimeRemainingColumn(),\\\\\\\\n+console=console,\\\\\\\\n+transient=False,  # Keep progress visible after completion\\\\\\\\n+) as progress:\\\\\\\\n+build_task = progress.add_task(\\\\\\\\\\\\\\\"Building graph nodes and edges...\\\\\\\\\\\\\\\", total=total_docs)\\\\\\\\n+\\\\\\\\n+# Process each document as a node\\\\\\\\n+for i, doc in enumerate(all_docs):\\\\\\\\n+try:\\\\\\\\n+# Basic filtering for non-markdown files or complex paths\\\\\\\\n+if not doc.endswith(\\\\\\\\\\\\\\\".md\\\\\\\\\\\\\\\") or \\\\\\\\\\\\\\\"/\\\\\\\\\\\\\\\" in doc or r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\.\\\\\\\\\\\\\\\" in doc or \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\" in doc:\\\\\\\\n+logger.debug(f\\\\\\\\\\\\\\\"Skipping non-standard doc path: {doc}\\\\\\\\\\\\\\\")\\\\\\\\n+progress.update(build_task, advance=1)\\\\\\\\n+continue\\\\\\\\n+\\\\\\\\n+# Extract subsystem from path\\\\\\\\n+subsystem = self._get_subsystem(doc)\\\\\\\\n+self.subsystems.add(subsystem)\\\\\\\\n+\\\\\\\\n+# Add node with attributes\\\\\\\\n+node_id = doc\\\\\\\\n+node_label = os.path.basename(doc)\\\\\\\\n+# Ensure references and referenced_by are accessed safely\\\\\\\\n+doc_refs = references.get(doc, [])\\\\\\\\n+doc_referenced_by = referenced_by.get(doc, [])\\\\\\\\n+\\\\\\\\n+node_title = self._generate_node_title(doc, doc_refs, doc_referenced_by)\\\\\\\\n+node_size = self._calculate_node_size(len(doc_refs), len(doc_referenced_by))\\\\\\\\n+node_color = self.color_palette.get(subsystem, self.color_palette[\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+self.nodes.append({\\\\\\\\n+\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\": node_id,\\\\\\\\n+\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\": node_label,\\\\\\\\n+\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\": node_title,\\\\\\\\n+\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\": node_size,\\\\\\\\n+\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\": node_color,\\\\\\\\n+\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\": subsystem\\\\\\\\n+})\\\\\\\\n+\\\\\\\\n+# Add edges (references from this doc to others)\\\\\\\\n+for target_doc in doc_refs:\\\\\\\\n+# Skip self-references or invalid targets\\\\\\\\n+if target_doc == doc or not target_doc.endswith(\\\\\\\\\\\\\\\".md\\\\\\\\\\\\\\\") or \\\\\\\\\\\\\\\"/\\\\\\\\\\\\\\\" in target_doc or r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\.\\\\\\\\\\\\\\\" in target_doc or \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\" in target_doc:\\\\\\\\n+continue\\\\\\\\n+self.edges.append({\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\": node_id, \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\": target_doc, \\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"})\\\\\\\\n+\\\\\\\\n+# Update progress\\\\\\\\n+progress.update(build_task, advance=1)\\\\\\\\n+\\\\\\\\n+except Exception as node_error:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"Error processing node '{doc}': {node_error}\\\\\\\\\\\\\\\")\\\\\\\\n+# Still advance progress even if one node fails\\\\\\\\n+progress.update(build_task, advance=1)\\\\\\\\n+continue\\\\\\\\n+else:\\\\\\\\n+# Fallback for when rich is not available\\\\\\\\n+logger.info(\\\\\\\\\\\\\\\"Building graph nodes and edges (basic logging)...\\\\\\\\\\\\\\\")\\\\\\\\n {{ ... }}\\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Graph built successfully: {len(self.nodes)} nodes, {len(self.edges)} edges.\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-    def generate_pyvis_visualization(self, output_path: Optional[str] = None) -> str:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generate an interactive HTML visualization using Pyvis.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            output_path: Path to save the HTML file (optional)\\\\\\\\n-            \\\\\\\\n-        Returns:\\\\\\\\n-            Path to the generated HTML file\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not hasattr(self, 'nodes') or not self.nodes:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No graph data to visualize. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Create network\\\\\\\\n-        timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n-        output_filename = f\\\\\\\\\\\\\\\"documentation_graph_{timestamp}.html\\\\\\\\\\\\\\\"\\\\\\\\n-        \\\\\\\\n-        if output_path:\\\\\\\\n-            output_file = Path(output_path)\\\\\\\\n-        else:\\\\\\\\n-            output_file = self.visualization_path / output_filename\\\\\\\\n-            \\\\\\\\n-        # Log the start of visualization generation\\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Generating interactive visualization ({len(self.nodes)} nodes, {len(self.edges)} edges)...\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        if console:\\\\\\\\n-            with Progress(\\\\\\\\n-                SpinnerColumn(),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"[progress.description]{task.description}\\\\\\\\\\\\\\\"),\\\\\\\\n-                BarColumn(),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"[progress.percentage]{task.percentage:>3.0f}%\\\\\\\\\\\\\\\"),\\\\\\\\n-                TimeRemainingColumn(),\\\\\\\\n-                console=console,\\\\\\\\n-                transient=False, # Keep progress visible after completion\\\\\\\\n-            ) as progress:\\\\\\\\n-                # Create network\\\\\\\\n-                net_task = progress.add_task(\\\\\\\\\\\\\\\"Creating network...\\\\\\\\\\\\\\\", total=1)\\\\\\\\n-                net = Network(\\\\\\\\n-                    height=\\\\\\\\\\\\\\\"800px\\\\\\\\\\\\\\\", \\\\\\\\n-                    width=\\\\\\\\\\\\\\\"100%\\\\\\\\\\\\\\\", \\\\\\\\n-                    bgcolor=theme.color.white, \\\\\\\\n-                    font_color=theme.color.text_primary,\\\\\\\\n-                    heading=f\\\\\\\\\\\\\\\"EGOS Documentation Network ({timestamp})\\\\\\\\\\\\\\\"\\\\\\\\n-                )\\\\\\\\n-                progress.update(net_task, completed=1)\\\\\\\\n-                \\\\\\\\n-                # Add nodes with progress\\\\\\\\n-                node_task = progress.add_task(\\\\\\\\\\\\\\\"Adding nodes...\\\\\\\\\\\\\\\", total=len(self.nodes))\\\\\\\\n-                for node in self.nodes:\\\\\\\\n-                    net.add_node(\\\\\\\\n-                        node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"], \\\\\\\\n-                        label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"], \\\\\\\\n-                        title=node[\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\"],\\\\\\\\n-                        color=node[\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\"],\\\\\\\\n-                        size=node[\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\"],\\\\\\\\n-                        group=node[\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\"]\\\\\\\\n-                    )\\\\\\\\n-                    progress.update(node_task, advance=1)\\\\\\\\n-                \\\\\\\\n-                # Add edges with progress\\\\\\\\n-                edge_task = progress.add_task(\\\\\\\\\\\\\\\"Adding edges...\\\\\\\\\\\\\\\", total=len(self.edges))\\\\\\\\n-                for edge in self.edges:\\\\\\\\n-                    net.add_edge(\\\\\\\\n-                        edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], \\\\\\\\n-                        edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"], \\\\\\\\n-                        arrows=edge.get(\\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"),\\\\\\\\n-                        title=edge.get(\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n-                    )\\\\\\\\n-                    progress.update(edge_task, advance=1)\\\\\\\\n-                \\\\\\\\n-                # Configure physics for better readability\\\\\\\\n-                save_task = progress.add_task(\\\\\\\\\\\\\\\"Configuring and saving...\\\\\\\\\\\\\\\", total=1)\\\\\\\\n-                net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\\\\\\\n-                net.set_options(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-                {\\\\\\\\n-                  \\\\\\\\\\\\\\\"physics\\\\\\\\\\\\\\\": {\\\\\\\\n-                    \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\": {\\\\\\\\n-                      \\\\\\\\\\\\\\\"gravitationalConstant\\\\\\\\\\\\\\\": -100,\\\\\\\\n-                      \\\\\\\\\\\\\\\"centralGravity\\\\\\\\\\\\\\\": 0.01,\\\\\\\\n-                      \\\\\\\\\\\\\\\"springLength\\\\\\\\\\\\\\\": 200,\\\\\\\\n-                      \\\\\\\\\\\\\\\"springConstant\\\\\\\\\\\\\\\": 0.08\\\\\\\\n-                    },\\\\\\\\n-                    \\\\\\\\\\\\\\\"solver\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\",\\\\\\\\n-                    \\\\\\\\\\\\\\\"stabilization\\\\\\\\\\\\\\\": {\\\\\\\\n-                      \\\\\\\\\\\\\\\"iterations\\\\\\\\\\\\\\\": 100\\\\\\\\n-                    }\\\\\\\\n-                  }\\\\\\\\n-                }\\\\\\\\n-                \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n-                \\\\\\\\n-                # Save the network\\\\\\\\n-                net.save_graph(str(output_file))\\\\\\\\n-                progress.update(save_task, completed=1)\\\\\\\\n-        else:\\\\\\\\n-            # Fallback for when rich is not available\\\\\\\\n-            logger.info(\\\\\\\\\\\\\\\"Generating visualization (basic logging)...\\\\\\\\\\\\\\\")\\\\\\\\n-            net = Network(\\\\\\\\n-                height=\\\\\\\\\\\\\\\"800px\\\\\\\\\\\\\\\", \\\\\\\\n-                width=\\\\\\\\\\\\\\\"100%\\\\\\\\\\\\\\\", \\\\\\\\n-                bgcolor=theme.color.white, \\\\\\\\n-                font_color=theme.color.text_primary,\\\\\\\\n-                heading=f\\\\\\\\\\\\\\\"EGOS Documentation Network ({timestamp})\\\\\\\\\\\\\\\"\\\\\\\\n-            )\\\\\\\\n-            \\\\\\\\n-            # Add nodes\\\\\\\\n-            for node in self.nodes:\\\\\\\\n-                net.add_node(\\\\\\\\n-                    node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"], \\\\\\\\n-                    label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"], \\\\\\\\n-                    title=node[\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\"],\\\\\\\\n-                    color=node[\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\"],\\\\\\\\n-                    size=node[\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\"],\\\\\\\\n-                    group=node[\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\"]\\\\\\\\n-                )\\\\\\\\n-            \\\\\\\\n-            # Add edges\\\\\\\\n-            for edge in self.edges:\\\\\\\\n-                net.add_edge(\\\\\\\\n-                    edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], \\\\\\\\n-                    edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"], \\\\\\\\n-                    arrows=edge.get(\\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"),\\\\\\\\n-                    title=edge.get(\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n-                )\\\\\\\\n-            \\\\\\\\n-            # Configure physics for better readability\\\\\\\\n-            net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\\\\\\\n-            net.set_options(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            {\\\\\\\\n-              \\\\\\\\\\\\\\\"physics\\\\\\\\\\\\\\\": {\\\\\\\\n-                \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\": {\\\\\\\\n-                  \\\\\\\\\\\\\\\"gravitationalConstant\\\\\\\\\\\\\\\": -100,\\\\\\\\n-                  \\\\\\\\\\\\\\\"centralGravity\\\\\\\\\\\\\\\": 0.01,\\\\\\\\n-                  \\\\\\\\\\\\\\\"springLength\\\\\\\\\\\\\\\": 200,\\\\\\\\n-                  \\\\\\\\\\\\\\\"springConstant\\\\\\\\\\\\\\\": 0.08\\\\\\\\n-                },\\\\\\\\n-                \\\\\\\\\\\\\\\"solver\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\",\\\\\\\\n-                \\\\\\\\\\\\\\\"stabilization\\\\\\\\\\\\\\\": {\\\\\\\\n-                  \\\\\\\\\\\\\\\"iterations\\\\\\\\\\\\\\\": 100\\\\\\\\n-                }\\\\\\\\n-              }\\\\\\\\n-            }\\\\\\\\n-            \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n-            \\\\\\\\n-            # Save the network\\\\\\\\n-            net.save_graph(str(output_file))\\\\\\\\n-            \\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Interactive visualization saved to: {output_file}\\\\\\\\\\\\\\\")\\\\\\\\n-        return str(output_file)\\\\\\\\n-    \\\\\\\\n-    def generate_networkx_visualization(self, output_path: Optional[str] = None) -> str:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generate a static visualization using NetworkX and matplotlib.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            output_path: Path to save the PNG file (optional)\\\\\\\\n-            \\\\\\\\n-        Returns:\\\\\\\\n-            Path to the generated PNG file\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not NETWORKX_AVAILABLE:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"NetworkX is not available. Install with: pip install networkx matplotlib\\\\\\\\\\\\\\\")\\\\\\\\n-            return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        if not self.nodes:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Create graph\\\\\\\\n-        G = nx.DiGraph()\\\\\\\\n-        \\\\\\\\n-        # Add nodes with attributes\\\\\\\\n-        for node in self.nodes:\\\\\\\\n-            G.add_node(\\\\\\\\n-                node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"],\\\\\\\\n-                label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"],\\\\\\\\n-                subsystem=node.get(\\\\\\\\\\\\\\\"subsystem\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\")\\\\\\\\n-            )\\\\\\\\n-            \\\\\\\\n-        # Add edges\\\\\\\\n-        for edge in self.edges:\\\\\\\\n-            G.add_edge(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n-            \\\\\\\\n-        # Generate output filename if not provided\\\\\\\\n-        if not output_path:\\\\\\\\n-            timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n-            output_path = self.visualization_path / f\\\\\\\\\\\\\\\"documentation_graph_{timestamp}.png\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Set up figure\\\\\\\\n-        plt.figure(figsize=(20, 20))\\\\\\\\n-        \\\\\\\\n-        # Use spring layout with seed for reproducibility\\\\\\\\n-        pos = nx.spring_layout(G, seed=42)\\\\\\\\n-        \\\\\\\\n-        # Node colors based on subsystem\\\\\\\\n-        node_colors = [self.node_colors.get(node, self.color_palette[\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\"]) for node in G.nodes()]\\\\\\\\n-        \\\\\\\\n-        # Node sizes\\\\\\\\n-        node_sizes = [self.node_sizes.get(node, 10) for node in G.nodes()]\\\\\\\\n-        \\\\\\\\n-        # Draw the graph\\\\\\\\n-        nx.draw_networkx(\\\\\\\\n-            G, \\\\\\\\n-            pos=pos,\\\\\\\\n-            with_labels=False,\\\\\\\\n-            node_color=node_colors,\\\\\\\\n-            node_size=node_sizes,\\\\\\\\n-            alpha=0.8,\\\\\\\\n-            arrows=True,\\\\\\\\n-            edge_color=theme.color.medium_grey\\\\\\\\n-        )\\\\\\\\n-        \\\\\\\\n-        # Draw labels for larger nodes only\\\\\\\\n-        large_nodes = {node: data for node, data in G.nodes(data=True) \\\\\\\\n-                      if self.node_sizes.get(node, 10) > 25}\\\\\\\\n-        if large_nodes:\\\\\\\\n-            nx.draw_networkx_labels(\\\\\\\\n-                G, \\\\\\\\n-                pos=pos,\\\\\\\\n-                labels={node: data[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"] for node, data in large_nodes.items()},\\\\\\\\n-                font_size=8\\\\\\\\n-            )\\\\\\\\n-            \\\\\\\\n-        # Save the visualization\\\\\\\\n-        plt.tight_layout()\\\\\\\\n-        plt.axis(\\\\\\\\\\\\\\\"off\\\\\\\\\\\\\\\")\\\\\\\\n-        plt.savefig(str(output_path), dpi=300, bbox_inches=\\\\\\\\\\\\\\\"tight\\\\\\\\\\\\\\\")\\\\\\\\n-        plt.close()\\\\\\\\n-        \\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Static visualization saved to: {output_path}\\\\\\\\\\\\\\\")\\\\\\\\n-        return str(output_path)\\\\\\\\n-    \\\\\\\\n-    def export_graph_data(self, output_path: Optional[str] = None) -> str:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Export graph data to JSON for use in custom visualizations.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            output_path: Path to save the JSON file (optional)\\\\\\\\n-            \\\\\\\\n-        Returns:\\\\\\\\n-            Path to the generated JSON file\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not self.nodes:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Create graph data structure\\\\\\\\n-        graph_data = {\\\\\\\\n-            \\\\\\\\\\\\\\\"nodes\\\\\\\\\\\\\\\": self.nodes,\\\\\\\\n-            \\\\\\\\\\\\\\\"edges\\\\\\\\\\\\\\\": self.edges,\\\\\\\\n-            \\\\\\\\\\\\\\\"metadata\\\\\\\\\\\\\\\": {\\\\\\\\n-                \\\\\\\\\\\\\\\"generated\\\\\\\\\\\\\\\": datetime.now().isoformat(),\\\\\\\\n-                \\\\\\\\\\\\\\\"subsystems\\\\\\\\\\\\\\\": list(sorted(self.subsystems)),\\\\\\\\n-                \\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(self.nodes),\\\\\\\\n-                \\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(self.edges)\\\\\\\\n-            }\\\\\\\\n-        }\\\\\\\\n-        \\\\\\\\n-        # Generate output filename if not provided\\\\\\\\n-        if not output_path:\\\\\\\\n-            timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n-            output_path = self.visualization_path / f\\\\\\\\\\\\\\\"documentation_graph_data_{timestamp}.json\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Save the graph data\\\\\\\\n-        with open(output_path, 'w', encoding='utf-8') as f:\\\\\\\\n-            json.dump(graph_data, f, indent=2)\\\\\\\\n-            \\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Graph data exported to: {output_path}\\\\\\\\\\\\\\\")\\\\\\\\n-        return str(output_path)\\\\\\\\n-    \\\\\\\\n-    def analyze_graph(self) -> Dict[str, Any]:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Analyze the graph structure and generate metrics.\\\\\\\\n-        \\\\\\\\n-        Returns:\\\\\\\\n-            Dictionary of graph metrics and insights\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not self.nodes or not self.edges:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return {}\\\\\\\\n-            \\\\\\\\n-        # If NetworkX is available, use it for analysis\\\\\\\\n-        if NETWORKX_AVAILABLE:\\\\\\\\n-            return self._analyze_with_networkx()\\\\\\\\n-        else:\\\\\\\\n-            return self._analyze_basic()\\\\\\\\n-    \\\\\\\\n-    def _analyze_with_networkx(self) -> Dict[str, Any]:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Analyze graph using NetworkX metrics.\\\\\\\\n-        \\\\\\\\n-        Returns:\\\\\\\\n-            Dictionary of graph metrics\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        # Create graph\\\\\\\\n-        G = nx.DiGraph()\\\\\\\\n-        \\\\\\\\n-        # Add nodes\\\\\\\\n-        for node in self.nodes:\\\\\\\\n-            G.add_node(node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"])\\\\\\\\n-            \\\\\\\\n-        # Add edges\\\\\\\\n-        for edge in self.edges:\\\\\\\\n-            G.add_edge(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n-            \\\\\\\\n-        # Calculate metrics\\\\\\\\n-        metrics = {\\\\\\\\n-            \\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(G),\\\\\\\\n-            \\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(G.edges()),\\\\\\\\n-            \\\\\\\\\\\\\\\"density\\\\\\\\\\\\\\\": nx.density(G),\\\\\\\\n-        }\\\\\\\\n-        \\\\\\\\n-        # Calculate centrality measures\\\\\\\\n-        try:\\\\\\\\n-            in_degree_centrality = nx.in_degree_centrality(G)\\\\\\\\n-            out_degree_centrality = nx.out_degree_centrality(G)\\\\\\\\n-            betweenness_centrality = nx.betweenness_centrality(G)\\\\\\\\n-            \\\\\\\\n-            # Sort by value to find highest centrality nodes\\\\\\\\n-            top_in_degree = sorted(in_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n-            top_out_degree = sorted(out_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n-            top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n-            \\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_in_degree]\\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"most_referencing_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_out_degree]\\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_betweenness]\\\\\\\\n-        except:\\\\\\\\n-            logger.warning(\\\\\\\\\\\\\\\"Error calculating centrality measures\\\\\\\\\\\\\\\")\\\\\\\\n-            \\\\\\\\n-        # Find isolated nodes (no connections)\\\\\\\\n-        isolated = list(nx.isolates(G))\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"] = isolated\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\"] = len(isolated)\\\\\\\\n-        \\\\\\\\n-        # Find strongly connected components\\\\\\\\n-        strongly_connected = list(nx.strongly_connected_components(G))\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"strongly_connected_components\\\\\\\\\\\\\\\"] = len(strongly_connected)\\\\\\\\n-        \\\\\\\\n-        # Find largest strongly connected component\\\\\\\\n-        if strongly_connected:\\\\\\\\n-            largest_component = max(strongly_connected, key=len)\\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"largest_component_size\\\\\\\\\\\\\\\"] = len(largest_component)\\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"largest_component_ratio\\\\\\\\\\\\\\\"] = len(largest_component) / len(G)\\\\\\\\n-        \\\\\\\\n-        return metrics\\\\\\\\n-    \\\\\\\\n-    def _analyze_basic(self) -> Dict[str, Any]:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Perform basic graph analysis without NetworkX.\\\\\\\\n-        \\\\\\\\n-        Returns:\\\\\\\\n-            Dictionary of basic graph metrics\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        # Count nodes and edges\\\\\\\\n-        metrics = {\\\\\\\\n-            \\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(self.nodes),\\\\\\\\n-            \\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(self.edges),\\\\\\\\n-        }\\\\\\\\n-        \\\\\\\\n-        # Count nodes by subsystem\\\\\\\\n-        subsystem_counts = {}\\\\\\\\n-        for node in self.nodes:\\\\\\\\n-            subsystem = node.get(\\\\\\\\\\\\\\\"subsystem\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\")\\\\\\\\n-            subsystem_counts[subsystem] = subsystem_counts.get(subsystem, 0) + 1\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"subsystem_counts\\\\\\\\\\\\\\\"] = subsystem_counts\\\\\\\\n-        \\\\\\\\n-        # Find nodes with most connections\\\\\\\\n-        node_connections = {}\\\\\\\\n-        for edge in self.edges:\\\\\\\\n-            source = edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"]\\\\\\\\n-            target = edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"]\\\\\\\\n-            \\\\\\\\n-            # Count outgoing connections\\\\\\\\n-            node_connections.setdefault(source, {\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\": 0, \\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\": 0})\\\\\\\\n-            node_connections[source][\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\"] += 1\\\\\\\\n-            \\\\\\\\n-            # Count incoming connections\\\\\\\\n-            node_connections.setdefault(target, {\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\": 0, \\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\": 0})\\\\\\\\n-            node_connections[target][\\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\"] += 1\\\\\\\\n-            \\\\\\\\n-        # Find most referenced documents\\\\\\\\n-        most_referenced = sorted(\\\\\\\\n-            [(node, data[\\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\"]) for node, data in node_connections.items()],\\\\\\\\n-            key=lambda x: x[1],\\\\\\\\n-            reverse=True\\\\\\\\n-        )[:10]\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"count\\\\\\\\\\\\\\\": count} for doc, count in most_referenced]\\\\\\\\n-        \\\\\\\\n-        # Find most referencing documents\\\\\\\\n-        most_referencing = sorted(\\\\\\\\n-            [(node, data[\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\"]) for node, data in node_connections.items()],\\\\\\\\n-            key=lambda x: x[1],\\\\\\\\n-            reverse=True\\\\\\\\n-        )[:10]\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"most_referencing_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"count\\\\\\\\\\\\\\\": count} for doc, count in most_referencing]\\\\\\\\n-        \\\\\\\\n-        # Find isolated documents (no connections)\\\\\\\\n-        connected_nodes = set()\\\\\\\\n-        for edge in self.edges:\\\\\\\\n-            connected_nodes.add(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"])\\\\\\\\n-            connected_nodes.add(edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n-            \\\\\\\\n-        all_node_ids = set(node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"] for node in self.nodes)\\\\\\\\n-        isolated = all_node_ids - connected_nodes\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"] = list(isolated)\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\"] = len(isolated)\\\\\\\\n-        \\\\\\\\n-        return metrics\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Graph built successfully: {len(self.nodes)} nodes, {len(self.edges)} edges.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+def generate_pyvis_visualization(self, output_path: Optional[str] = None) -> str:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generate an interactive HTML visualization using Pyvis.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+output_path: Path to save the HTML file (optional)\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Path to the generated HTML file\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not hasattr(self, 'nodes') or not self.nodes:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No graph data to visualize. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Create network\\\\\\\\n+timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n+output_filename = f\\\\\\\\\\\\\\\"documentation_graph_{timestamp}.html\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+if output_path:\\\\\\\\n+output_file = Path(output_path)\\\\\\\\n+else:\\\\\\\\n+output_file = self.visualization_path / output_filename\\\\\\\\n+\\\\\\\\n+# Log the start of visualization generation\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Generating interactive visualization ({len(self.nodes)} nodes, {len(self.edges)} edges)...\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+if console:\\\\\\\\n+with Progress(\\\\\\\\n+SpinnerColumn(),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"[progress.description]{task.description}\\\\\\\\\\\\\\\"),\\\\\\\\n+BarColumn(),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"[progress.percentage]{task.percentage:>3.0f}%\\\\\\\\\\\\\\\"),\\\\\\\\n+TimeRemainingColumn(),\\\\\\\\n+console=console,\\\\\\\\n+transient=False, # Keep progress visible after completion\\\\\\\\n+) as progress:\\\\\\\\n+# Create network\\\\\\\\n+net_task = progress.add_task(\\\\\\\\\\\\\\\"Creating network...\\\\\\\\\\\\\\\", total=1)\\\\\\\\n+net = Network(\\\\\\\\n+height=\\\\\\\\\\\\\\\"800px\\\\\\\\\\\\\\\",\\\\\\\\n+width=\\\\\\\\\\\\\\\"100%\\\\\\\\\\\\\\\",\\\\\\\\n+bgcolor=theme.color.white,\\\\\\\\n+font_color=theme.color.text_primary,\\\\\\\\n+heading=f\\\\\\\\\\\\\\\"EGOS Documentation Network ({timestamp})\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+progress.update(net_task, completed=1)\\\\\\\\n+\\\\\\\\n+# Add nodes with progress\\\\\\\\n+node_task = progress.add_task(\\\\\\\\\\\\\\\"Adding nodes...\\\\\\\\\\\\\\\", total=len(self.nodes))\\\\\\\\n+for node in self.nodes:\\\\\\\\n+net.add_node(\\\\\\\\n+node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"],\\\\\\\\n+label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"],\\\\\\\\n+title=node[\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\"],\\\\\\\\n+color=node[\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\"],\\\\\\\\n+size=node[\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\"],\\\\\\\\n+group=node[\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\"]\\\\\\\\n+)\\\\\\\\n+progress.update(node_task, advance=1)\\\\\\\\n+\\\\\\\\n+# Add edges with progress\\\\\\\\n+edge_task = progress.add_task(\\\\\\\\\\\\\\\"Adding edges...\\\\\\\\\\\\\\\", total=len(self.edges))\\\\\\\\n+for edge in self.edges:\\\\\\\\n+net.add_edge(\\\\\\\\n+edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"],\\\\\\\\n+edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"],\\\\\\\\n+arrows=edge.get(\\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"),\\\\\\\\n+title=edge.get(\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n+)\\\\\\\\n+progress.update(edge_task, advance=1)\\\\\\\\n+\\\\\\\\n+# Configure physics for better readability\\\\\\\\n+save_task = progress.add_task(\\\\\\\\\\\\\\\"Configuring and saving...\\\\\\\\\\\\\\\", total=1)\\\\\\\\n+net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\\\\\\\n+net.set_options(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+{\\\\\\\\n+\\\\\\\\\\\\\\\"physics\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"gravitationalConstant\\\\\\\\\\\\\\\": -100,\\\\\\\\n+\\\\\\\\\\\\\\\"centralGravity\\\\\\\\\\\\\\\": 0.01,\\\\\\\\n+\\\\\\\\\\\\\\\"springLength\\\\\\\\\\\\\\\": 200,\\\\\\\\n+\\\\\\\\\\\\\\\"springConstant\\\\\\\\\\\\\\\": 0.08\\\\\\\\n+},\\\\\\\\n+\\\\\\\\\\\\\\\"solver\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\",\\\\\\\\n+\\\\\\\\\\\\\\\"stabilization\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"iterations\\\\\\\\\\\\\\\": 100\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Save the network\\\\\\\\n+net.save_graph(str(output_file))\\\\\\\\n+progress.update(save_task, completed=1)\\\\\\\\n+else:\\\\\\\\n+# Fallback for when rich is not available\\\\\\\\n+logger.info(\\\\\\\\\\\\\\\"Generating visualization (basic logging)...\\\\\\\\\\\\\\\")\\\\\\\\n+net = Network(\\\\\\\\n+height=\\\\\\\\\\\\\\\"800px\\\\\\\\\\\\\\\",\\\\\\\\n+width=\\\\\\\\\\\\\\\"100%\\\\\\\\\\\\\\\",\\\\\\\\n+bgcolor=theme.color.white,\\\\\\\\n+font_color=theme.color.text_primary,\\\\\\\\n+heading=f\\\\\\\\\\\\\\\"EGOS Documentation Network ({timestamp})\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Add nodes\\\\\\\\n+for node in self.nodes:\\\\\\\\n+net.add_node(\\\\\\\\n+node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"],\\\\\\\\n+label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"],\\\\\\\\n+title=node[\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\"],\\\\\\\\n+color=node[\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\"],\\\\\\\\n+size=node[\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\"],\\\\\\\\n+group=node[\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\"]\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Add edges\\\\\\\\n+for edge in self.edges:\\\\\\\\n+net.add_edge(\\\\\\\\n+edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"],\\\\\\\\n+edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"],\\\\\\\\n+arrows=edge.get(\\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"),\\\\\\\\n+title=edge.get(\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Configure physics for better readability\\\\\\\\n+net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\\\\\\\n+net.set_options(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+{\\\\\\\\n+\\\\\\\\\\\\\\\"physics\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"gravitationalConstant\\\\\\\\\\\\\\\": -100,\\\\\\\\n+\\\\\\\\\\\\\\\"centralGravity\\\\\\\\\\\\\\\": 0.01,\\\\\\\\n+\\\\\\\\\\\\\\\"springLength\\\\\\\\\\\\\\\": 200,\\\\\\\\n+\\\\\\\\\\\\\\\"springConstant\\\\\\\\\\\\\\\": 0.08\\\\\\\\n+},\\\\\\\\n+\\\\\\\\\\\\\\\"solver\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\",\\\\\\\\n+\\\\\\\\\\\\\\\"stabilization\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"iterations\\\\\\\\\\\\\\\": 100\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Save the network\\\\\\\\n+net.save_graph(str(output_file))\\\\\\\\n+\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Interactive visualization saved to: {output_file}\\\\\\\\\\\\\\\")\\\\\\\\n+return str(output_file)\\\\\\\\n+\\\\\\\\n+def generate_networkx_visualization(self, output_path: Optional[str] = None) -> str:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generate a static visualization using NetworkX and matplotlib.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+output_path: Path to save the PNG file (optional)\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Path to the generated PNG file\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not NETWORKX_AVAILABLE:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"NetworkX is not available. Install with: pip install networkx matplotlib\\\\\\\\\\\\\\\")\\\\\\\\n+return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+if not self.nodes:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Create graph\\\\\\\\n+G = nx.DiGraph()\\\\\\\\n+\\\\\\\\n+# Add nodes with attributes\\\\\\\\n+for node in self.nodes:\\\\\\\\n+G.add_node(\\\\\\\\n+node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"],\\\\\\\\n+label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"],\\\\\\\\n+subsystem=node.get(\\\\\\\\\\\\\\\"subsystem\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\")\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Add edges\\\\\\\\n+for edge in self.edges:\\\\\\\\n+G.add_edge(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+# Generate output filename if not provided\\\\\\\\n+if not output_path:\\\\\\\\n+timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n+output_path = self.visualization_path / f\\\\\\\\\\\\\\\"documentation_graph_{timestamp}.png\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Set up figure\\\\\\\\n+plt.figure(figsize=(20, 20))\\\\\\\\n+\\\\\\\\n+# Use spring layout with seed for reproducibility\\\\\\\\n+pos = nx.spring_layout(G, seed=42)\\\\\\\\n+\\\\\\\\n+# Node colors based on subsystem\\\\\\\\n+node_colors = [self.node_colors.get(node, self.color_palette[\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\"]) for node in G.nodes()]\\\\\\\\n+\\\\\\\\n+# Node sizes\\\\\\\\n+node_sizes = [self.node_sizes.get(node, 10) for node in G.nodes()]\\\\\\\\n+\\\\\\\\n+# Draw the graph\\\\\\\\n+nx.draw_networkx(\\\\\\\\n+G,\\\\\\\\n+pos=pos,\\\\\\\\n+with_labels=False,\\\\\\\\n+node_color=node_colors,\\\\\\\\n+node_size=node_sizes,\\\\\\\\n+alpha=0.8,\\\\\\\\n+arrows=True,\\\\\\\\n+edge_color=theme.color.medium_grey\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Draw labels for larger nodes only\\\\\\\\n+large_nodes = {node: data for node, data in G.nodes(data=True)\\\\\\\\n+if self.node_sizes.get(node, 10) > 25}\\\\\\\\n+if large_nodes:\\\\\\\\n+nx.draw_networkx_labels(\\\\\\\\n+G,\\\\\\\\n+pos=pos,\\\\\\\\n+labels={node: data[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"] for node, data in large_nodes.items()},\\\\\\\\n+font_size=8\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Save the visualization\\\\\\\\n+plt.tight_layout()\\\\\\\\n+plt.axis(\\\\\\\\\\\\\\\"off\\\\\\\\\\\\\\\")\\\\\\\\n+plt.savefig(str(output_path), dpi=300, bbox_inches=\\\\\\\\\\\\\\\"tight\\\\\\\\\\\\\\\")\\\\\\\\n+plt.close()\\\\\\\\n+\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Static visualization saved to: {output_path}\\\\\\\\\\\\\\\")\\\\\\\\n+return str(output_path)\\\\\\\\n+\\\\\\\\n+def export_graph_data(self, output_path: Optional[str] = None) -> str:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Export graph data to JSON for use in custom visualizations.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+output_path: Path to save the JSON file (optional)\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Path to the generated JSON file\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not self.nodes:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Create graph data structure\\\\\\\\n+graph_data = {\\\\\\\\n+\\\\\\\\\\\\\\\"nodes\\\\\\\\\\\\\\\": self.nodes,\\\\\\\\n+\\\\\\\\\\\\\\\"edges\\\\\\\\\\\\\\\": self.edges,\\\\\\\\n+\\\\\\\\\\\\\\\"metadata\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"generated\\\\\\\\\\\\\\\": datetime.now().isoformat(),\\\\\\\\n+\\\\\\\\\\\\\\\"subsystems\\\\\\\\\\\\\\\": list(sorted(self.subsystems)),\\\\\\\\n+\\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(self.nodes),\\\\\\\\n+\\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(self.edges)\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+\\\\\\\\n+# Generate output filename if not provided\\\\\\\\n+if not output_path:\\\\\\\\n+timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n+output_path = self.visualization_path / f\\\\\\\\\\\\\\\"documentation_graph_data_{timestamp}.json\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Save the graph data\\\\\\\\n+with open(output_path, 'w', encoding='utf-8') as f:\\\\\\\\n+json.dump(graph_data, f, indent=2)\\\\\\\\n+\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Graph data exported to: {output_path}\\\\\\\\\\\\\\\")\\\\\\\\n+return str(output_path)\\\\\\\\n+\\\\\\\\n+def analyze_graph(self) -> Dict[str, Any]:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Analyze the graph structure and generate metrics.\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Dictionary of graph metrics and insights\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not self.nodes or not self.edges:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return {}\\\\\\\\n+\\\\\\\\n+# If NetworkX is available, use it for analysis\\\\\\\\n+if NETWORKX_AVAILABLE:\\\\\\\\n+return self._analyze_with_networkx()\\\\\\\\n+else:\\\\\\\\n+return self._analyze_basic()\\\\\\\\n+\\\\\\\\n+def _analyze_with_networkx(self) -> Dict[str, Any]:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Analyze graph using NetworkX metrics.\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Dictionary of graph metrics\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+# Create graph\\\\\\\\n+G = nx.DiGraph()\\\\\\\\n+\\\\\\\\n+# Add nodes\\\\\\\\n+for node in self.nodes:\\\\\\\\n+G.add_node(node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+# Add edges\\\\\\\\n+for edge in self.edges:\\\\\\\\n+G.add_edge(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+# Calculate metrics\\\\\\\\n+metrics = {\\\\\\\\n+\\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(G),\\\\\\\\n+\\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(G.edges()),\\\\\\\\n+\\\\\\\\\\\\\\\"density\\\\\\\\\\\\\\\": nx.density(G),\\\\\\\\n+}\\\\\\\\n+\\\\\\\\n+# Calculate centrality measures\\\\\\\\n+try:\\\\\\\\n+in_degree_centrality = nx.in_degree_centrality(G)\\\\\\\\n+out_degree_centrality = nx.out_degree_centrality(G)\\\\\\\\n+betweenness_centrality = nx.betweenness_centrality(G)\\\\\\\\n+\\\\\\\\n+# Sort by value to find highest centrality nodes\\\\\\\\n+top_in_degree = sorted(in_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n+top_out_degree = sorted(out_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n+top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n+\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_in_degree]\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"most_referencing_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_out_degree]\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_betweenness]\\\\\\\\n+except:\\\\\\\\n+logger.warning(\\\\\\\\\\\\\\\"Error calculating centrality measures\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Find isolated nodes (no connections)\\\\\\\\n+isolated = list(nx.isolates(G))\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"] = isolated\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\"] = len(isolated)\\\\\\\\n+\\\\\\\\n+# Find strongly connected components\\\\\\\\n+strongly_connected = list(nx.strongly_connected_components(G))\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"strongly_connected_components\\\\\\\\\\\\\\\"] = len(strongly_connected)\\\\\\\\n+\\\\\\\\n+# Find largest strongly connected component\\\\\\\\n+if strongly_connected:\\\\\\\\n+largest_component = max(strongly_connected, key=len)\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"largest_component_size\\\\\\\\\\\\\\\"] = len(largest_component)\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"largest_component_ratio\\\\\\\\\\\\\\\"] = len(largest_component) / len(G)\\\\\\\\n+\\\\\\\\n+return metrics\\\\\\\\n+\\\\\\\\n+def _analyze_basic(self) -> Dict[str, Any]:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Perform basic graph analysis without NetworkX.\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Dictionary of basic graph metrics\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+# Count nodes and edges\\\\\\\\n+metrics = {\\\\\\\\n+\\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(self.nodes),\\\\\\\\n+\\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(self.edges),\\\\\\\\n+}\\\\\\\\n+\\\\\\\\n+# Count nodes by subsystem\\\\\\\\n+subsystem_counts = {}\\\\\\\\n+for node in self.nodes:\\\\\\\\n+subsystem = node.get(\\\\\\\\\\\\\\\"subsystem\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\")\\\\\\\\n+subsystem_counts[subsystem] = subsystem_counts.get(subsystem, 0) + 1\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"subsystem_counts\\\\\\\\\\\\\\\"] = subsystem_counts\\\\\\\\n+\\\\\\\\n+# Find nodes with most connections\\\\\\\\n+node_connections = {}\\\\\\\\n+for edge in self.edges:\\\\\\\\n+source = edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"]\\\\\\\\n+target = edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"]\\\\\\\\n+\\\\\\\\n+# Count outgoing connections\\\\\\\\n+node_connections.setdefault(source, {\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\": 0, \\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\": 0})\\\\\\\\n+node_connections[source][\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\"] += 1\\\\\\\\n+\\\\\\\\n+# Count incoming connections\\\\\\\\n+node_connections.setdefault(target, {\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\": 0, \\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\": 0})\\\\\\\\n+node_connections[target][\\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\"] += 1\\\\\\\\n+\\\\\\\\n+# Find most referenced documents\\\\\\\\n+most_referenced = sorted(\\\\\\\\n+[(node, data[\\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\"]) for node, data in node_connections.items()],\\\\\\\\n+key=lambda x: x[1],\\\\\\\\n+reverse=True\\\\\\\\n+)[:10]\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"count\\\\\\\\\\\\\\\": count} for doc, count in most_referenced]\\\\\\\\n+\\\\\\\\n+# Find most referencing documents\\\\\\\\n+most_referencing = sorted(\\\\\\\\n+[(node, data[\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\"]) for node, data in node_connections.items()],\\\\\\\\n+key=lambda x: x[1],\\\\\\\\n+reverse=True\\\\\\\\n+)[:10]\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"most_referencing_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"count\\\\\\\\\\\\\\\": count} for doc, count in most_referencing]\\\\\\\\n+\\\\\\\\n+# Find isolated documents (no connections)\\\\\\\\n+connected_nodes = set()\\\\\\\\n+for edge in self.edges:\\\\\\\\n+connected_nodes.add(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"])\\\\\\\\n+connected_nodes.add(edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+all_node_ids = set(node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"] for node in self.nodes)\\\\\\\\n+isolated = all_node_ids - connected_nodes\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"] = list(isolated)\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\"] = len(isolated)\\\\\\\\n+\\\\\\\\n+return metrics\\\\\\\\n \\\\\\\\n \\\\\\\\n def main():\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Main entry point for the documentation graph generator.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    parser = argparse.ArgumentParser(\\\\\\\\n-        description=\\\\\\\\\\\\\\\"Generate interactive visualization of EGOS documentation connections\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--base-path\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-b\\\\\\\\\\\\\\\", default=\\\\\\\\\\\\\\\".\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Base path of the EGOS project\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--report\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-r\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Path to the cross-reference analysis JSON report\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--output\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-o\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Path to save the visualization\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--format\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-f\\\\\\\\\\\\\\\", choices=[\\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"], default=\\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Output format (default: all available formats)\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--analyze\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-a\\\\\\\\\\\\\\\", action=\\\\\\\\\\\\\\\"store_true\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Analyze graph and print metrics\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    \\\\\\\\n-    args = parser.parse_args()\\\\\\\\n-    \\\\\\\\n-    try:\\\\\\\\n-        # Create graph generator\\\\\\\\n-        generator = DocumentationGraphGenerator(args.base_path)\\\\\\\\n-        \\\\\\\\n-        # Load report data\\\\\\\\n-        if not generator.load_report_data(args.report):\\\\\\\\n-            return 1\\\\\\\\n-            \\\\\\\\n-        # --- Start of new try block for core logic ---\\\\\\\\n-        try:\\\\\\\\n-            # Build graph\\\\\\\\n-            generator.build_graph()\\\\\\\\n-            \\\\\\\\n-            # Generate visualizations based on format\\\\\\\\n-            if args.format in [\\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"] and PYVIS_AVAILABLE:\\\\\\\\n-                generator.generate_pyvis_visualization(\\\\\\\\n-                    args.output if args.format == \\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\" else None\\\\\\\\n-                )\\\\\\\\n-                \\\\\\\\n-            if args.format in [\\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"] and NETWORKX_AVAILABLE:\\\\\\\\n-                generator.generate_networkx_visualization(\\\\\\\\n-                    args.output if args.format == \\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\" else None\\\\\\\\n-                )\\\\\\\\n-                \\\\\\\\n-            if args.format in [\\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"]:\\\\\\\\n-                generator.export_graph_data(\\\\\\\\n-                    args.output if args.format == \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\" else None\\\\\\\\n-                )\\\\\\\\n-                \\\\\\\\n-            # Analyze graph if requested\\\\\\\\n-            if args.analyze:\\\\\\\\n-                analysis = generator.analyze_graph()\\\\\\\\n-                \\\\\\\\n-                # Print analysis results using rich tables if available\\\\\\\\n-                console.print(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n [bold cyan]DOCUMENTATION GRAPH ANALYSIS[/bold cyan]\\\\\\\\\\\\\\\")\\\\\\\\n-                \\\\\\\\n-                summary_table = Table(title=\\\\\\\\\\\\\\\"Graph Summary\\\\\\\\\\\\\\\", show_header=False, box=None)\\\\\\\\n-                summary_table.add_row(\\\\\\\\\\\\\\\"Nodes:\\\\\\\\\\\\\\\", str(analysis.get('node_count', 'N/A')))\\\\\\\\n-                summary_table.add_row(\\\\\\\\\\\\\\\"Edges:\\\\\\\\\\\\\\\", str(analysis.get('edge_count', 'N/A')))\\\\\\\\n-                if \\\\\\\\\\\\\\\"density\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n-                    summary_table.add_row(\\\\\\\\\\\\\\\"Graph Density:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"{analysis['density']:.4f}\\\\\\\\\\\\\\\")\\\\\\\\n-                if \\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n-                    summary_table.add_row(\\\\\\\\\\\\\\\"Isolated Documents:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"[yellow]{analysis['isolated_count']}[/yellow]\\\\\\\\\\\\\\\")\\\\\\\\n-                if \\\\\\\\\\\\\\\"strongly_connected_components\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n-                    summary_table.add_row(\\\\\\\\\\\\\\\"Strongly Connected Components:\\\\\\\\\\\\\\\", str(analysis['strongly_connected_components']))\\\\\\\\n-                if \\\\\\\\\\\\\\\"largest_component_size\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n-                    ratio = analysis.get('largest_component_ratio', 0)\\\\\\\\n-                    summary_table.add_row(\\\\\\\\\\\\\\\"Largest Component Size:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"{analysis['largest_component_size']} ({ratio:.1%})\\\\\\\\\\\\\\\")\\\\\\\\n-                console.print(summary_table)\\\\\\\\n-\\\\\\\\n-                # --- Most Referenced Documents Table ---\\\\\\\\n-                if \\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n-                    ref_table = Table(title=\\\\\\\\\\\\\\\" Most Referenced Documents (Top 5)\\\\\\\\\\\\\\\", expand=True)\\\\\\\\n-                    ref_table.add_column(\\\\\\\\\\\\\\\"Rank\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"dim\\\\\\\\\\\\\\\", width=5)\\\\\\\\n-                    ref_table.add_column(\\\\\\\\\\\\\\\"Document\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"magenta\\\\\\\\\\\\\\\")\\\\\\\\n-                    ref_table.add_column(\\\\\\\\\\\\\\\"Score/Count\\\\\\\\\\\\\\\", justify=\\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"green\\\\\\\\\\\\\\\")\\\\\\\\n-                    \\\\\\\\n-                    for i, doc_info in enumerate(analysis[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"][:5], 1):\\\\\\\\n-                        score = doc_info.get('score', doc_info.get('count', 'N/A'))\\\\\\\\n-                        score_str = f\\\\\\\\\\\\\\\"{score:.3f}\\\\\\\\\\\\\\\" if isinstance(score, float) else str(score)\\\\\\\\n-                        ref_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\\\\\\\n-                    console.print(ref_table)\\\\\\\\n-\\\\\\\\n-                # --- Bridge Documents Table ---\\\\\\\\n-                if \\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n-                    bridge_table = Table(title=\\\\\\\\\\\\\\\" Bridge Documents (High Betweenness - Top 5)\\\\\\\\\\\\\\\", expand=True)\\\\\\\\n-                    bridge_table.add_column(\\\\\\\\\\\\\\\"Rank\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"dim\\\\\\\\\\\\\\\", width=5)\\\\\\\\n-                    bridge_table.add_column(\\\\\\\\\\\\\\\"Document\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"cyan\\\\\\\\\\\\\\\")\\\\\\\\n-                    bridge_table.add_column(\\\\\\\\\\\\\\\"Score\\\\\\\\\\\\\\\", justify=\\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"blue\\\\\\\\\\\\\\\")\\\\\\\\n-                    \\\\\\\\n-                    for i, doc_info in enumerate(analysis[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"][:5], 1):\\\\\\\\n-                        score = doc_info.get('score', 'N/A')\\\\\\\\n-                        score_str = f\\\\\\\\\\\\\\\"{score:.3f}\\\\\\\\\\\\\\\" if isinstance(score, float) else str(score)\\\\\\\\n-                        bridge_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\\\\\\\n-                    console.print(bridge_table)\\\\\\\\n-\\\\\\\\n-                # --- Isolated Documents --- \\\\\\\\n-                if \\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n-                     console.print(f\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n [yellow]Isolated Documents ({analysis['isolated_count']}):[/yellow]\\\\\\\\\\\\\\\")\\\\\\\\n-                     # Print only first few for brevity\\\\\\\\n-                     for doc in analysis['isolated_docs'][:10]:\\\\\\\\n-                         console.print(f\\\\\\\\\\\\\\\"  - {os.path.basename(doc)}\\\\\\\\\\\\\\\")\\\\\\\\n-                     if len(analysis['isolated_docs']) > 10:\\\\\\\\n-                         console.print(f\\\\\\\\\\\\\\\"  ... and {len(analysis['isolated_docs']) - 10} more\\\\\\\\\\\\\\\")\\\\\\\\n-                \\\\\\\\n-                # Save analysis to JSON\\\\\\\\n-                analysis_path = generator.visualization_path / \\\\\\\\\\\\\\\"documentation_graph_analysis.json\\\\\\\\\\\\\\\"\\\\\\\\n-                with open(analysis_path, 'w', encoding='utf-8') as f:\\\\\\\\n-                    json.dump(analysis, f, indent=2)\\\\\\\\n-                logger.info(f\\\\\\\\\\\\\\\"Analysis saved to: {analysis_path}\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-        # --- End of new try block, add except block ---\\\\\\\\n-        except Exception as core_error:\\\\\\\\n-            logger.error(f\\\\\\\\\\\\\\\"!!! Critical Error during graph processing/visualization: {core_error}\\\\\\\\\\\\\\\")\\\\\\\\n-            logger.error(traceback.format_exc())\\\\\\\\n-            return 1 # Ensure exit with error code\\\\\\\\n-                \\\\\\\\n-        return 0\\\\\\\\n-            \\\\\\\\n-    # Outer except block remains for initialization/loading errors\\\\\\\\n-    except Exception as e:\\\\\\\\n-        logger.error(f\\\\\\\\\\\\\\\"Initialization or Loading Error: {str(e)}\\\\\\\\\\\\\\\")\\\\\\\\n-        # Ensure traceback is imported if needed here too\\\\\\\\n-        if 'traceback' not in sys.modules:\\\\\\\\n-            import traceback\\\\\\\\n-        logger.error(traceback.format_exc())\\\\\\\\n-        return 1\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Main entry point for the documentation graph generator.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+parser = argparse.ArgumentParser(\\\\\\\\n+description=\\\\\\\\\\\\\\\"Generate interactive visualization of EGOS documentation connections\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--base-path\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-b\\\\\\\\\\\\\\\", default=\\\\\\\\\\\\\\\".\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Base path of the EGOS project\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--report\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-r\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Path to the cross-reference analysis JSON report\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--output\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-o\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Path to save the visualization\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--format\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-f\\\\\\\\\\\\\\\", choices=[\\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"], default=\\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Output format (default: all available formats)\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--analyze\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-a\\\\\\\\\\\\\\\", action=\\\\\\\\\\\\\\\"store_true\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Analyze graph and print metrics\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+args = parser.parse_args()\\\\\\\\n+\\\\\\\\n+try:\\\\\\\\n+# Create graph generator\\\\\\\\n+generator = DocumentationGraphGenerator(args.base_path)\\\\\\\\n+\\\\\\\\n+# Load report data\\\\\\\\n+if not generator.load_report_data(args.report):\\\\\\\\n+return 1\\\\\\\\n+\\\\\\\\n+# --- Start of new try block for core logic ---\\\\\\\\n+try:\\\\\\\\n+# Build graph\\\\\\\\n+generator.build_graph()\\\\\\\\n+\\\\\\\\n+# Generate visualizations based on format\\\\\\\\n+if args.format in [\\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"] and PYVIS_AVAILABLE:\\\\\\\\n+generator.generate_pyvis_visualization(\\\\\\\\n+args.output if args.format == \\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\" else None\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+if args.format in [\\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"] and NETWORKX_AVAILABLE:\\\\\\\\n+generator.generate_networkx_visualization(\\\\\\\\n+args.output if args.format == \\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\" else None\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+if args.format in [\\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"]:\\\\\\\\n+generator.export_graph_data(\\\\\\\\n+args.output if args.format == \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\" else None\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Analyze graph if requested\\\\\\\\n+if args.analyze:\\\\\\\\n+analysis = generator.analyze_graph()\\\\\\\\n+\\\\\\\\n+# Print analysis results using rich tables if available\\\\\\\\n+console.print(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n [bold cyan]DOCUMENTATION GRAPH ANALYSIS[/bold cyan]\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+summary_table = Table(title=\\\\\\\\\\\\\\\"Graph Summary\\\\\\\\\\\\\\\", show_header=False, box=None)\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Nodes:\\\\\\\\\\\\\\\", str(analysis.get('node_count', 'N/A')))\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Edges:\\\\\\\\\\\\\\\", str(analysis.get('edge_count', 'N/A')))\\\\\\\\n+if \\\\\\\\\\\\\\\"density\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Graph Density:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"{analysis['density']:.4f}\\\\\\\\\\\\\\\")\\\\\\\\n+if \\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Isolated Documents:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"[yellow]{analysis['isolated_count']}[/yellow]\\\\\\\\\\\\\\\")\\\\\\\\n+if \\\\\\\\\\\\\\\"strongly_connected_components\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Strongly Connected Components:\\\\\\\\\\\\\\\", str(analysis['strongly_connected_components']))\\\\\\\\n+if \\\\\\\\\\\\\\\"largest_component_size\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n+ratio = analysis.get('largest_component_ratio', 0)\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Largest Component Size:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"{analysis['largest_component_size']} ({ratio:.1%})\\\\\\\\\\\\\\\")\\\\\\\\n+console.print(summary_table)\\\\\\\\n+\\\\\\\\n+# --- Most Referenced Documents Table ---\\\\\\\\n+if \\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n+ref_table = Table(title=\\\\\\\\\\\\\\\" Most Referenced Documents (Top 5)\\\\\\\\\\\\\\\", expand=True)\\\\\\\\n+ref_table.add_column(\\\\\\\\\\\\\\\"Rank\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"dim\\\\\\\\\\\\\\\", width=5)\\\\\\\\n+ref_table.add_column(\\\\\\\\\\\\\\\"Document\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"magenta\\\\\\\\\\\\\\\")\\\\\\\\n+ref_table.add_column(\\\\\\\\\\\\\\\"Score/Count\\\\\\\\\\\\\\\", justify=\\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"green\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+for i, doc_info in enumerate(analysis[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"][:5], 1):\\\\\\\\n+score = doc_info.get('score', doc_info.get('count', 'N/A'))\\\\\\\\n+score_str = f\\\\\\\\\\\\\\\"{score:.3f}\\\\\\\\\\\\\\\" if isinstance(score, float) else str(score)\\\\\\\\n+ref_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\\\\\\\n+console.print(ref_table)\\\\\\\\n+\\\\\\\\n+# --- Bridge Documents Table ---\\\\\\\\n+if \\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n+bridge_table = Table(title=\\\\\\\\\\\\\\\" Bridge Documents (High Betweenness - Top 5)\\\\\\\\\\\\\\\", expand=True)\\\\\\\\n+bridge_table.add_column(\\\\\\\\\\\\\\\"Rank\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"dim\\\\\\\\\\\\\\\", width=5)\\\\\\\\n+bridge_table.add_column(\\\\\\\\\\\\\\\"Document\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"cyan\\\\\\\\\\\\\\\")\\\\\\\\n+bridge_table.add_column(\\\\\\\\\\\\\\\"Score\\\\\\\\\\\\\\\", justify=\\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"blue\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+for i, doc_info in enumerate(analysis[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"][:5], 1):\\\\\\\\n+score = doc_info.get('score', 'N/A')\\\\\\\\n+score_str = f\\\\\\\\\\\\\\\"{score:.3f}\\\\\\\\\\\\\\\" if isinstance(score, float) else str(score)\\\\\\\\n+bridge_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\\\\\\\n+console.print(bridge_table)\\\\\\\\n+\\\\\\\\n+# --- Isolated Documents ---\\\\\\\\n+if \\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n+console.print(f\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n [yellow]Isolated Documents ({analysis['isolated_count']}):[/yellow]\\\\\\\\\\\\\\\")\\\\\\\\n+# Print only first few for brevity\\\\\\\\n+for doc in analysis['isolated_docs'][:10]:\\\\\\\\n+console.print(f\\\\\\\\\\\\\\\"  - {os.path.basename(doc)}\\\\\\\\\\\\\\\")\\\\\\\\n+if len(analysis['isolated_docs']) > 10:\\\\\\\\n+console.print(f\\\\\\\\\\\\\\\"  ... and {len(analysis['isolated_docs']) - 10} more\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Save analysis to JSON\\\\\\\\n+analysis_path = generator.visualization_path / \\\\\\\\\\\\\\\"documentation_graph_analysis.json\\\\\\\\\\\\\\\"\\\\\\\\n+with open(analysis_path, 'w', encoding='utf-8') as f:\\\\\\\\n+json.dump(analysis, f, indent=2)\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Analysis saved to: {analysis_path}\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# --- End of new try block, add except block ---\\\\\\\\n+except Exception as core_error:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"!!! Critical Error during graph processing/visualization: {core_error}\\\\\\\\\\\\\\\")\\\\\\\\n+logger.error(traceback.format_exc())\\\\\\\\n+return 1 # Ensure exit with error code\\\\\\\\n+\\\\\\\\n+return 0\\\\\\\\n+\\\\\\\\n+# Outer except block remains for initialization/loading errors\\\\\\\\n+except Exception as e:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"Initialization or Loading Error: {str(e)}\\\\\\\\\\\\\\\")\\\\\\\\n+# Ensure traceback is imported if needed here too\\\\\\\\n+if 'traceback' not in sys.modules:\\\\\\\\n+import traceback\\\\\\\\n+logger.error(traceback.format_exc())\\\\\\\\n+return 1\\\\\\\\n \\\\\\\\n \\\\\\\\n if __name__ == \\\\\\\\\\\\\\\"__main__\\\\\\\\\\\\\\\":\\\\\\\\n-    sys.exit(main())+sys.exit(main())\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\"\\\"\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_LinkTo_results.json",
    "LineNumber": 5,
    "LineContent": "    \"LineContent\": \"            *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, `Related:`, `Doc:`, `Link to:`).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_Reference_results.json",
    "LineNumber": 10,
    "LineContent": "    \"LineContent\": \"            *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, `Related:`, `Doc:`, `Link to:`).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_Ref_results.json",
    "LineNumber": 20,
    "LineContent": "    \"LineContent\": \"            *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, `Related:`, `Doc:`, `Link to:`).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_Related_results.json",
    "LineNumber": 5,
    "LineContent": "    \"LineContent\": \"            *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, `Related:`, `Doc:`, `Link to:`).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_RelativePathParent_results.json",
    "LineNumber": 1015,
    "LineContent": "    \"LineContent\": \"    \\\"LineContent\\\": \\\"      \\\\\\\"diff\\\\\\\": \\\\\\\"--- a/generate_documentation_graph.py\\\\\\\\n+++ b/generate_documentation_graph.py\\\\\\\\n@@ -1,10 +1,10 @@\\\\\\\\n-<!-- \\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n @references:\\\\\\\\n - Core References:\\\\\\\\n-  - [MQP.md](mdc:../../MQP.md) - Master Quantum Prompt defining EGOS principles\\\\\\\\n-  - [ROADMAP.md](mdc:../../ROADMAP.md) - Project roadmap and planning\\\\\\\\n+- [MQP.md](mdc:../../MQP.md) - Master Quantum Prompt defining EGOS principles\\\\\\\\n+- [ROADMAP.md](mdc:../../ROADMAP.md) - Project roadmap and planning\\\\\\\\n - Process Documentation:\\\\\\\\n-  - [system_maintenance.md](mdc:../../docs_egos/process/system_maintenance.md)\\\\\\\\n+- [system_maintenance.md](mdc:../../docs_egos/process/system_maintenance.md)\\\\\\\\n \\\\\\\\n \\\\\\\\n \\\\\\\\n@@ -42,809 +42,810 @@\\\\\\\\n \\\\\\\\n # Import Rich components for progress bars\\\\\\\\n from rich.progress import (\\\\\\\\n-    Progress,\\\\\\\\n-    SpinnerColumn,\\\\\\\\n-    TextColumn,\\\\\\\\n-    BarColumn,\\\\\\\\n-    TimeRemainingColumn,\\\\\\\\n-    TaskID,\\\\\\\\n+Progress,\\\\\\\\n+SpinnerColumn,\\\\\\\\n+TextColumn,\\\\\\\\n+BarColumn,\\\\\\\\n+TimeRemainingColumn,\\\\\\\\n+TaskID,\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Setup logging\\\\\\\\n logging.basicConfig(\\\\\\\\n-    level=logging.INFO,\\\\\\\\n-    format=\\\\\\\\\\\\\\\"%(message)s\\\\\\\\\\\\\\\", # Rich handler handles time/level\\\\\\\\n-    datefmt=\\\\\\\\\\\\\\\"[%X]\\\\\\\\\\\\\\\",\\\\\\\\n-    handlers=[logging.StreamHandler()]\\\\\\\\n+level=logging.INFO,\\\\\\\\n+format=\\\\\\\\\\\\\\\"%(message)s\\\\\\\\\\\\\\\", # Rich handler handles time/level\\\\\\\\n+datefmt=\\\\\\\\\\\\\\\"[%X]\\\\\\\\\\\\\\\",\\\\\\\\n+handlers=[logging.StreamHandler()]\\\\\\\\n )\\\\\\\\n \\\\\\\\n logger = logging.getLogger(__name__)\\\\\\\\n \\\\\\\\n # Optional imports with fallbacks\\\\\\\\n try:\\\\\\\\n-    from pyvis.network import Network\\\\\\\\n-    PYVIS_AVAILABLE = True\\\\\\\\n+from pyvis.network import Network\\\\\\\\n+PYVIS_AVAILABLE = True\\\\\\\\n except ImportError:\\\\\\\\n-    logger.warning(\\\\\\\\\\\\\\\"Pyvis not installed. Will use simplified visualization.\\\\\\\\\\\\\\\")\\\\\\\\n-    PYVIS_AVAILABLE = False\\\\\\\\n+logger.warning(\\\\\\\\\\\\\\\"Pyvis not installed. Will use simplified visualization.\\\\\\\\\\\\\\\")\\\\\\\\n+PYVIS_AVAILABLE = False\\\\\\\\n \\\\\\\\n # NetworkX for basic/static visualization\\\\\\\\n try:\\\\\\\\n-    import networkx as nx\\\\\\\\n-    import matplotlib.pyplot as plt\\\\\\\\n-    NETWORKX_AVAILABLE = True\\\\\\\\n+import networkx as nx\\\\\\\\n+import matplotlib.pyplot as plt\\\\\\\\n+NETWORKX_AVAILABLE = True\\\\\\\\n except ImportError:\\\\\\\\n-    logger.warning(\\\\\\\\\\\\\\\"NetworkX not installed. Only JSON graph data will be generated.\\\\\\\\\\\\\\\")\\\\\\\\n-    NETWORKX_AVAILABLE = False\\\\\\\\n+logger.warning(\\\\\\\\\\\\\\\"NetworkX not installed. Only JSON graph data will be generated.\\\\\\\\\\\\\\\")\\\\\\\\n+NETWORKX_AVAILABLE = False\\\\\\\\n \\\\\\\\n \\\\\\\\n class DocumentationGraphGenerator:\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generates interactive visualizations of EGOS documentation connections.\\\\\\\\n-    \\\\\\\\n-    This class creates visual representations of the documentation cross-reference\\\\\\\\n-    network, helping identify key documents, clusters, and connection patterns.\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    \\\\\\\\n-    def __init__(self, base_path: str = \\\\\\\\\\\\\\\".\\\\\\\\\\\\\\\"):\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Initialize the graph generator.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            base_path: Base path of the EGOS project\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        self.base_path = Path(base_path)\\\\\\\\n-        self.report_path = self.base_path / \\\\\\\\\\\\\\\"reports\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\"\\\\\\\\n-        self.visualization_path = self.report_path / \\\\\\\\\\\\\\\"visualizations\\\\\\\\\\\\\\\"\\\\\\\\n-        \\\\\\\\n-        # Ensure output directories exist\\\\\\\\n-        self.visualization_path.mkdir(parents=True, exist_ok=True)\\\\\\\\n-        \\\\\\\\n-        # Graph data\\\\\\\\n-        self.nodes = []\\\\\\\\n-        self.edges = []\\\\\\\\n-        self.subsystems = set()\\\\\\\\n-        \\\\\\\\n-        # Node attributes\\\\\\\\n-        self.node_sizes = {}\\\\\\\\n-        self.node_colors = {}\\\\\\\\n-        self.node_clusters = {}\\\\\\\\n-        \\\\\\\\n-        # Color palette for subsystems with EGOS-aligned color scheme\\\\\\\\n-        self.color_palette = {\\\\\\\\n-            \\\\\\\\\\\\\\\"koios\\\\\\\\\\\\\\\": theme.color.koios,    # Blue - Knowledge\\\\\\\\n-            \\\\\\\\\\\\\\\"cronos\\\\\\\\\\\\\\\": theme.color.cronos,   # Purple - Time\\\\\\\\n-            \\\\\\\\\\\\\\\"nexus\\\\\\\\\\\\\\\": theme.color.nexus,    # Green - Connections\\\\\\\\n-            \\\\\\\\\\\\\\\"ethik\\\\\\\\\\\\\\\": theme.color.ethik,    # Red - Ethics\\\\\\\\n-            \\\\\\\\\\\\\\\"atlas\\\\\\\\\\\\\\\": theme.color.atlas,    # Orange - Mapping\\\\\\\\n-            \\\\\\\\\\\\\\\"mycelium\\\\\\\\\\\\\\\": theme.color.mycelium, # Teal - Integration\\\\\\\\n-            \\\\\\\\\\\\\\\"harmony\\\\\\\\\\\\\\\": theme.color.harmony,  # Dark Blue - Harmony\\\\\\\\n-            \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\": theme.color.default   # Gray - Default\\\\\\\\n-        }\\\\\\\\n-    \\\\\\\\n-    def load_report_data(self, report_path: Optional[str] = None) -> bool:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Load cross-reference report data from a JSON file.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            report_path: Path to the JSON report file (optional)\\\\\\\\n-            \\\\\\\\n-        Returns:\\\\\\\\n-            True if data was loaded successfully, False otherwise\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if report_path:\\\\\\\\n-            json_path = Path(report_path)\\\\\\\\n-        else:\\\\\\\\n-            # Look for the latest 'analysis' stage checkpoint file\\\\\\\\n-            checkpoint_dir = self.base_path / \\\\\\\\\\\\\\\"reports\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"checkpoints\\\\\\\\\\\\\\\"\\\\\\\\n-            analysis_checkpoints = list(checkpoint_dir.glob(\\\\\\\\\\\\\\\"xref_checkpoint_analysis_*.json\\\\\\\\\\\\\\\"))\\\\\\\\n-            \\\\\\\\n-            if not analysis_checkpoints:\\\\\\\\n-                # Fallback: Check for older format without timestamp\\\\\\\\n-                old_format_checkpoint = checkpoint_dir / \\\\\\\\\\\\\\\"xref_checkpoint_analysis.json\\\\\\\\\\\\\\\"\\\\\\\\n-                if old_format_checkpoint.exists():\\\\\\\\n-                    analysis_checkpoints.append(old_format_checkpoint)\\\\\\\\n-                else:\\\\\\\\n-                    logger.error(\\\\\\\\n-                        \\\\\\\\\\\\\\\"No analysis checkpoint found in reports/documentation/checkpoints/. \\\\\\\\\\\\\\\"\\\\\\\\n-                        \\\\\\\\\\\\\\\"Run add_cross_references.py --stage analysis first.\\\\\\\\\\\\\\\"\\\\\\\\n-                    )\\\\\\\\n-                    return False\\\\\\\\n-                    \\\\\\\\n-            # Get the most recent analysis checkpoint\\\\\\\\n-            json_path = max(analysis_checkpoints, key=os.path.getmtime)\\\\\\\\n-            logger.info(f\\\\\\\\\\\\\\\"Using latest analysis checkpoint: {json_path.name}\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Loading and processing graph data from: {json_path}\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        try:\\\\\\\\n-            with open(json_path, 'r', encoding='utf-8') as f:\\\\\\\\n-                checkpoint_content = json.load(f)\\\\\\\\n-            \\\\\\\\n-            # Extract the core data list\\\\\\\\n-            raw_data = None\\\\\\\\n-            if isinstance(checkpoint_content, dict) and \\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\" in checkpoint_content:\\\\\\\\n-                if isinstance(checkpoint_content[\\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\"], list):\\\\\\\\n-                     raw_data = checkpoint_content[\\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\"]\\\\\\\\n-            elif isinstance(checkpoint_content, list):\\\\\\\\n-                 raw_data = checkpoint_content # Assume direct list data\\\\\\\\n-                 \\\\\\\\n-            if raw_data is None:\\\\\\\\n-                logger.error(f\\\\\\\\\\\\\\\"Could not extract valid list data from checkpoint {json_path.name}\\\\\\\\\\\\\\\")\\\\\\\\n-                return False\\\\\\\\n-                \\\\\\\\n-            # Process the list data to build the required dictionary structure\\\\\\\\n-            processed_data = {\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\": {}, \\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\": {}}\\\\\\\\n-            all_docs = set()\\\\\\\\n-\\\\\\\\n-            # First pass: Build referenced_by and collect all docs\\\\\\\\n-            for item in raw_data:\\\\\\\\n-                # Expecting [doc_path, some_int, list_of_referencing_docs]\\\\\\\\n-                if len(item) == 3 and isinstance(item[0], str) and isinstance(item[2], list):\\\\\\\\n-                    doc = item[0]\\\\\\\\n-                    referencing_docs = item[2]\\\\\\\\n-                    all_docs.add(doc)\\\\\\\\n-                    processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][doc] = referencing_docs\\\\\\\\n-                    all_docs.update(referencing_docs) # Add referencing docs too\\\\\\\\n-                else:\\\\\\\\n-                     logger.warning(f\\\\\\\\\\\\\\\"Skipping malformed item in checkpoint data: {item}\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-            # Initialize all docs in references dict\\\\\\\\n-            for doc in all_docs:\\\\\\\\n-                # Ensure doc path is cleaned/normalized if needed (optional)\\\\\\\\n-                # doc_clean = Path(doc).as_posix() # Example normalization\\\\\\\\n-                processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][doc] = [] \\\\\\\\n-                # Ensure referenced_by also has all docs initialized, even if empty initially\\\\\\\\n-                if doc not in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"]:\\\\\\\\n-                    processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][doc] = []\\\\\\\\n-                \\\\\\\\n-            # Second pass: Build references from referenced_by\\\\\\\\n-            for doc, referencing_docs in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"].items():\\\\\\\\n-                for ref_doc in referencing_docs:\\\\\\\\n-                    if ref_doc in processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"]:\\\\\\\\n-                        processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][ref_doc].append(doc)\\\\\\\\n-                    else:\\\\\\\\n-                        # This case handles if a referencing doc wasn't in the initial list's first element\\\\\\\\n-                        # It means ref_doc references 'doc', but ref_doc itself might not have been a primary entry\\\\\\\\n-                        processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][ref_doc] = [doc]\\\\\\\\n-                        if ref_doc not in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"]:\\\\\\\\n-                           processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][ref_doc] = [] # Initialize if missing\\\\\\\\n-                        logger.debug(f\\\\\\\\\\\\\\\"Added '{ref_doc}' to structures during second pass.\\\\\\\\\\\\\\\")\\\\\\\\n-                        \\\\\\\\n-            self.report_data = processed_data\\\\\\\\n-            logger.info(f\\\\\\\\\\\\\\\"Successfully processed {len(self.report_data['references'])} documents from checkpoint.\\\\\\\\\\\\\\\")\\\\\\\\n-            return True\\\\\\\\n-                \\\\\\\\n-        except Exception as e:\\\\\\\\n-            logger.error(f\\\\\\\\\\\\\\\"Error loading and processing graph data: {str(e)}\\\\\\\\\\\\\\\")\\\\\\\\n-            import traceback # Added for better debugging\\\\\\\\n-            logger.error(traceback.format_exc()) # Added for better debugging\\\\\\\\n-            return False\\\\\\\\n-    \\\\\\\\n-    def build_graph(self) -> None:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Build the document graph from loaded report data.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not hasattr(self, 'report_data'):\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No report data loaded. Call load_report_data() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return\\\\\\\\n-            \\\\\\\\n-        references = self.report_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"]\\\\\\\\n-        referenced_by = self.report_data.get(\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\", {})\\\\\\\\n-        \\\\\\\\n-        logger.info(\\\\\\\\\\\\\\\"Building documentation graph...\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        # Create nodes for all documents\\\\\\\\n-        all_docs = set(references.keys())\\\\\\\\n-        for refs in references.values():\\\\\\\\n-            all_docs.update(refs)\\\\\\\\n-        \\\\\\\\n-        # Show progress during graph building\\\\\\\\n-        total_docs = len(all_docs)\\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Processing {total_docs} documents...\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        if console:\\\\\\\\n-            with Progress(\\\\\\\\n-                SpinnerColumn(),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"[progress.description]{task.description}\\\\\\\\\\\\\\\"),\\\\\\\\n-                BarColumn(),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"[progress.percentage]{task.percentage:>3.0f}%\\\\\\\\\\\\\\\"),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"({task.completed}/{task.total})\\\\\\\\\\\\\\\"),\\\\\\\\n-                TimeRemainingColumn(),\\\\\\\\n-                console=console,\\\\\\\\n-                transient=False,  # Keep progress visible after completion\\\\\\\\n-            ) as progress:\\\\\\\\n-                build_task = progress.add_task(\\\\\\\\\\\\\\\"Building graph nodes and edges...\\\\\\\\\\\\\\\", total=total_docs)\\\\\\\\n-                \\\\\\\\n-                # Process each document as a node\\\\\\\\n-                for i, doc in enumerate(all_docs):\\\\\\\\n-                    try: \\\\\\\\n-                        # Basic filtering for non-markdown files or complex paths\\\\\\\\n-                        if not doc.endswith(\\\\\\\\\\\\\\\".md\\\\\\\\\\\\\\\") or \\\\\\\\\\\\\\\"/\\\\\\\\\\\\\\\" in doc or r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\.\\\\\\\\\\\\\\\" in doc or \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\" in doc:\\\\\\\\n-                            logger.debug(f\\\\\\\\\\\\\\\"Skipping non-standard doc path: {doc}\\\\\\\\\\\\\\\")\\\\\\\\n-                            progress.update(build_task, advance=1)\\\\\\\\n-                            continue\\\\\\\\n-        \\\\\\\\n-                        # Extract subsystem from path\\\\\\\\n-                        subsystem = self._get_subsystem(doc)\\\\\\\\n-                        self.subsystems.add(subsystem)\\\\\\\\n-        \\\\\\\\n-                        # Add node with attributes\\\\\\\\n-                        node_id = doc\\\\\\\\n-                        node_label = os.path.basename(doc)\\\\\\\\n-                        # Ensure references and referenced_by are accessed safely\\\\\\\\n-                        doc_refs = references.get(doc, [])\\\\\\\\n-                        doc_referenced_by = referenced_by.get(doc, [])\\\\\\\\n-        \\\\\\\\n-                        node_title = self._generate_node_title(doc, doc_refs, doc_referenced_by) \\\\\\\\n-                        node_size = self._calculate_node_size(len(doc_refs), len(doc_referenced_by))\\\\\\\\n-                        node_color = self.color_palette.get(subsystem, self.color_palette[\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\"])\\\\\\\\n-        \\\\\\\\n-                        self.nodes.append({\\\\\\\\n-                            \\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\": node_id,\\\\\\\\n-                            \\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\": node_label,\\\\\\\\n-                            \\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\": node_title,\\\\\\\\n-                            \\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\": node_size,\\\\\\\\n-                            \\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\": node_color,\\\\\\\\n-                            \\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\": subsystem\\\\\\\\n-                        })\\\\\\\\n-        \\\\\\\\n-                        # Add edges (references from this doc to others)\\\\\\\\n-                        for target_doc in doc_refs:\\\\\\\\n-                            # Skip self-references or invalid targets\\\\\\\\n-                            if target_doc == doc or not target_doc.endswith(\\\\\\\\\\\\\\\".md\\\\\\\\\\\\\\\") or \\\\\\\\\\\\\\\"/\\\\\\\\\\\\\\\" in target_doc or r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\.\\\\\\\\\\\\\\\" in target_doc or \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\" in target_doc:\\\\\\\\n-                                continue\\\\\\\\n-                            self.edges.append({\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\": node_id, \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\": target_doc, \\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"})\\\\\\\\n-        \\\\\\\\n-                        # Update progress\\\\\\\\n-                        progress.update(build_task, advance=1)\\\\\\\\n-        \\\\\\\\n-                    except Exception as node_error:\\\\\\\\n-                        logger.error(f\\\\\\\\\\\\\\\"Error processing node '{doc}': {node_error}\\\\\\\\\\\\\\\")\\\\\\\\n-                        # Still advance progress even if one node fails\\\\\\\\n-                        progress.update(build_task, advance=1)\\\\\\\\n-                        continue\\\\\\\\n-        else:\\\\\\\\n-            # Fallback for when rich is not available\\\\\\\\n-            logger.info(\\\\\\\\\\\\\\\"Building graph nodes and edges (basic logging)...\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generates interactive visualizations of EGOS documentation connections.\\\\\\\\n+\\\\\\\\n+This class creates visual representations of the documentation cross-reference\\\\\\\\n+network, helping identify key documents, clusters, and connection patterns.\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+def __init__(self, base_path: str = \\\\\\\\\\\\\\\".\\\\\\\\\\\\\\\"):\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Initialize the graph generator.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+base_path: Base path of the EGOS project\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+self.base_path = Path(base_path)\\\\\\\\n+self.report_path = self.base_path / \\\\\\\\\\\\\\\"reports\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\"\\\\\\\\n+self.visualization_path = self.report_path / \\\\\\\\\\\\\\\"visualizations\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Ensure output directories exist\\\\\\\\n+self.visualization_path.mkdir(parents=True, exist_ok=True)\\\\\\\\n+\\\\\\\\n+# Graph data\\\\\\\\n+self.nodes = []\\\\\\\\n+self.edges = []\\\\\\\\n+self.subsystems = set()\\\\\\\\n+\\\\\\\\n+# Node attributes\\\\\\\\n+self.node_sizes = {}\\\\\\\\n+self.node_colors = {}\\\\\\\\n+self.node_clusters = {}\\\\\\\\n+\\\\\\\\n+# Color palette for subsystems with EGOS-aligned color scheme\\\\\\\\n+self.color_palette = {\\\\\\\\n+\\\\\\\\\\\\\\\"koios\\\\\\\\\\\\\\\": theme.color.koios,    # Blue - Knowledge\\\\\\\\n+\\\\\\\\\\\\\\\"cronos\\\\\\\\\\\\\\\": theme.color.cronos,   # Purple - Time\\\\\\\\n+\\\\\\\\\\\\\\\"nexus\\\\\\\\\\\\\\\": theme.color.nexus,    # Green - Connections\\\\\\\\n+\\\\\\\\\\\\\\\"ethik\\\\\\\\\\\\\\\": theme.color.ethik,    # Red - Ethics\\\\\\\\n+\\\\\\\\\\\\\\\"atlas\\\\\\\\\\\\\\\": theme.color.atlas,    # Orange - Mapping\\\\\\\\n+\\\\\\\\\\\\\\\"mycelium\\\\\\\\\\\\\\\": theme.color.mycelium, # Teal - Integration\\\\\\\\n+\\\\\\\\\\\\\\\"harmony\\\\\\\\\\\\\\\": theme.color.harmony,  # Dark Blue - Harmony\\\\\\\\n+\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\": theme.color.default   # Gray - Default\\\\\\\\n+}\\\\\\\\n+\\\\\\\\n+def load_report_data(self, report_path: Optional[str] = None) -> bool:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Load cross-reference report data from a JSON file.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+report_path: Path to the JSON report file (optional)\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+True if data was loaded successfully, False otherwise\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if report_path:\\\\\\\\n+json_path = Path(report_path)\\\\\\\\n+else:\\\\\\\\n+# Look for the latest 'analysis' stage checkpoint file\\\\\\\\n+checkpoint_dir = self.base_path / \\\\\\\\\\\\\\\"reports\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"checkpoints\\\\\\\\\\\\\\\"\\\\\\\\n+analysis_checkpoints = list(checkpoint_dir.glob(\\\\\\\\\\\\\\\"xref_checkpoint_analysis_*.json\\\\\\\\\\\\\\\"))\\\\\\\\n+\\\\\\\\n+if not analysis_checkpoints:\\\\\\\\n+# Fallback: Check for older format without timestamp\\\\\\\\n+old_format_checkpoint = checkpoint_dir / \\\\\\\\\\\\\\\"xref_checkpoint_analysis.json\\\\\\\\\\\\\\\"\\\\\\\\n+if old_format_checkpoint.exists():\\\\\\\\n+analysis_checkpoints.append(old_format_checkpoint)\\\\\\\\n+else:\\\\\\\\n+logger.error(\\\\\\\\n+\\\\\\\\\\\\\\\"No analysis checkpoint found in reports/documentation/checkpoints/. \\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\\\\\\\\"Run add_cross_references.py --stage analysis first.\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+return False\\\\\\\\n+\\\\\\\\n+# Get the most recent analysis checkpoint\\\\\\\\n+json_path = max(analysis_checkpoints, key=os.path.getmtime)\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Using latest analysis checkpoint: {json_path.name}\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Loading and processing graph data from: {json_path}\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+try:\\\\\\\\n+with open(json_path, 'r', encoding='utf-8') as f:\\\\\\\\n+checkpoint_content = json.load(f)\\\\\\\\n+\\\\\\\\n+# Extract the core data list\\\\\\\\n+raw_data = None\\\\\\\\n+if isinstance(checkpoint_content, dict) and \\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\" in checkpoint_content:\\\\\\\\n+if isinstance(checkpoint_content[\\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\"], list):\\\\\\\\n+raw_data = checkpoint_content[\\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\"]\\\\\\\\n+elif isinstance(checkpoint_content, list):\\\\\\\\n+raw_data = checkpoint_content # Assume direct list data\\\\\\\\n+\\\\\\\\n+if raw_data is None:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"Could not extract valid list data from checkpoint {json_path.name}\\\\\\\\\\\\\\\")\\\\\\\\n+return False\\\\\\\\n+\\\\\\\\n+# Process the list data to build the required dictionary structure\\\\\\\\n+processed_data = {\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\": {}, \\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\": {}}\\\\\\\\n+all_docs = set()\\\\\\\\n+\\\\\\\\n+# First pass: Build referenced_by and collect all docs\\\\\\\\n+for item in raw_data:\\\\\\\\n+# Expecting [doc_path, some_int, list_of_referencing_docs]\\\\\\\\n+if len(item) == 3 and isinstance(item[0], str) and isinstance(item[2], list):\\\\\\\\n+doc = item[0]\\\\\\\\n+referencing_docs = item[2]\\\\\\\\n+all_docs.add(doc)\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][doc] = referencing_docs\\\\\\\\n+all_docs.update(referencing_docs) # Add referencing docs too\\\\\\\\n+else:\\\\\\\\n+logger.warning(f\\\\\\\\\\\\\\\"Skipping malformed item in checkpoint data: {item}\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Initialize all docs in references dict\\\\\\\\n+for doc in all_docs:\\\\\\\\n+# Ensure doc path is cleaned/normalized if needed (optional)\\\\\\\\n+# doc_clean = Path(doc).as_posix() # Example normalization\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][doc] = []\\\\\\\\n+# Ensure referenced_by also has all docs initialized, even if empty initially\\\\\\\\n+if doc not in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"]:\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][doc] = []\\\\\\\\n+\\\\\\\\n+# Second pass: Build references from referenced_by\\\\\\\\n+for doc, referencing_docs in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"].items():\\\\\\\\n+for ref_doc in referencing_docs:\\\\\\\\n+if ref_doc in processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"]:\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][ref_doc].append(doc)\\\\\\\\n+else:\\\\\\\\n+# This case handles if a referencing doc wasn't in the initial list's first element\\\\\\\\n+# It means ref_doc references 'doc', but ref_doc itself might not have been a primary entry\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][ref_doc] = [doc]\\\\\\\\n+if ref_doc not in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"]:\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][ref_doc] = [] # Initialize if missing\\\\\\\\n+logger.debug(f\\\\\\\\\\\\\\\"Added '{ref_doc}' to structures during second pass.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+self.report_data = processed_data\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Successfully processed {len(self.report_data['references'])} documents from checkpoint.\\\\\\\\\\\\\\\")\\\\\\\\n+return True\\\\\\\\n+\\\\\\\\n+except Exception as e:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"Error loading and processing graph data: {str(e)}\\\\\\\\\\\\\\\")\\\\\\\\n+import traceback # Added for better debugging\\\\\\\\n+logger.error(traceback.format_exc()) # Added for better debugging\\\\\\\\n+return False\\\\\\\\n+\\\\\\\\n+def build_graph(self) -> None:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Build the document graph from loaded report data.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not hasattr(self, 'report_data'):\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No report data loaded. Call load_report_data() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return\\\\\\\\n+\\\\\\\\n+references = self.report_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"]\\\\\\\\n+referenced_by = self.report_data.get(\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\", {})\\\\\\\\n+\\\\\\\\n+logger.info(\\\\\\\\\\\\\\\"Building documentation graph...\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Create nodes for all documents\\\\\\\\n+all_docs = set(references.keys())\\\\\\\\n+for refs in references.values():\\\\\\\\n+all_docs.update(refs)\\\\\\\\n+\\\\\\\\n+# Show progress during graph building\\\\\\\\n+total_docs = len(all_docs)\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Processing {total_docs} documents...\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+if console:\\\\\\\\n+with Progress(\\\\\\\\n+SpinnerColumn(),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"[progress.description]{task.description}\\\\\\\\\\\\\\\"),\\\\\\\\n+BarColumn(),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"[progress.percentage]{task.percentage:>3.0f}%\\\\\\\\\\\\\\\"),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"({task.completed}/{task.total})\\\\\\\\\\\\\\\"),\\\\\\\\n+TimeRemainingColumn(),\\\\\\\\n+console=console,\\\\\\\\n+transient=False,  # Keep progress visible after completion\\\\\\\\n+) as progress:\\\\\\\\n+build_task = progress.add_task(\\\\\\\\\\\\\\\"Building graph nodes and edges...\\\\\\\\\\\\\\\", total=total_docs)\\\\\\\\n+\\\\\\\\n+# Process each document as a node\\\\\\\\n+for i, doc in enumerate(all_docs):\\\\\\\\n+try:\\\\\\\\n+# Basic filtering for non-markdown files or complex paths\\\\\\\\n+if not doc.endswith(\\\\\\\\\\\\\\\".md\\\\\\\\\\\\\\\") or \\\\\\\\\\\\\\\"/\\\\\\\\\\\\\\\" in doc or r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\.\\\\\\\\\\\\\\\" in doc or \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\" in doc:\\\\\\\\n+logger.debug(f\\\\\\\\\\\\\\\"Skipping non-standard doc path: {doc}\\\\\\\\\\\\\\\")\\\\\\\\n+progress.update(build_task, advance=1)\\\\\\\\n+continue\\\\\\\\n+\\\\\\\\n+# Extract subsystem from path\\\\\\\\n+subsystem = self._get_subsystem(doc)\\\\\\\\n+self.subsystems.add(subsystem)\\\\\\\\n+\\\\\\\\n+# Add node with attributes\\\\\\\\n+node_id = doc\\\\\\\\n+node_label = os.path.basename(doc)\\\\\\\\n+# Ensure references and referenced_by are accessed safely\\\\\\\\n+doc_refs = references.get(doc, [])\\\\\\\\n+doc_referenced_by = referenced_by.get(doc, [])\\\\\\\\n+\\\\\\\\n+node_title = self._generate_node_title(doc, doc_refs, doc_referenced_by)\\\\\\\\n+node_size = self._calculate_node_size(len(doc_refs), len(doc_referenced_by))\\\\\\\\n+node_color = self.color_palette.get(subsystem, self.color_palette[\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+self.nodes.append({\\\\\\\\n+\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\": node_id,\\\\\\\\n+\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\": node_label,\\\\\\\\n+\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\": node_title,\\\\\\\\n+\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\": node_size,\\\\\\\\n+\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\": node_color,\\\\\\\\n+\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\": subsystem\\\\\\\\n+})\\\\\\\\n+\\\\\\\\n+# Add edges (references from this doc to others)\\\\\\\\n+for target_doc in doc_refs:\\\\\\\\n+# Skip self-references or invalid targets\\\\\\\\n+if target_doc == doc or not target_doc.endswith(\\\\\\\\\\\\\\\".md\\\\\\\\\\\\\\\") or \\\\\\\\\\\\\\\"/\\\\\\\\\\\\\\\" in target_doc or r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\.\\\\\\\\\\\\\\\" in target_doc or \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\" in target_doc:\\\\\\\\n+continue\\\\\\\\n+self.edges.append({\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\": node_id, \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\": target_doc, \\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"})\\\\\\\\n+\\\\\\\\n+# Update progress\\\\\\\\n+progress.update(build_task, advance=1)\\\\\\\\n+\\\\\\\\n+except Exception as node_error:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"Error processing node '{doc}': {node_error}\\\\\\\\\\\\\\\")\\\\\\\\n+# Still advance progress even if one node fails\\\\\\\\n+progress.update(build_task, advance=1)\\\\\\\\n+continue\\\\\\\\n+else:\\\\\\\\n+# Fallback for when rich is not available\\\\\\\\n+logger.info(\\\\\\\\\\\\\\\"Building graph nodes and edges (basic logging)...\\\\\\\\\\\\\\\")\\\\\\\\n {{ ... }}\\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Graph built successfully: {len(self.nodes)} nodes, {len(self.edges)} edges.\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-    def generate_pyvis_visualization(self, output_path: Optional[str] = None) -> str:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generate an interactive HTML visualization using Pyvis.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            output_path: Path to save the HTML file (optional)\\\\\\\\n-            \\\\\\\\n-        Returns:\\\\\\\\n-            Path to the generated HTML file\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not hasattr(self, 'nodes') or not self.nodes:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No graph data to visualize. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Create network\\\\\\\\n-        timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n-        output_filename = f\\\\\\\\\\\\\\\"documentation_graph_{timestamp}.html\\\\\\\\\\\\\\\"\\\\\\\\n-        \\\\\\\\n-        if output_path:\\\\\\\\n-            output_file = Path(output_path)\\\\\\\\n-        else:\\\\\\\\n-            output_file = self.visualization_path / output_filename\\\\\\\\n-            \\\\\\\\n-        # Log the start of visualization generation\\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Generating interactive visualization ({len(self.nodes)} nodes, {len(self.edges)} edges)...\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        if console:\\\\\\\\n-            with Progress(\\\\\\\\n-                SpinnerColumn(),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"[progress.description]{task.description}\\\\\\\\\\\\\\\"),\\\\\\\\n-                BarColumn(),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"[progress.percentage]{task.percentage:>3.0f}%\\\\\\\\\\\\\\\"),\\\\\\\\n-                TimeRemainingColumn(),\\\\\\\\n-                console=console,\\\\\\\\n-                transient=False, # Keep progress visible after completion\\\\\\\\n-            ) as progress:\\\\\\\\n-                # Create network\\\\\\\\n-                net_task = progress.add_task(\\\\\\\\\\\\\\\"Creating network...\\\\\\\\\\\\\\\", total=1)\\\\\\\\n-                net = Network(\\\\\\\\n-                    height=\\\\\\\\\\\\\\\"800px\\\\\\\\\\\\\\\", \\\\\\\\n-                    width=\\\\\\\\\\\\\\\"100%\\\\\\\\\\\\\\\", \\\\\\\\n-                    bgcolor=theme.color.white, \\\\\\\\n-                    font_color=theme.color.text_primary,\\\\\\\\n-                    heading=f\\\\\\\\\\\\\\\"EGOS Documentation Network ({timestamp})\\\\\\\\\\\\\\\"\\\\\\\\n-                )\\\\\\\\n-                progress.update(net_task, completed=1)\\\\\\\\n-                \\\\\\\\n-                # Add nodes with progress\\\\\\\\n-                node_task = progress.add_task(\\\\\\\\\\\\\\\"Adding nodes...\\\\\\\\\\\\\\\", total=len(self.nodes))\\\\\\\\n-                for node in self.nodes:\\\\\\\\n-                    net.add_node(\\\\\\\\n-                        node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"], \\\\\\\\n-                        label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"], \\\\\\\\n-                        title=node[\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\"],\\\\\\\\n-                        color=node[\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\"],\\\\\\\\n-                        size=node[\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\"],\\\\\\\\n-                        group=node[\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\"]\\\\\\\\n-                    )\\\\\\\\n-                    progress.update(node_task, advance=1)\\\\\\\\n-                \\\\\\\\n-                # Add edges with progress\\\\\\\\n-                edge_task = progress.add_task(\\\\\\\\\\\\\\\"Adding edges...\\\\\\\\\\\\\\\", total=len(self.edges))\\\\\\\\n-                for edge in self.edges:\\\\\\\\n-                    net.add_edge(\\\\\\\\n-                        edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], \\\\\\\\n-                        edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"], \\\\\\\\n-                        arrows=edge.get(\\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"),\\\\\\\\n-                        title=edge.get(\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n-                    )\\\\\\\\n-                    progress.update(edge_task, advance=1)\\\\\\\\n-                \\\\\\\\n-                # Configure physics for better readability\\\\\\\\n-                save_task = progress.add_task(\\\\\\\\\\\\\\\"Configuring and saving...\\\\\\\\\\\\\\\", total=1)\\\\\\\\n-                net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\\\\\\\n-                net.set_options(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-                {\\\\\\\\n-                  \\\\\\\\\\\\\\\"physics\\\\\\\\\\\\\\\": {\\\\\\\\n-                    \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\": {\\\\\\\\n-                      \\\\\\\\\\\\\\\"gravitationalConstant\\\\\\\\\\\\\\\": -100,\\\\\\\\n-                      \\\\\\\\\\\\\\\"centralGravity\\\\\\\\\\\\\\\": 0.01,\\\\\\\\n-                      \\\\\\\\\\\\\\\"springLength\\\\\\\\\\\\\\\": 200,\\\\\\\\n-                      \\\\\\\\\\\\\\\"springConstant\\\\\\\\\\\\\\\": 0.08\\\\\\\\n-                    },\\\\\\\\n-                    \\\\\\\\\\\\\\\"solver\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\",\\\\\\\\n-                    \\\\\\\\\\\\\\\"stabilization\\\\\\\\\\\\\\\": {\\\\\\\\n-                      \\\\\\\\\\\\\\\"iterations\\\\\\\\\\\\\\\": 100\\\\\\\\n-                    }\\\\\\\\n-                  }\\\\\\\\n-                }\\\\\\\\n-                \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n-                \\\\\\\\n-                # Save the network\\\\\\\\n-                net.save_graph(str(output_file))\\\\\\\\n-                progress.update(save_task, completed=1)\\\\\\\\n-        else:\\\\\\\\n-            # Fallback for when rich is not available\\\\\\\\n-            logger.info(\\\\\\\\\\\\\\\"Generating visualization (basic logging)...\\\\\\\\\\\\\\\")\\\\\\\\n-            net = Network(\\\\\\\\n-                height=\\\\\\\\\\\\\\\"800px\\\\\\\\\\\\\\\", \\\\\\\\n-                width=\\\\\\\\\\\\\\\"100%\\\\\\\\\\\\\\\", \\\\\\\\n-                bgcolor=theme.color.white, \\\\\\\\n-                font_color=theme.color.text_primary,\\\\\\\\n-                heading=f\\\\\\\\\\\\\\\"EGOS Documentation Network ({timestamp})\\\\\\\\\\\\\\\"\\\\\\\\n-            )\\\\\\\\n-            \\\\\\\\n-            # Add nodes\\\\\\\\n-            for node in self.nodes:\\\\\\\\n-                net.add_node(\\\\\\\\n-                    node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"], \\\\\\\\n-                    label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"], \\\\\\\\n-                    title=node[\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\"],\\\\\\\\n-                    color=node[\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\"],\\\\\\\\n-                    size=node[\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\"],\\\\\\\\n-                    group=node[\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\"]\\\\\\\\n-                )\\\\\\\\n-            \\\\\\\\n-            # Add edges\\\\\\\\n-            for edge in self.edges:\\\\\\\\n-                net.add_edge(\\\\\\\\n-                    edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], \\\\\\\\n-                    edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"], \\\\\\\\n-                    arrows=edge.get(\\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"),\\\\\\\\n-                    title=edge.get(\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n-                )\\\\\\\\n-            \\\\\\\\n-            # Configure physics for better readability\\\\\\\\n-            net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\\\\\\\n-            net.set_options(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            {\\\\\\\\n-              \\\\\\\\\\\\\\\"physics\\\\\\\\\\\\\\\": {\\\\\\\\n-                \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\": {\\\\\\\\n-                  \\\\\\\\\\\\\\\"gravitationalConstant\\\\\\\\\\\\\\\": -100,\\\\\\\\n-                  \\\\\\\\\\\\\\\"centralGravity\\\\\\\\\\\\\\\": 0.01,\\\\\\\\n-                  \\\\\\\\\\\\\\\"springLength\\\\\\\\\\\\\\\": 200,\\\\\\\\n-                  \\\\\\\\\\\\\\\"springConstant\\\\\\\\\\\\\\\": 0.08\\\\\\\\n-                },\\\\\\\\n-                \\\\\\\\\\\\\\\"solver\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\",\\\\\\\\n-                \\\\\\\\\\\\\\\"stabilization\\\\\\\\\\\\\\\": {\\\\\\\\n-                  \\\\\\\\\\\\\\\"iterations\\\\\\\\\\\\\\\": 100\\\\\\\\n-                }\\\\\\\\n-              }\\\\\\\\n-            }\\\\\\\\n-            \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n-            \\\\\\\\n-            # Save the network\\\\\\\\n-            net.save_graph(str(output_file))\\\\\\\\n-            \\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Interactive visualization saved to: {output_file}\\\\\\\\\\\\\\\")\\\\\\\\n-        return str(output_file)\\\\\\\\n-    \\\\\\\\n-    def generate_networkx_visualization(self, output_path: Optional[str] = None) -> str:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generate a static visualization using NetworkX and matplotlib.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            output_path: Path to save the PNG file (optional)\\\\\\\\n-            \\\\\\\\n-        Returns:\\\\\\\\n-            Path to the generated PNG file\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not NETWORKX_AVAILABLE:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"NetworkX is not available. Install with: pip install networkx matplotlib\\\\\\\\\\\\\\\")\\\\\\\\n-            return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        if not self.nodes:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Create graph\\\\\\\\n-        G = nx.DiGraph()\\\\\\\\n-        \\\\\\\\n-        # Add nodes with attributes\\\\\\\\n-        for node in self.nodes:\\\\\\\\n-            G.add_node(\\\\\\\\n-                node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"],\\\\\\\\n-                label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"],\\\\\\\\n-                subsystem=node.get(\\\\\\\\\\\\\\\"subsystem\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\")\\\\\\\\n-            )\\\\\\\\n-            \\\\\\\\n-        # Add edges\\\\\\\\n-        for edge in self.edges:\\\\\\\\n-            G.add_edge(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n-            \\\\\\\\n-        # Generate output filename if not provided\\\\\\\\n-        if not output_path:\\\\\\\\n-            timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n-            output_path = self.visualization_path / f\\\\\\\\\\\\\\\"documentation_graph_{timestamp}.png\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Set up figure\\\\\\\\n-        plt.figure(figsize=(20, 20))\\\\\\\\n-        \\\\\\\\n-        # Use spring layout with seed for reproducibility\\\\\\\\n-        pos = nx.spring_layout(G, seed=42)\\\\\\\\n-        \\\\\\\\n-        # Node colors based on subsystem\\\\\\\\n-        node_colors = [self.node_colors.get(node, self.color_palette[\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\"]) for node in G.nodes()]\\\\\\\\n-        \\\\\\\\n-        # Node sizes\\\\\\\\n-        node_sizes = [self.node_sizes.get(node, 10) for node in G.nodes()]\\\\\\\\n-        \\\\\\\\n-        # Draw the graph\\\\\\\\n-        nx.draw_networkx(\\\\\\\\n-            G, \\\\\\\\n-            pos=pos,\\\\\\\\n-            with_labels=False,\\\\\\\\n-            node_color=node_colors,\\\\\\\\n-            node_size=node_sizes,\\\\\\\\n-            alpha=0.8,\\\\\\\\n-            arrows=True,\\\\\\\\n-            edge_color=theme.color.medium_grey\\\\\\\\n-        )\\\\\\\\n-        \\\\\\\\n-        # Draw labels for larger nodes only\\\\\\\\n-        large_nodes = {node: data for node, data in G.nodes(data=True) \\\\\\\\n-                      if self.node_sizes.get(node, 10) > 25}\\\\\\\\n-        if large_nodes:\\\\\\\\n-            nx.draw_networkx_labels(\\\\\\\\n-                G, \\\\\\\\n-                pos=pos,\\\\\\\\n-                labels={node: data[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"] for node, data in large_nodes.items()},\\\\\\\\n-                font_size=8\\\\\\\\n-            )\\\\\\\\n-            \\\\\\\\n-        # Save the visualization\\\\\\\\n-        plt.tight_layout()\\\\\\\\n-        plt.axis(\\\\\\\\\\\\\\\"off\\\\\\\\\\\\\\\")\\\\\\\\n-        plt.savefig(str(output_path), dpi=300, bbox_inches=\\\\\\\\\\\\\\\"tight\\\\\\\\\\\\\\\")\\\\\\\\n-        plt.close()\\\\\\\\n-        \\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Static visualization saved to: {output_path}\\\\\\\\\\\\\\\")\\\\\\\\n-        return str(output_path)\\\\\\\\n-    \\\\\\\\n-    def export_graph_data(self, output_path: Optional[str] = None) -> str:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Export graph data to JSON for use in custom visualizations.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            output_path: Path to save the JSON file (optional)\\\\\\\\n-            \\\\\\\\n-        Returns:\\\\\\\\n-            Path to the generated JSON file\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not self.nodes:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Create graph data structure\\\\\\\\n-        graph_data = {\\\\\\\\n-            \\\\\\\\\\\\\\\"nodes\\\\\\\\\\\\\\\": self.nodes,\\\\\\\\n-            \\\\\\\\\\\\\\\"edges\\\\\\\\\\\\\\\": self.edges,\\\\\\\\n-            \\\\\\\\\\\\\\\"metadata\\\\\\\\\\\\\\\": {\\\\\\\\n-                \\\\\\\\\\\\\\\"generated\\\\\\\\\\\\\\\": datetime.now().isoformat(),\\\\\\\\n-                \\\\\\\\\\\\\\\"subsystems\\\\\\\\\\\\\\\": list(sorted(self.subsystems)),\\\\\\\\n-                \\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(self.nodes),\\\\\\\\n-                \\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(self.edges)\\\\\\\\n-            }\\\\\\\\n-        }\\\\\\\\n-        \\\\\\\\n-        # Generate output filename if not provided\\\\\\\\n-        if not output_path:\\\\\\\\n-            timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n-            output_path = self.visualization_path / f\\\\\\\\\\\\\\\"documentation_graph_data_{timestamp}.json\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Save the graph data\\\\\\\\n-        with open(output_path, 'w', encoding='utf-8') as f:\\\\\\\\n-            json.dump(graph_data, f, indent=2)\\\\\\\\n-            \\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Graph data exported to: {output_path}\\\\\\\\\\\\\\\")\\\\\\\\n-        return str(output_path)\\\\\\\\n-    \\\\\\\\n-    def analyze_graph(self) -> Dict[str, Any]:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Analyze the graph structure and generate metrics.\\\\\\\\n-        \\\\\\\\n-        Returns:\\\\\\\\n-            Dictionary of graph metrics and insights\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not self.nodes or not self.edges:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return {}\\\\\\\\n-            \\\\\\\\n-        # If NetworkX is available, use it for analysis\\\\\\\\n-        if NETWORKX_AVAILABLE:\\\\\\\\n-            return self._analyze_with_networkx()\\\\\\\\n-        else:\\\\\\\\n-            return self._analyze_basic()\\\\\\\\n-    \\\\\\\\n-    def _analyze_with_networkx(self) -> Dict[str, Any]:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Analyze graph using NetworkX metrics.\\\\\\\\n-        \\\\\\\\n-        Returns:\\\\\\\\n-            Dictionary of graph metrics\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        # Create graph\\\\\\\\n-        G = nx.DiGraph()\\\\\\\\n-        \\\\\\\\n-        # Add nodes\\\\\\\\n-        for node in self.nodes:\\\\\\\\n-            G.add_node(node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"])\\\\\\\\n-            \\\\\\\\n-        # Add edges\\\\\\\\n-        for edge in self.edges:\\\\\\\\n-            G.add_edge(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n-            \\\\\\\\n-        # Calculate metrics\\\\\\\\n-        metrics = {\\\\\\\\n-            \\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(G),\\\\\\\\n-            \\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(G.edges()),\\\\\\\\n-            \\\\\\\\\\\\\\\"density\\\\\\\\\\\\\\\": nx.density(G),\\\\\\\\n-        }\\\\\\\\n-        \\\\\\\\n-        # Calculate centrality measures\\\\\\\\n-        try:\\\\\\\\n-            in_degree_centrality = nx.in_degree_centrality(G)\\\\\\\\n-            out_degree_centrality = nx.out_degree_centrality(G)\\\\\\\\n-            betweenness_centrality = nx.betweenness_centrality(G)\\\\\\\\n-            \\\\\\\\n-            # Sort by value to find highest centrality nodes\\\\\\\\n-            top_in_degree = sorted(in_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n-            top_out_degree = sorted(out_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n-            top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n-            \\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_in_degree]\\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"most_referencing_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_out_degree]\\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_betweenness]\\\\\\\\n-        except:\\\\\\\\n-            logger.warning(\\\\\\\\\\\\\\\"Error calculating centrality measures\\\\\\\\\\\\\\\")\\\\\\\\n-            \\\\\\\\n-        # Find isolated nodes (no connections)\\\\\\\\n-        isolated = list(nx.isolates(G))\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"] = isolated\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\"] = len(isolated)\\\\\\\\n-        \\\\\\\\n-        # Find strongly connected components\\\\\\\\n-        strongly_connected = list(nx.strongly_connected_components(G))\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"strongly_connected_components\\\\\\\\\\\\\\\"] = len(strongly_connected)\\\\\\\\n-        \\\\\\\\n-        # Find largest strongly connected component\\\\\\\\n-        if strongly_connected:\\\\\\\\n-            largest_component = max(strongly_connected, key=len)\\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"largest_component_size\\\\\\\\\\\\\\\"] = len(largest_component)\\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"largest_component_ratio\\\\\\\\\\\\\\\"] = len(largest_component) / len(G)\\\\\\\\n-        \\\\\\\\n-        return metrics\\\\\\\\n-    \\\\\\\\n-    def _analyze_basic(self) -> Dict[str, Any]:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Perform basic graph analysis without NetworkX.\\\\\\\\n-        \\\\\\\\n-        Returns:\\\\\\\\n-            Dictionary of basic graph metrics\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        # Count nodes and edges\\\\\\\\n-        metrics = {\\\\\\\\n-            \\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(self.nodes),\\\\\\\\n-            \\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(self.edges),\\\\\\\\n-        }\\\\\\\\n-        \\\\\\\\n-        # Count nodes by subsystem\\\\\\\\n-        subsystem_counts = {}\\\\\\\\n-        for node in self.nodes:\\\\\\\\n-            subsystem = node.get(\\\\\\\\\\\\\\\"subsystem\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\")\\\\\\\\n-            subsystem_counts[subsystem] = subsystem_counts.get(subsystem, 0) + 1\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"subsystem_counts\\\\\\\\\\\\\\\"] = subsystem_counts\\\\\\\\n-        \\\\\\\\n-        # Find nodes with most connections\\\\\\\\n-        node_connections = {}\\\\\\\\n-        for edge in self.edges:\\\\\\\\n-            source = edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"]\\\\\\\\n-            target = edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"]\\\\\\\\n-            \\\\\\\\n-            # Count outgoing connections\\\\\\\\n-            node_connections.setdefault(source, {\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\": 0, \\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\": 0})\\\\\\\\n-            node_connections[source][\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\"] += 1\\\\\\\\n-            \\\\\\\\n-            # Count incoming connections\\\\\\\\n-            node_connections.setdefault(target, {\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\": 0, \\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\": 0})\\\\\\\\n-            node_connections[target][\\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\"] += 1\\\\\\\\n-            \\\\\\\\n-        # Find most referenced documents\\\\\\\\n-        most_referenced = sorted(\\\\\\\\n-            [(node, data[\\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\"]) for node, data in node_connections.items()],\\\\\\\\n-            key=lambda x: x[1],\\\\\\\\n-            reverse=True\\\\\\\\n-        )[:10]\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"count\\\\\\\\\\\\\\\": count} for doc, count in most_referenced]\\\\\\\\n-        \\\\\\\\n-        # Find most referencing documents\\\\\\\\n-        most_referencing = sorted(\\\\\\\\n-            [(node, data[\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\"]) for node, data in node_connections.items()],\\\\\\\\n-            key=lambda x: x[1],\\\\\\\\n-            reverse=True\\\\\\\\n-        )[:10]\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"most_referencing_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"count\\\\\\\\\\\\\\\": count} for doc, count in most_referencing]\\\\\\\\n-        \\\\\\\\n-        # Find isolated documents (no connections)\\\\\\\\n-        connected_nodes = set()\\\\\\\\n-        for edge in self.edges:\\\\\\\\n-            connected_nodes.add(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"])\\\\\\\\n-            connected_nodes.add(edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n-            \\\\\\\\n-        all_node_ids = set(node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"] for node in self.nodes)\\\\\\\\n-        isolated = all_node_ids - connected_nodes\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"] = list(isolated)\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\"] = len(isolated)\\\\\\\\n-        \\\\\\\\n-        return metrics\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Graph built successfully: {len(self.nodes)} nodes, {len(self.edges)} edges.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+def generate_pyvis_visualization(self, output_path: Optional[str] = None) -> str:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generate an interactive HTML visualization using Pyvis.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+output_path: Path to save the HTML file (optional)\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Path to the generated HTML file\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not hasattr(self, 'nodes') or not self.nodes:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No graph data to visualize. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Create network\\\\\\\\n+timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n+output_filename = f\\\\\\\\\\\\\\\"documentation_graph_{timestamp}.html\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+if output_path:\\\\\\\\n+output_file = Path(output_path)\\\\\\\\n+else:\\\\\\\\n+output_file = self.visualization_path / output_filename\\\\\\\\n+\\\\\\\\n+# Log the start of visualization generation\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Generating interactive visualization ({len(self.nodes)} nodes, {len(self.edges)} edges)...\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+if console:\\\\\\\\n+with Progress(\\\\\\\\n+SpinnerColumn(),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"[progress.description]{task.description}\\\\\\\\\\\\\\\"),\\\\\\\\n+BarColumn(),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"[progress.percentage]{task.percentage:>3.0f}%\\\\\\\\\\\\\\\"),\\\\\\\\n+TimeRemainingColumn(),\\\\\\\\n+console=console,\\\\\\\\n+transient=False, # Keep progress visible after completion\\\\\\\\n+) as progress:\\\\\\\\n+# Create network\\\\\\\\n+net_task = progress.add_task(\\\\\\\\\\\\\\\"Creating network...\\\\\\\\\\\\\\\", total=1)\\\\\\\\n+net = Network(\\\\\\\\n+height=\\\\\\\\\\\\\\\"800px\\\\\\\\\\\\\\\",\\\\\\\\n+width=\\\\\\\\\\\\\\\"100%\\\\\\\\\\\\\\\",\\\\\\\\n+bgcolor=theme.color.white,\\\\\\\\n+font_color=theme.color.text_primary,\\\\\\\\n+heading=f\\\\\\\\\\\\\\\"EGOS Documentation Network ({timestamp})\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+progress.update(net_task, completed=1)\\\\\\\\n+\\\\\\\\n+# Add nodes with progress\\\\\\\\n+node_task = progress.add_task(\\\\\\\\\\\\\\\"Adding nodes...\\\\\\\\\\\\\\\", total=len(self.nodes))\\\\\\\\n+for node in self.nodes:\\\\\\\\n+net.add_node(\\\\\\\\n+node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"],\\\\\\\\n+label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"],\\\\\\\\n+title=node[\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\"],\\\\\\\\n+color=node[\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\"],\\\\\\\\n+size=node[\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\"],\\\\\\\\n+group=node[\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\"]\\\\\\\\n+)\\\\\\\\n+progress.update(node_task, advance=1)\\\\\\\\n+\\\\\\\\n+# Add edges with progress\\\\\\\\n+edge_task = progress.add_task(\\\\\\\\\\\\\\\"Adding edges...\\\\\\\\\\\\\\\", total=len(self.edges))\\\\\\\\n+for edge in self.edges:\\\\\\\\n+net.add_edge(\\\\\\\\n+edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"],\\\\\\\\n+edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"],\\\\\\\\n+arrows=edge.get(\\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"),\\\\\\\\n+title=edge.get(\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n+)\\\\\\\\n+progress.update(edge_task, advance=1)\\\\\\\\n+\\\\\\\\n+# Configure physics for better readability\\\\\\\\n+save_task = progress.add_task(\\\\\\\\\\\\\\\"Configuring and saving...\\\\\\\\\\\\\\\", total=1)\\\\\\\\n+net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\\\\\\\n+net.set_options(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+{\\\\\\\\n+\\\\\\\\\\\\\\\"physics\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"gravitationalConstant\\\\\\\\\\\\\\\": -100,\\\\\\\\n+\\\\\\\\\\\\\\\"centralGravity\\\\\\\\\\\\\\\": 0.01,\\\\\\\\n+\\\\\\\\\\\\\\\"springLength\\\\\\\\\\\\\\\": 200,\\\\\\\\n+\\\\\\\\\\\\\\\"springConstant\\\\\\\\\\\\\\\": 0.08\\\\\\\\n+},\\\\\\\\n+\\\\\\\\\\\\\\\"solver\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\",\\\\\\\\n+\\\\\\\\\\\\\\\"stabilization\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"iterations\\\\\\\\\\\\\\\": 100\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Save the network\\\\\\\\n+net.save_graph(str(output_file))\\\\\\\\n+progress.update(save_task, completed=1)\\\\\\\\n+else:\\\\\\\\n+# Fallback for when rich is not available\\\\\\\\n+logger.info(\\\\\\\\\\\\\\\"Generating visualization (basic logging)...\\\\\\\\\\\\\\\")\\\\\\\\n+net = Network(\\\\\\\\n+height=\\\\\\\\\\\\\\\"800px\\\\\\\\\\\\\\\",\\\\\\\\n+width=\\\\\\\\\\\\\\\"100%\\\\\\\\\\\\\\\",\\\\\\\\n+bgcolor=theme.color.white,\\\\\\\\n+font_color=theme.color.text_primary,\\\\\\\\n+heading=f\\\\\\\\\\\\\\\"EGOS Documentation Network ({timestamp})\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Add nodes\\\\\\\\n+for node in self.nodes:\\\\\\\\n+net.add_node(\\\\\\\\n+node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"],\\\\\\\\n+label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"],\\\\\\\\n+title=node[\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\"],\\\\\\\\n+color=node[\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\"],\\\\\\\\n+size=node[\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\"],\\\\\\\\n+group=node[\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\"]\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Add edges\\\\\\\\n+for edge in self.edges:\\\\\\\\n+net.add_edge(\\\\\\\\n+edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"],\\\\\\\\n+edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"],\\\\\\\\n+arrows=edge.get(\\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"),\\\\\\\\n+title=edge.get(\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Configure physics for better readability\\\\\\\\n+net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\\\\\\\n+net.set_options(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+{\\\\\\\\n+\\\\\\\\\\\\\\\"physics\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"gravitationalConstant\\\\\\\\\\\\\\\": -100,\\\\\\\\n+\\\\\\\\\\\\\\\"centralGravity\\\\\\\\\\\\\\\": 0.01,\\\\\\\\n+\\\\\\\\\\\\\\\"springLength\\\\\\\\\\\\\\\": 200,\\\\\\\\n+\\\\\\\\\\\\\\\"springConstant\\\\\\\\\\\\\\\": 0.08\\\\\\\\n+},\\\\\\\\n+\\\\\\\\\\\\\\\"solver\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\",\\\\\\\\n+\\\\\\\\\\\\\\\"stabilization\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"iterations\\\\\\\\\\\\\\\": 100\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Save the network\\\\\\\\n+net.save_graph(str(output_file))\\\\\\\\n+\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Interactive visualization saved to: {output_file}\\\\\\\\\\\\\\\")\\\\\\\\n+return str(output_file)\\\\\\\\n+\\\\\\\\n+def generate_networkx_visualization(self, output_path: Optional[str] = None) -> str:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generate a static visualization using NetworkX and matplotlib.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+output_path: Path to save the PNG file (optional)\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Path to the generated PNG file\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not NETWORKX_AVAILABLE:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"NetworkX is not available. Install with: pip install networkx matplotlib\\\\\\\\\\\\\\\")\\\\\\\\n+return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+if not self.nodes:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Create graph\\\\\\\\n+G = nx.DiGraph()\\\\\\\\n+\\\\\\\\n+# Add nodes with attributes\\\\\\\\n+for node in self.nodes:\\\\\\\\n+G.add_node(\\\\\\\\n+node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"],\\\\\\\\n+label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"],\\\\\\\\n+subsystem=node.get(\\\\\\\\\\\\\\\"subsystem\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\")\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Add edges\\\\\\\\n+for edge in self.edges:\\\\\\\\n+G.add_edge(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+# Generate output filename if not provided\\\\\\\\n+if not output_path:\\\\\\\\n+timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n+output_path = self.visualization_path / f\\\\\\\\\\\\\\\"documentation_graph_{timestamp}.png\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Set up figure\\\\\\\\n+plt.figure(figsize=(20, 20))\\\\\\\\n+\\\\\\\\n+# Use spring layout with seed for reproducibility\\\\\\\\n+pos = nx.spring_layout(G, seed=42)\\\\\\\\n+\\\\\\\\n+# Node colors based on subsystem\\\\\\\\n+node_colors = [self.node_colors.get(node, self.color_palette[\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\"]) for node in G.nodes()]\\\\\\\\n+\\\\\\\\n+# Node sizes\\\\\\\\n+node_sizes = [self.node_sizes.get(node, 10) for node in G.nodes()]\\\\\\\\n+\\\\\\\\n+# Draw the graph\\\\\\\\n+nx.draw_networkx(\\\\\\\\n+G,\\\\\\\\n+pos=pos,\\\\\\\\n+with_labels=False,\\\\\\\\n+node_color=node_colors,\\\\\\\\n+node_size=node_sizes,\\\\\\\\n+alpha=0.8,\\\\\\\\n+arrows=True,\\\\\\\\n+edge_color=theme.color.medium_grey\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Draw labels for larger nodes only\\\\\\\\n+large_nodes = {node: data for node, data in G.nodes(data=True)\\\\\\\\n+if self.node_sizes.get(node, 10) > 25}\\\\\\\\n+if large_nodes:\\\\\\\\n+nx.draw_networkx_labels(\\\\\\\\n+G,\\\\\\\\n+pos=pos,\\\\\\\\n+labels={node: data[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"] for node, data in large_nodes.items()},\\\\\\\\n+font_size=8\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Save the visualization\\\\\\\\n+plt.tight_layout()\\\\\\\\n+plt.axis(\\\\\\\\\\\\\\\"off\\\\\\\\\\\\\\\")\\\\\\\\n+plt.savefig(str(output_path), dpi=300, bbox_inches=\\\\\\\\\\\\\\\"tight\\\\\\\\\\\\\\\")\\\\\\\\n+plt.close()\\\\\\\\n+\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Static visualization saved to: {output_path}\\\\\\\\\\\\\\\")\\\\\\\\n+return str(output_path)\\\\\\\\n+\\\\\\\\n+def export_graph_data(self, output_path: Optional[str] = None) -> str:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Export graph data to JSON for use in custom visualizations.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+output_path: Path to save the JSON file (optional)\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Path to the generated JSON file\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not self.nodes:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Create graph data structure\\\\\\\\n+graph_data = {\\\\\\\\n+\\\\\\\\\\\\\\\"nodes\\\\\\\\\\\\\\\": self.nodes,\\\\\\\\n+\\\\\\\\\\\\\\\"edges\\\\\\\\\\\\\\\": self.edges,\\\\\\\\n+\\\\\\\\\\\\\\\"metadata\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"generated\\\\\\\\\\\\\\\": datetime.now().isoformat(),\\\\\\\\n+\\\\\\\\\\\\\\\"subsystems\\\\\\\\\\\\\\\": list(sorted(self.subsystems)),\\\\\\\\n+\\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(self.nodes),\\\\\\\\n+\\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(self.edges)\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+\\\\\\\\n+# Generate output filename if not provided\\\\\\\\n+if not output_path:\\\\\\\\n+timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n+output_path = self.visualization_path / f\\\\\\\\\\\\\\\"documentation_graph_data_{timestamp}.json\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Save the graph data\\\\\\\\n+with open(output_path, 'w', encoding='utf-8') as f:\\\\\\\\n+json.dump(graph_data, f, indent=2)\\\\\\\\n+\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Graph data exported to: {output_path}\\\\\\\\\\\\\\\")\\\\\\\\n+return str(output_path)\\\\\\\\n+\\\\\\\\n+def analyze_graph(self) -> Dict[str, Any]:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Analyze the graph structure and generate metrics.\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Dictionary of graph metrics and insights\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not self.nodes or not self.edges:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return {}\\\\\\\\n+\\\\\\\\n+# If NetworkX is available, use it for analysis\\\\\\\\n+if NETWORKX_AVAILABLE:\\\\\\\\n+return self._analyze_with_networkx()\\\\\\\\n+else:\\\\\\\\n+return self._analyze_basic()\\\\\\\\n+\\\\\\\\n+def _analyze_with_networkx(self) -> Dict[str, Any]:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Analyze graph using NetworkX metrics.\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Dictionary of graph metrics\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+# Create graph\\\\\\\\n+G = nx.DiGraph()\\\\\\\\n+\\\\\\\\n+# Add nodes\\\\\\\\n+for node in self.nodes:\\\\\\\\n+G.add_node(node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+# Add edges\\\\\\\\n+for edge in self.edges:\\\\\\\\n+G.add_edge(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+# Calculate metrics\\\\\\\\n+metrics = {\\\\\\\\n+\\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(G),\\\\\\\\n+\\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(G.edges()),\\\\\\\\n+\\\\\\\\\\\\\\\"density\\\\\\\\\\\\\\\": nx.density(G),\\\\\\\\n+}\\\\\\\\n+\\\\\\\\n+# Calculate centrality measures\\\\\\\\n+try:\\\\\\\\n+in_degree_centrality = nx.in_degree_centrality(G)\\\\\\\\n+out_degree_centrality = nx.out_degree_centrality(G)\\\\\\\\n+betweenness_centrality = nx.betweenness_centrality(G)\\\\\\\\n+\\\\\\\\n+# Sort by value to find highest centrality nodes\\\\\\\\n+top_in_degree = sorted(in_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n+top_out_degree = sorted(out_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n+top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n+\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_in_degree]\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"most_referencing_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_out_degree]\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_betweenness]\\\\\\\\n+except:\\\\\\\\n+logger.warning(\\\\\\\\\\\\\\\"Error calculating centrality measures\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Find isolated nodes (no connections)\\\\\\\\n+isolated = list(nx.isolates(G))\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"] = isolated\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\"] = len(isolated)\\\\\\\\n+\\\\\\\\n+# Find strongly connected components\\\\\\\\n+strongly_connected = list(nx.strongly_connected_components(G))\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"strongly_connected_components\\\\\\\\\\\\\\\"] = len(strongly_connected)\\\\\\\\n+\\\\\\\\n+# Find largest strongly connected component\\\\\\\\n+if strongly_connected:\\\\\\\\n+largest_component = max(strongly_connected, key=len)\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"largest_component_size\\\\\\\\\\\\\\\"] = len(largest_component)\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"largest_component_ratio\\\\\\\\\\\\\\\"] = len(largest_component) / len(G)\\\\\\\\n+\\\\\\\\n+return metrics\\\\\\\\n+\\\\\\\\n+def _analyze_basic(self) -> Dict[str, Any]:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Perform basic graph analysis without NetworkX.\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Dictionary of basic graph metrics\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+# Count nodes and edges\\\\\\\\n+metrics = {\\\\\\\\n+\\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(self.nodes),\\\\\\\\n+\\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(self.edges),\\\\\\\\n+}\\\\\\\\n+\\\\\\\\n+# Count nodes by subsystem\\\\\\\\n+subsystem_counts = {}\\\\\\\\n+for node in self.nodes:\\\\\\\\n+subsystem = node.get(\\\\\\\\\\\\\\\"subsystem\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\")\\\\\\\\n+subsystem_counts[subsystem] = subsystem_counts.get(subsystem, 0) + 1\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"subsystem_counts\\\\\\\\\\\\\\\"] = subsystem_counts\\\\\\\\n+\\\\\\\\n+# Find nodes with most connections\\\\\\\\n+node_connections = {}\\\\\\\\n+for edge in self.edges:\\\\\\\\n+source = edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"]\\\\\\\\n+target = edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"]\\\\\\\\n+\\\\\\\\n+# Count outgoing connections\\\\\\\\n+node_connections.setdefault(source, {\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\": 0, \\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\": 0})\\\\\\\\n+node_connections[source][\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\"] += 1\\\\\\\\n+\\\\\\\\n+# Count incoming connections\\\\\\\\n+node_connections.setdefault(target, {\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\": 0, \\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\": 0})\\\\\\\\n+node_connections[target][\\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\"] += 1\\\\\\\\n+\\\\\\\\n+# Find most referenced documents\\\\\\\\n+most_referenced = sorted(\\\\\\\\n+[(node, data[\\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\"]) for node, data in node_connections.items()],\\\\\\\\n+key=lambda x: x[1],\\\\\\\\n+reverse=True\\\\\\\\n+)[:10]\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"count\\\\\\\\\\\\\\\": count} for doc, count in most_referenced]\\\\\\\\n+\\\\\\\\n+# Find most referencing documents\\\\\\\\n+most_referencing = sorted(\\\\\\\\n+[(node, data[\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\"]) for node, data in node_connections.items()],\\\\\\\\n+key=lambda x: x[1],\\\\\\\\n+reverse=True\\\\\\\\n+)[:10]\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"most_referencing_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"count\\\\\\\\\\\\\\\": count} for doc, count in most_referencing]\\\\\\\\n+\\\\\\\\n+# Find isolated documents (no connections)\\\\\\\\n+connected_nodes = set()\\\\\\\\n+for edge in self.edges:\\\\\\\\n+connected_nodes.add(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"])\\\\\\\\n+connected_nodes.add(edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+all_node_ids = set(node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"] for node in self.nodes)\\\\\\\\n+isolated = all_node_ids - connected_nodes\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"] = list(isolated)\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\"] = len(isolated)\\\\\\\\n+\\\\\\\\n+return metrics\\\\\\\\n \\\\\\\\n \\\\\\\\n def main():\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Main entry point for the documentation graph generator.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    parser = argparse.ArgumentParser(\\\\\\\\n-        description=\\\\\\\\\\\\\\\"Generate interactive visualization of EGOS documentation connections\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--base-path\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-b\\\\\\\\\\\\\\\", default=\\\\\\\\\\\\\\\".\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Base path of the EGOS project\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--report\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-r\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Path to the cross-reference analysis JSON report\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--output\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-o\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Path to save the visualization\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--format\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-f\\\\\\\\\\\\\\\", choices=[\\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"], default=\\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Output format (default: all available formats)\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--analyze\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-a\\\\\\\\\\\\\\\", action=\\\\\\\\\\\\\\\"store_true\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Analyze graph and print metrics\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    \\\\\\\\n-    args = parser.parse_args()\\\\\\\\n-    \\\\\\\\n-    try:\\\\\\\\n-        # Create graph generator\\\\\\\\n-        generator = DocumentationGraphGenerator(args.base_path)\\\\\\\\n-        \\\\\\\\n-        # Load report data\\\\\\\\n-        if not generator.load_report_data(args.report):\\\\\\\\n-            return 1\\\\\\\\n-            \\\\\\\\n-        # --- Start of new try block for core logic ---\\\\\\\\n-        try:\\\\\\\\n-            # Build graph\\\\\\\\n-            generator.build_graph()\\\\\\\\n-            \\\\\\\\n-            # Generate visualizations based on format\\\\\\\\n-            if args.format in [\\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"] and PYVIS_AVAILABLE:\\\\\\\\n-                generator.generate_pyvis_visualization(\\\\\\\\n-                    args.output if args.format == \\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\" else None\\\\\\\\n-                )\\\\\\\\n-                \\\\\\\\n-            if args.format in [\\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"] and NETWORKX_AVAILABLE:\\\\\\\\n-                generator.generate_networkx_visualization(\\\\\\\\n-                    args.output if args.format == \\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\" else None\\\\\\\\n-                )\\\\\\\\n-                \\\\\\\\n-            if args.format in [\\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"]:\\\\\\\\n-                generator.export_graph_data(\\\\\\\\n-                    args.output if args.format == \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\" else None\\\\\\\\n-                )\\\\\\\\n-                \\\\\\\\n-            # Analyze graph if requested\\\\\\\\n-            if args.analyze:\\\\\\\\n-                analysis = generator.analyze_graph()\\\\\\\\n-                \\\\\\\\n-                # Print analysis results using rich tables if available\\\\\\\\n-                console.print(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n [bold cyan]DOCUMENTATION GRAPH ANALYSIS[/bold cyan]\\\\\\\\\\\\\\\")\\\\\\\\n-                \\\\\\\\n-                summary_table = Table(title=\\\\\\\\\\\\\\\"Graph Summary\\\\\\\\\\\\\\\", show_header=False, box=None)\\\\\\\\n-                summary_table.add_row(\\\\\\\\\\\\\\\"Nodes:\\\\\\\\\\\\\\\", str(analysis.get('node_count', 'N/A')))\\\\\\\\n-                summary_table.add_row(\\\\\\\\\\\\\\\"Edges:\\\\\\\\\\\\\\\", str(analysis.get('edge_count', 'N/A')))\\\\\\\\n-                if \\\\\\\\\\\\\\\"density\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n-                    summary_table.add_row(\\\\\\\\\\\\\\\"Graph Density:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"{analysis['density']:.4f}\\\\\\\\\\\\\\\")\\\\\\\\n-                if \\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n-                    summary_table.add_row(\\\\\\\\\\\\\\\"Isolated Documents:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"[yellow]{analysis['isolated_count']}[/yellow]\\\\\\\\\\\\\\\")\\\\\\\\n-                if \\\\\\\\\\\\\\\"strongly_connected_components\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n-                    summary_table.add_row(\\\\\\\\\\\\\\\"Strongly Connected Components:\\\\\\\\\\\\\\\", str(analysis['strongly_connected_components']))\\\\\\\\n-                if \\\\\\\\\\\\\\\"largest_component_size\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n-                    ratio = analysis.get('largest_component_ratio', 0)\\\\\\\\n-                    summary_table.add_row(\\\\\\\\\\\\\\\"Largest Component Size:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"{analysis['largest_component_size']} ({ratio:.1%})\\\\\\\\\\\\\\\")\\\\\\\\n-                console.print(summary_table)\\\\\\\\n-\\\\\\\\n-                # --- Most Referenced Documents Table ---\\\\\\\\n-                if \\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n-                    ref_table = Table(title=\\\\\\\\\\\\\\\" Most Referenced Documents (Top 5)\\\\\\\\\\\\\\\", expand=True)\\\\\\\\n-                    ref_table.add_column(\\\\\\\\\\\\\\\"Rank\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"dim\\\\\\\\\\\\\\\", width=5)\\\\\\\\n-                    ref_table.add_column(\\\\\\\\\\\\\\\"Document\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"magenta\\\\\\\\\\\\\\\")\\\\\\\\n-                    ref_table.add_column(\\\\\\\\\\\\\\\"Score/Count\\\\\\\\\\\\\\\", justify=\\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"green\\\\\\\\\\\\\\\")\\\\\\\\n-                    \\\\\\\\n-                    for i, doc_info in enumerate(analysis[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"][:5], 1):\\\\\\\\n-                        score = doc_info.get('score', doc_info.get('count', 'N/A'))\\\\\\\\n-                        score_str = f\\\\\\\\\\\\\\\"{score:.3f}\\\\\\\\\\\\\\\" if isinstance(score, float) else str(score)\\\\\\\\n-                        ref_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\\\\\\\n-                    console.print(ref_table)\\\\\\\\n-\\\\\\\\n-                # --- Bridge Documents Table ---\\\\\\\\n-                if \\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n-                    bridge_table = Table(title=\\\\\\\\\\\\\\\" Bridge Documents (High Betweenness - Top 5)\\\\\\\\\\\\\\\", expand=True)\\\\\\\\n-                    bridge_table.add_column(\\\\\\\\\\\\\\\"Rank\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"dim\\\\\\\\\\\\\\\", width=5)\\\\\\\\n-                    bridge_table.add_column(\\\\\\\\\\\\\\\"Document\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"cyan\\\\\\\\\\\\\\\")\\\\\\\\n-                    bridge_table.add_column(\\\\\\\\\\\\\\\"Score\\\\\\\\\\\\\\\", justify=\\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"blue\\\\\\\\\\\\\\\")\\\\\\\\n-                    \\\\\\\\n-                    for i, doc_info in enumerate(analysis[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"][:5], 1):\\\\\\\\n-                        score = doc_info.get('score', 'N/A')\\\\\\\\n-                        score_str = f\\\\\\\\\\\\\\\"{score:.3f}\\\\\\\\\\\\\\\" if isinstance(score, float) else str(score)\\\\\\\\n-                        bridge_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\\\\\\\n-                    console.print(bridge_table)\\\\\\\\n-\\\\\\\\n-                # --- Isolated Documents --- \\\\\\\\n-                if \\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n-                     console.print(f\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n [yellow]Isolated Documents ({analysis['isolated_count']}):[/yellow]\\\\\\\\\\\\\\\")\\\\\\\\n-                     # Print only first few for brevity\\\\\\\\n-                     for doc in analysis['isolated_docs'][:10]:\\\\\\\\n-                         console.print(f\\\\\\\\\\\\\\\"  - {os.path.basename(doc)}\\\\\\\\\\\\\\\")\\\\\\\\n-                     if len(analysis['isolated_docs']) > 10:\\\\\\\\n-                         console.print(f\\\\\\\\\\\\\\\"  ... and {len(analysis['isolated_docs']) - 10} more\\\\\\\\\\\\\\\")\\\\\\\\n-                \\\\\\\\n-                # Save analysis to JSON\\\\\\\\n-                analysis_path = generator.visualization_path / \\\\\\\\\\\\\\\"documentation_graph_analysis.json\\\\\\\\\\\\\\\"\\\\\\\\n-                with open(analysis_path, 'w', encoding='utf-8') as f:\\\\\\\\n-                    json.dump(analysis, f, indent=2)\\\\\\\\n-                logger.info(f\\\\\\\\\\\\\\\"Analysis saved to: {analysis_path}\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-        # --- End of new try block, add except block ---\\\\\\\\n-        except Exception as core_error:\\\\\\\\n-            logger.error(f\\\\\\\\\\\\\\\"!!! Critical Error during graph processing/visualization: {core_error}\\\\\\\\\\\\\\\")\\\\\\\\n-            logger.error(traceback.format_exc())\\\\\\\\n-            return 1 # Ensure exit with error code\\\\\\\\n-                \\\\\\\\n-        return 0\\\\\\\\n-            \\\\\\\\n-    # Outer except block remains for initialization/loading errors\\\\\\\\n-    except Exception as e:\\\\\\\\n-        logger.error(f\\\\\\\\\\\\\\\"Initialization or Loading Error: {str(e)}\\\\\\\\\\\\\\\")\\\\\\\\n-        # Ensure traceback is imported if needed here too\\\\\\\\n-        if 'traceback' not in sys.modules:\\\\\\\\n-            import traceback\\\\\\\\n-        logger.error(traceback.format_exc())\\\\\\\\n-        return 1\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Main entry point for the documentation graph generator.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+parser = argparse.ArgumentParser(\\\\\\\\n+description=\\\\\\\\\\\\\\\"Generate interactive visualization of EGOS documentation connections\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--base-path\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-b\\\\\\\\\\\\\\\", default=\\\\\\\\\\\\\\\".\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Base path of the EGOS project\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--report\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-r\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Path to the cross-reference analysis JSON report\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--output\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-o\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Path to save the visualization\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--format\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-f\\\\\\\\\\\\\\\", choices=[\\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"], default=\\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Output format (default: all available formats)\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--analyze\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-a\\\\\\\\\\\\\\\", action=\\\\\\\\\\\\\\\"store_true\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Analyze graph and print metrics\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+args = parser.parse_args()\\\\\\\\n+\\\\\\\\n+try:\\\\\\\\n+# Create graph generator\\\\\\\\n+generator = DocumentationGraphGenerator(args.base_path)\\\\\\\\n+\\\\\\\\n+# Load report data\\\\\\\\n+if not generator.load_report_data(args.report):\\\\\\\\n+return 1\\\\\\\\n+\\\\\\\\n+# --- Start of new try block for core logic ---\\\\\\\\n+try:\\\\\\\\n+# Build graph\\\\\\\\n+generator.build_graph()\\\\\\\\n+\\\\\\\\n+# Generate visualizations based on format\\\\\\\\n+if args.format in [\\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"] and PYVIS_AVAILABLE:\\\\\\\\n+generator.generate_pyvis_visualization(\\\\\\\\n+args.output if args.format == \\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\" else None\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+if args.format in [\\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"] and NETWORKX_AVAILABLE:\\\\\\\\n+generator.generate_networkx_visualization(\\\\\\\\n+args.output if args.format == \\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\" else None\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+if args.format in [\\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"]:\\\\\\\\n+generator.export_graph_data(\\\\\\\\n+args.output if args.format == \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\" else None\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Analyze graph if requested\\\\\\\\n+if args.analyze:\\\\\\\\n+analysis = generator.analyze_graph()\\\\\\\\n+\\\\\\\\n+# Print analysis results using rich tables if available\\\\\\\\n+console.print(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n [bold cyan]DOCUMENTATION GRAPH ANALYSIS[/bold cyan]\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+summary_table = Table(title=\\\\\\\\\\\\\\\"Graph Summary\\\\\\\\\\\\\\\", show_header=False, box=None)\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Nodes:\\\\\\\\\\\\\\\", str(analysis.get('node_count', 'N/A')))\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Edges:\\\\\\\\\\\\\\\", str(analysis.get('edge_count', 'N/A')))\\\\\\\\n+if \\\\\\\\\\\\\\\"density\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Graph Density:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"{analysis['density']:.4f}\\\\\\\\\\\\\\\")\\\\\\\\n+if \\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Isolated Documents:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"[yellow]{analysis['isolated_count']}[/yellow]\\\\\\\\\\\\\\\")\\\\\\\\n+if \\\\\\\\\\\\\\\"strongly_connected_components\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Strongly Connected Components:\\\\\\\\\\\\\\\", str(analysis['strongly_connected_components']))\\\\\\\\n+if \\\\\\\\\\\\\\\"largest_component_size\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n+ratio = analysis.get('largest_component_ratio', 0)\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Largest Component Size:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"{analysis['largest_component_size']} ({ratio:.1%})\\\\\\\\\\\\\\\")\\\\\\\\n+console.print(summary_table)\\\\\\\\n+\\\\\\\\n+# --- Most Referenced Documents Table ---\\\\\\\\n+if \\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n+ref_table = Table(title=\\\\\\\\\\\\\\\" Most Referenced Documents (Top 5)\\\\\\\\\\\\\\\", expand=True)\\\\\\\\n+ref_table.add_column(\\\\\\\\\\\\\\\"Rank\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"dim\\\\\\\\\\\\\\\", width=5)\\\\\\\\n+ref_table.add_column(\\\\\\\\\\\\\\\"Document\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"magenta\\\\\\\\\\\\\\\")\\\\\\\\n+ref_table.add_column(\\\\\\\\\\\\\\\"Score/Count\\\\\\\\\\\\\\\", justify=\\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"green\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+for i, doc_info in enumerate(analysis[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"][:5], 1):\\\\\\\\n+score = doc_info.get('score', doc_info.get('count', 'N/A'))\\\\\\\\n+score_str = f\\\\\\\\\\\\\\\"{score:.3f}\\\\\\\\\\\\\\\" if isinstance(score, float) else str(score)\\\\\\\\n+ref_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\\\\\\\n+console.print(ref_table)\\\\\\\\n+\\\\\\\\n+# --- Bridge Documents Table ---\\\\\\\\n+if \\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n+bridge_table = Table(title=\\\\\\\\\\\\\\\" Bridge Documents (High Betweenness - Top 5)\\\\\\\\\\\\\\\", expand=True)\\\\\\\\n+bridge_table.add_column(\\\\\\\\\\\\\\\"Rank\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"dim\\\\\\\\\\\\\\\", width=5)\\\\\\\\n+bridge_table.add_column(\\\\\\\\\\\\\\\"Document\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"cyan\\\\\\\\\\\\\\\")\\\\\\\\n+bridge_table.add_column(\\\\\\\\\\\\\\\"Score\\\\\\\\\\\\\\\", justify=\\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"blue\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+for i, doc_info in enumerate(analysis[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"][:5], 1):\\\\\\\\n+score = doc_info.get('score', 'N/A')\\\\\\\\n+score_str = f\\\\\\\\\\\\\\\"{score:.3f}\\\\\\\\\\\\\\\" if isinstance(score, float) else str(score)\\\\\\\\n+bridge_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\\\\\\\n+console.print(bridge_table)\\\\\\\\n+\\\\\\\\n+# --- Isolated Documents ---\\\\\\\\n+if \\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n+console.print(f\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n [yellow]Isolated Documents ({analysis['isolated_count']}):[/yellow]\\\\\\\\\\\\\\\")\\\\\\\\n+# Print only first few for brevity\\\\\\\\n+for doc in analysis['isolated_docs'][:10]:\\\\\\\\n+console.print(f\\\\\\\\\\\\\\\"  - {os.path.basename(doc)}\\\\\\\\\\\\\\\")\\\\\\\\n+if len(analysis['isolated_docs']) > 10:\\\\\\\\n+console.print(f\\\\\\\\\\\\\\\"  ... and {len(analysis['isolated_docs']) - 10} more\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Save analysis to JSON\\\\\\\\n+analysis_path = generator.visualization_path / \\\\\\\\\\\\\\\"documentation_graph_analysis.json\\\\\\\\\\\\\\\"\\\\\\\\n+with open(analysis_path, 'w', encoding='utf-8') as f:\\\\\\\\n+json.dump(analysis, f, indent=2)\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Analysis saved to: {analysis_path}\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# --- End of new try block, add except block ---\\\\\\\\n+except Exception as core_error:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"!!! Critical Error during graph processing/visualization: {core_error}\\\\\\\\\\\\\\\")\\\\\\\\n+logger.error(traceback.format_exc())\\\\\\\\n+return 1 # Ensure exit with error code\\\\\\\\n+\\\\\\\\n+return 0\\\\\\\\n+\\\\\\\\n+# Outer except block remains for initialization/loading errors\\\\\\\\n+except Exception as e:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"Initialization or Loading Error: {str(e)}\\\\\\\\\\\\\\\")\\\\\\\\n+# Ensure traceback is imported if needed here too\\\\\\\\n+if 'traceback' not in sys.modules:\\\\\\\\n+import traceback\\\\\\\\n+logger.error(traceback.format_exc())\\\\\\\\n+return 1\\\\\\\\n \\\\\\\\n \\\\\\\\n if __name__ == \\\\\\\\\\\\\\\"__main__\\\\\\\\\\\\\\\":\\\\\\\\n-    sys.exit(main())+sys.exit(main())\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\"\\\"\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_RelativePathSelf_results.json",
    "LineNumber": 1095,
    "LineContent": "    \"LineContent\": \"    \\\"LineContent\\\": \\\"      \\\\\\\"diff\\\\\\\": \\\\\\\"--- a/generate_documentation_graph.py\\\\\\\\n+++ b/generate_documentation_graph.py\\\\\\\\n@@ -1,10 +1,10 @@\\\\\\\\n-<!-- \\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n @references:\\\\\\\\n - Core References:\\\\\\\\n-  - [MQP.md](mdc:../../MQP.md) - Master Quantum Prompt defining EGOS principles\\\\\\\\n-  - [ROADMAP.md](mdc:../../ROADMAP.md) - Project roadmap and planning\\\\\\\\n+- [MQP.md](mdc:../../MQP.md) - Master Quantum Prompt defining EGOS principles\\\\\\\\n+- [ROADMAP.md](mdc:../../ROADMAP.md) - Project roadmap and planning\\\\\\\\n - Process Documentation:\\\\\\\\n-  - [system_maintenance.md](mdc:../../docs_egos/process/system_maintenance.md)\\\\\\\\n+- [system_maintenance.md](mdc:../../docs_egos/process/system_maintenance.md)\\\\\\\\n \\\\\\\\n \\\\\\\\n \\\\\\\\n@@ -42,809 +42,810 @@\\\\\\\\n \\\\\\\\n # Import Rich components for progress bars\\\\\\\\n from rich.progress import (\\\\\\\\n-    Progress,\\\\\\\\n-    SpinnerColumn,\\\\\\\\n-    TextColumn,\\\\\\\\n-    BarColumn,\\\\\\\\n-    TimeRemainingColumn,\\\\\\\\n-    TaskID,\\\\\\\\n+Progress,\\\\\\\\n+SpinnerColumn,\\\\\\\\n+TextColumn,\\\\\\\\n+BarColumn,\\\\\\\\n+TimeRemainingColumn,\\\\\\\\n+TaskID,\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Setup logging\\\\\\\\n logging.basicConfig(\\\\\\\\n-    level=logging.INFO,\\\\\\\\n-    format=\\\\\\\\\\\\\\\"%(message)s\\\\\\\\\\\\\\\", # Rich handler handles time/level\\\\\\\\n-    datefmt=\\\\\\\\\\\\\\\"[%X]\\\\\\\\\\\\\\\",\\\\\\\\n-    handlers=[logging.StreamHandler()]\\\\\\\\n+level=logging.INFO,\\\\\\\\n+format=\\\\\\\\\\\\\\\"%(message)s\\\\\\\\\\\\\\\", # Rich handler handles time/level\\\\\\\\n+datefmt=\\\\\\\\\\\\\\\"[%X]\\\\\\\\\\\\\\\",\\\\\\\\n+handlers=[logging.StreamHandler()]\\\\\\\\n )\\\\\\\\n \\\\\\\\n logger = logging.getLogger(__name__)\\\\\\\\n \\\\\\\\n # Optional imports with fallbacks\\\\\\\\n try:\\\\\\\\n-    from pyvis.network import Network\\\\\\\\n-    PYVIS_AVAILABLE = True\\\\\\\\n+from pyvis.network import Network\\\\\\\\n+PYVIS_AVAILABLE = True\\\\\\\\n except ImportError:\\\\\\\\n-    logger.warning(\\\\\\\\\\\\\\\"Pyvis not installed. Will use simplified visualization.\\\\\\\\\\\\\\\")\\\\\\\\n-    PYVIS_AVAILABLE = False\\\\\\\\n+logger.warning(\\\\\\\\\\\\\\\"Pyvis not installed. Will use simplified visualization.\\\\\\\\\\\\\\\")\\\\\\\\n+PYVIS_AVAILABLE = False\\\\\\\\n \\\\\\\\n # NetworkX for basic/static visualization\\\\\\\\n try:\\\\\\\\n-    import networkx as nx\\\\\\\\n-    import matplotlib.pyplot as plt\\\\\\\\n-    NETWORKX_AVAILABLE = True\\\\\\\\n+import networkx as nx\\\\\\\\n+import matplotlib.pyplot as plt\\\\\\\\n+NETWORKX_AVAILABLE = True\\\\\\\\n except ImportError:\\\\\\\\n-    logger.warning(\\\\\\\\\\\\\\\"NetworkX not installed. Only JSON graph data will be generated.\\\\\\\\\\\\\\\")\\\\\\\\n-    NETWORKX_AVAILABLE = False\\\\\\\\n+logger.warning(\\\\\\\\\\\\\\\"NetworkX not installed. Only JSON graph data will be generated.\\\\\\\\\\\\\\\")\\\\\\\\n+NETWORKX_AVAILABLE = False\\\\\\\\n \\\\\\\\n \\\\\\\\n class DocumentationGraphGenerator:\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generates interactive visualizations of EGOS documentation connections.\\\\\\\\n-    \\\\\\\\n-    This class creates visual representations of the documentation cross-reference\\\\\\\\n-    network, helping identify key documents, clusters, and connection patterns.\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    \\\\\\\\n-    def __init__(self, base_path: str = \\\\\\\\\\\\\\\".\\\\\\\\\\\\\\\"):\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Initialize the graph generator.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            base_path: Base path of the EGOS project\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        self.base_path = Path(base_path)\\\\\\\\n-        self.report_path = self.base_path / \\\\\\\\\\\\\\\"reports\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\"\\\\\\\\n-        self.visualization_path = self.report_path / \\\\\\\\\\\\\\\"visualizations\\\\\\\\\\\\\\\"\\\\\\\\n-        \\\\\\\\n-        # Ensure output directories exist\\\\\\\\n-        self.visualization_path.mkdir(parents=True, exist_ok=True)\\\\\\\\n-        \\\\\\\\n-        # Graph data\\\\\\\\n-        self.nodes = []\\\\\\\\n-        self.edges = []\\\\\\\\n-        self.subsystems = set()\\\\\\\\n-        \\\\\\\\n-        # Node attributes\\\\\\\\n-        self.node_sizes = {}\\\\\\\\n-        self.node_colors = {}\\\\\\\\n-        self.node_clusters = {}\\\\\\\\n-        \\\\\\\\n-        # Color palette for subsystems with EGOS-aligned color scheme\\\\\\\\n-        self.color_palette = {\\\\\\\\n-            \\\\\\\\\\\\\\\"koios\\\\\\\\\\\\\\\": theme.color.koios,    # Blue - Knowledge\\\\\\\\n-            \\\\\\\\\\\\\\\"cronos\\\\\\\\\\\\\\\": theme.color.cronos,   # Purple - Time\\\\\\\\n-            \\\\\\\\\\\\\\\"nexus\\\\\\\\\\\\\\\": theme.color.nexus,    # Green - Connections\\\\\\\\n-            \\\\\\\\\\\\\\\"ethik\\\\\\\\\\\\\\\": theme.color.ethik,    # Red - Ethics\\\\\\\\n-            \\\\\\\\\\\\\\\"atlas\\\\\\\\\\\\\\\": theme.color.atlas,    # Orange - Mapping\\\\\\\\n-            \\\\\\\\\\\\\\\"mycelium\\\\\\\\\\\\\\\": theme.color.mycelium, # Teal - Integration\\\\\\\\n-            \\\\\\\\\\\\\\\"harmony\\\\\\\\\\\\\\\": theme.color.harmony,  # Dark Blue - Harmony\\\\\\\\n-            \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\": theme.color.default   # Gray - Default\\\\\\\\n-        }\\\\\\\\n-    \\\\\\\\n-    def load_report_data(self, report_path: Optional[str] = None) -> bool:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Load cross-reference report data from a JSON file.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            report_path: Path to the JSON report file (optional)\\\\\\\\n-            \\\\\\\\n-        Returns:\\\\\\\\n-            True if data was loaded successfully, False otherwise\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if report_path:\\\\\\\\n-            json_path = Path(report_path)\\\\\\\\n-        else:\\\\\\\\n-            # Look for the latest 'analysis' stage checkpoint file\\\\\\\\n-            checkpoint_dir = self.base_path / \\\\\\\\\\\\\\\"reports\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"checkpoints\\\\\\\\\\\\\\\"\\\\\\\\n-            analysis_checkpoints = list(checkpoint_dir.glob(\\\\\\\\\\\\\\\"xref_checkpoint_analysis_*.json\\\\\\\\\\\\\\\"))\\\\\\\\n-            \\\\\\\\n-            if not analysis_checkpoints:\\\\\\\\n-                # Fallback: Check for older format without timestamp\\\\\\\\n-                old_format_checkpoint = checkpoint_dir / \\\\\\\\\\\\\\\"xref_checkpoint_analysis.json\\\\\\\\\\\\\\\"\\\\\\\\n-                if old_format_checkpoint.exists():\\\\\\\\n-                    analysis_checkpoints.append(old_format_checkpoint)\\\\\\\\n-                else:\\\\\\\\n-                    logger.error(\\\\\\\\n-                        \\\\\\\\\\\\\\\"No analysis checkpoint found in reports/documentation/checkpoints/. \\\\\\\\\\\\\\\"\\\\\\\\n-                        \\\\\\\\\\\\\\\"Run add_cross_references.py --stage analysis first.\\\\\\\\\\\\\\\"\\\\\\\\n-                    )\\\\\\\\n-                    return False\\\\\\\\n-                    \\\\\\\\n-            # Get the most recent analysis checkpoint\\\\\\\\n-            json_path = max(analysis_checkpoints, key=os.path.getmtime)\\\\\\\\n-            logger.info(f\\\\\\\\\\\\\\\"Using latest analysis checkpoint: {json_path.name}\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Loading and processing graph data from: {json_path}\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        try:\\\\\\\\n-            with open(json_path, 'r', encoding='utf-8') as f:\\\\\\\\n-                checkpoint_content = json.load(f)\\\\\\\\n-            \\\\\\\\n-            # Extract the core data list\\\\\\\\n-            raw_data = None\\\\\\\\n-            if isinstance(checkpoint_content, dict) and \\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\" in checkpoint_content:\\\\\\\\n-                if isinstance(checkpoint_content[\\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\"], list):\\\\\\\\n-                     raw_data = checkpoint_content[\\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\"]\\\\\\\\n-            elif isinstance(checkpoint_content, list):\\\\\\\\n-                 raw_data = checkpoint_content # Assume direct list data\\\\\\\\n-                 \\\\\\\\n-            if raw_data is None:\\\\\\\\n-                logger.error(f\\\\\\\\\\\\\\\"Could not extract valid list data from checkpoint {json_path.name}\\\\\\\\\\\\\\\")\\\\\\\\n-                return False\\\\\\\\n-                \\\\\\\\n-            # Process the list data to build the required dictionary structure\\\\\\\\n-            processed_data = {\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\": {}, \\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\": {}}\\\\\\\\n-            all_docs = set()\\\\\\\\n-\\\\\\\\n-            # First pass: Build referenced_by and collect all docs\\\\\\\\n-            for item in raw_data:\\\\\\\\n-                # Expecting [doc_path, some_int, list_of_referencing_docs]\\\\\\\\n-                if len(item) == 3 and isinstance(item[0], str) and isinstance(item[2], list):\\\\\\\\n-                    doc = item[0]\\\\\\\\n-                    referencing_docs = item[2]\\\\\\\\n-                    all_docs.add(doc)\\\\\\\\n-                    processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][doc] = referencing_docs\\\\\\\\n-                    all_docs.update(referencing_docs) # Add referencing docs too\\\\\\\\n-                else:\\\\\\\\n-                     logger.warning(f\\\\\\\\\\\\\\\"Skipping malformed item in checkpoint data: {item}\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-            # Initialize all docs in references dict\\\\\\\\n-            for doc in all_docs:\\\\\\\\n-                # Ensure doc path is cleaned/normalized if needed (optional)\\\\\\\\n-                # doc_clean = Path(doc).as_posix() # Example normalization\\\\\\\\n-                processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][doc] = [] \\\\\\\\n-                # Ensure referenced_by also has all docs initialized, even if empty initially\\\\\\\\n-                if doc not in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"]:\\\\\\\\n-                    processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][doc] = []\\\\\\\\n-                \\\\\\\\n-            # Second pass: Build references from referenced_by\\\\\\\\n-            for doc, referencing_docs in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"].items():\\\\\\\\n-                for ref_doc in referencing_docs:\\\\\\\\n-                    if ref_doc in processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"]:\\\\\\\\n-                        processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][ref_doc].append(doc)\\\\\\\\n-                    else:\\\\\\\\n-                        # This case handles if a referencing doc wasn't in the initial list's first element\\\\\\\\n-                        # It means ref_doc references 'doc', but ref_doc itself might not have been a primary entry\\\\\\\\n-                        processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][ref_doc] = [doc]\\\\\\\\n-                        if ref_doc not in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"]:\\\\\\\\n-                           processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][ref_doc] = [] # Initialize if missing\\\\\\\\n-                        logger.debug(f\\\\\\\\\\\\\\\"Added '{ref_doc}' to structures during second pass.\\\\\\\\\\\\\\\")\\\\\\\\n-                        \\\\\\\\n-            self.report_data = processed_data\\\\\\\\n-            logger.info(f\\\\\\\\\\\\\\\"Successfully processed {len(self.report_data['references'])} documents from checkpoint.\\\\\\\\\\\\\\\")\\\\\\\\n-            return True\\\\\\\\n-                \\\\\\\\n-        except Exception as e:\\\\\\\\n-            logger.error(f\\\\\\\\\\\\\\\"Error loading and processing graph data: {str(e)}\\\\\\\\\\\\\\\")\\\\\\\\n-            import traceback # Added for better debugging\\\\\\\\n-            logger.error(traceback.format_exc()) # Added for better debugging\\\\\\\\n-            return False\\\\\\\\n-    \\\\\\\\n-    def build_graph(self) -> None:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Build the document graph from loaded report data.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not hasattr(self, 'report_data'):\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No report data loaded. Call load_report_data() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return\\\\\\\\n-            \\\\\\\\n-        references = self.report_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"]\\\\\\\\n-        referenced_by = self.report_data.get(\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\", {})\\\\\\\\n-        \\\\\\\\n-        logger.info(\\\\\\\\\\\\\\\"Building documentation graph...\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        # Create nodes for all documents\\\\\\\\n-        all_docs = set(references.keys())\\\\\\\\n-        for refs in references.values():\\\\\\\\n-            all_docs.update(refs)\\\\\\\\n-        \\\\\\\\n-        # Show progress during graph building\\\\\\\\n-        total_docs = len(all_docs)\\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Processing {total_docs} documents...\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        if console:\\\\\\\\n-            with Progress(\\\\\\\\n-                SpinnerColumn(),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"[progress.description]{task.description}\\\\\\\\\\\\\\\"),\\\\\\\\n-                BarColumn(),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"[progress.percentage]{task.percentage:>3.0f}%\\\\\\\\\\\\\\\"),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"({task.completed}/{task.total})\\\\\\\\\\\\\\\"),\\\\\\\\n-                TimeRemainingColumn(),\\\\\\\\n-                console=console,\\\\\\\\n-                transient=False,  # Keep progress visible after completion\\\\\\\\n-            ) as progress:\\\\\\\\n-                build_task = progress.add_task(\\\\\\\\\\\\\\\"Building graph nodes and edges...\\\\\\\\\\\\\\\", total=total_docs)\\\\\\\\n-                \\\\\\\\n-                # Process each document as a node\\\\\\\\n-                for i, doc in enumerate(all_docs):\\\\\\\\n-                    try: \\\\\\\\n-                        # Basic filtering for non-markdown files or complex paths\\\\\\\\n-                        if not doc.endswith(\\\\\\\\\\\\\\\".md\\\\\\\\\\\\\\\") or \\\\\\\\\\\\\\\"/\\\\\\\\\\\\\\\" in doc or r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\.\\\\\\\\\\\\\\\" in doc or \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\" in doc:\\\\\\\\n-                            logger.debug(f\\\\\\\\\\\\\\\"Skipping non-standard doc path: {doc}\\\\\\\\\\\\\\\")\\\\\\\\n-                            progress.update(build_task, advance=1)\\\\\\\\n-                            continue\\\\\\\\n-        \\\\\\\\n-                        # Extract subsystem from path\\\\\\\\n-                        subsystem = self._get_subsystem(doc)\\\\\\\\n-                        self.subsystems.add(subsystem)\\\\\\\\n-        \\\\\\\\n-                        # Add node with attributes\\\\\\\\n-                        node_id = doc\\\\\\\\n-                        node_label = os.path.basename(doc)\\\\\\\\n-                        # Ensure references and referenced_by are accessed safely\\\\\\\\n-                        doc_refs = references.get(doc, [])\\\\\\\\n-                        doc_referenced_by = referenced_by.get(doc, [])\\\\\\\\n-        \\\\\\\\n-                        node_title = self._generate_node_title(doc, doc_refs, doc_referenced_by) \\\\\\\\n-                        node_size = self._calculate_node_size(len(doc_refs), len(doc_referenced_by))\\\\\\\\n-                        node_color = self.color_palette.get(subsystem, self.color_palette[\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\"])\\\\\\\\n-        \\\\\\\\n-                        self.nodes.append({\\\\\\\\n-                            \\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\": node_id,\\\\\\\\n-                            \\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\": node_label,\\\\\\\\n-                            \\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\": node_title,\\\\\\\\n-                            \\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\": node_size,\\\\\\\\n-                            \\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\": node_color,\\\\\\\\n-                            \\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\": subsystem\\\\\\\\n-                        })\\\\\\\\n-        \\\\\\\\n-                        # Add edges (references from this doc to others)\\\\\\\\n-                        for target_doc in doc_refs:\\\\\\\\n-                            # Skip self-references or invalid targets\\\\\\\\n-                            if target_doc == doc or not target_doc.endswith(\\\\\\\\\\\\\\\".md\\\\\\\\\\\\\\\") or \\\\\\\\\\\\\\\"/\\\\\\\\\\\\\\\" in target_doc or r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\.\\\\\\\\\\\\\\\" in target_doc or \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\" in target_doc:\\\\\\\\n-                                continue\\\\\\\\n-                            self.edges.append({\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\": node_id, \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\": target_doc, \\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"})\\\\\\\\n-        \\\\\\\\n-                        # Update progress\\\\\\\\n-                        progress.update(build_task, advance=1)\\\\\\\\n-        \\\\\\\\n-                    except Exception as node_error:\\\\\\\\n-                        logger.error(f\\\\\\\\\\\\\\\"Error processing node '{doc}': {node_error}\\\\\\\\\\\\\\\")\\\\\\\\n-                        # Still advance progress even if one node fails\\\\\\\\n-                        progress.update(build_task, advance=1)\\\\\\\\n-                        continue\\\\\\\\n-        else:\\\\\\\\n-            # Fallback for when rich is not available\\\\\\\\n-            logger.info(\\\\\\\\\\\\\\\"Building graph nodes and edges (basic logging)...\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generates interactive visualizations of EGOS documentation connections.\\\\\\\\n+\\\\\\\\n+This class creates visual representations of the documentation cross-reference\\\\\\\\n+network, helping identify key documents, clusters, and connection patterns.\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+def __init__(self, base_path: str = \\\\\\\\\\\\\\\".\\\\\\\\\\\\\\\"):\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Initialize the graph generator.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+base_path: Base path of the EGOS project\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+self.base_path = Path(base_path)\\\\\\\\n+self.report_path = self.base_path / \\\\\\\\\\\\\\\"reports\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\"\\\\\\\\n+self.visualization_path = self.report_path / \\\\\\\\\\\\\\\"visualizations\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Ensure output directories exist\\\\\\\\n+self.visualization_path.mkdir(parents=True, exist_ok=True)\\\\\\\\n+\\\\\\\\n+# Graph data\\\\\\\\n+self.nodes = []\\\\\\\\n+self.edges = []\\\\\\\\n+self.subsystems = set()\\\\\\\\n+\\\\\\\\n+# Node attributes\\\\\\\\n+self.node_sizes = {}\\\\\\\\n+self.node_colors = {}\\\\\\\\n+self.node_clusters = {}\\\\\\\\n+\\\\\\\\n+# Color palette for subsystems with EGOS-aligned color scheme\\\\\\\\n+self.color_palette = {\\\\\\\\n+\\\\\\\\\\\\\\\"koios\\\\\\\\\\\\\\\": theme.color.koios,    # Blue - Knowledge\\\\\\\\n+\\\\\\\\\\\\\\\"cronos\\\\\\\\\\\\\\\": theme.color.cronos,   # Purple - Time\\\\\\\\n+\\\\\\\\\\\\\\\"nexus\\\\\\\\\\\\\\\": theme.color.nexus,    # Green - Connections\\\\\\\\n+\\\\\\\\\\\\\\\"ethik\\\\\\\\\\\\\\\": theme.color.ethik,    # Red - Ethics\\\\\\\\n+\\\\\\\\\\\\\\\"atlas\\\\\\\\\\\\\\\": theme.color.atlas,    # Orange - Mapping\\\\\\\\n+\\\\\\\\\\\\\\\"mycelium\\\\\\\\\\\\\\\": theme.color.mycelium, # Teal - Integration\\\\\\\\n+\\\\\\\\\\\\\\\"harmony\\\\\\\\\\\\\\\": theme.color.harmony,  # Dark Blue - Harmony\\\\\\\\n+\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\": theme.color.default   # Gray - Default\\\\\\\\n+}\\\\\\\\n+\\\\\\\\n+def load_report_data(self, report_path: Optional[str] = None) -> bool:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Load cross-reference report data from a JSON file.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+report_path: Path to the JSON report file (optional)\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+True if data was loaded successfully, False otherwise\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if report_path:\\\\\\\\n+json_path = Path(report_path)\\\\\\\\n+else:\\\\\\\\n+# Look for the latest 'analysis' stage checkpoint file\\\\\\\\n+checkpoint_dir = self.base_path / \\\\\\\\\\\\\\\"reports\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\" / \\\\\\\\\\\\\\\"checkpoints\\\\\\\\\\\\\\\"\\\\\\\\n+analysis_checkpoints = list(checkpoint_dir.glob(\\\\\\\\\\\\\\\"xref_checkpoint_analysis_*.json\\\\\\\\\\\\\\\"))\\\\\\\\n+\\\\\\\\n+if not analysis_checkpoints:\\\\\\\\n+# Fallback: Check for older format without timestamp\\\\\\\\n+old_format_checkpoint = checkpoint_dir / \\\\\\\\\\\\\\\"xref_checkpoint_analysis.json\\\\\\\\\\\\\\\"\\\\\\\\n+if old_format_checkpoint.exists():\\\\\\\\n+analysis_checkpoints.append(old_format_checkpoint)\\\\\\\\n+else:\\\\\\\\n+logger.error(\\\\\\\\n+\\\\\\\\\\\\\\\"No analysis checkpoint found in reports/documentation/checkpoints/. \\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\\\\\\\\"Run add_cross_references.py --stage analysis first.\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+return False\\\\\\\\n+\\\\\\\\n+# Get the most recent analysis checkpoint\\\\\\\\n+json_path = max(analysis_checkpoints, key=os.path.getmtime)\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Using latest analysis checkpoint: {json_path.name}\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Loading and processing graph data from: {json_path}\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+try:\\\\\\\\n+with open(json_path, 'r', encoding='utf-8') as f:\\\\\\\\n+checkpoint_content = json.load(f)\\\\\\\\n+\\\\\\\\n+# Extract the core data list\\\\\\\\n+raw_data = None\\\\\\\\n+if isinstance(checkpoint_content, dict) and \\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\" in checkpoint_content:\\\\\\\\n+if isinstance(checkpoint_content[\\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\"], list):\\\\\\\\n+raw_data = checkpoint_content[\\\\\\\\\\\\\\\"data\\\\\\\\\\\\\\\"]\\\\\\\\n+elif isinstance(checkpoint_content, list):\\\\\\\\n+raw_data = checkpoint_content # Assume direct list data\\\\\\\\n+\\\\\\\\n+if raw_data is None:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"Could not extract valid list data from checkpoint {json_path.name}\\\\\\\\\\\\\\\")\\\\\\\\n+return False\\\\\\\\n+\\\\\\\\n+# Process the list data to build the required dictionary structure\\\\\\\\n+processed_data = {\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\": {}, \\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\": {}}\\\\\\\\n+all_docs = set()\\\\\\\\n+\\\\\\\\n+# First pass: Build referenced_by and collect all docs\\\\\\\\n+for item in raw_data:\\\\\\\\n+# Expecting [doc_path, some_int, list_of_referencing_docs]\\\\\\\\n+if len(item) == 3 and isinstance(item[0], str) and isinstance(item[2], list):\\\\\\\\n+doc = item[0]\\\\\\\\n+referencing_docs = item[2]\\\\\\\\n+all_docs.add(doc)\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][doc] = referencing_docs\\\\\\\\n+all_docs.update(referencing_docs) # Add referencing docs too\\\\\\\\n+else:\\\\\\\\n+logger.warning(f\\\\\\\\\\\\\\\"Skipping malformed item in checkpoint data: {item}\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Initialize all docs in references dict\\\\\\\\n+for doc in all_docs:\\\\\\\\n+# Ensure doc path is cleaned/normalized if needed (optional)\\\\\\\\n+# doc_clean = Path(doc).as_posix() # Example normalization\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][doc] = []\\\\\\\\n+# Ensure referenced_by also has all docs initialized, even if empty initially\\\\\\\\n+if doc not in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"]:\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][doc] = []\\\\\\\\n+\\\\\\\\n+# Second pass: Build references from referenced_by\\\\\\\\n+for doc, referencing_docs in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"].items():\\\\\\\\n+for ref_doc in referencing_docs:\\\\\\\\n+if ref_doc in processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"]:\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][ref_doc].append(doc)\\\\\\\\n+else:\\\\\\\\n+# This case handles if a referencing doc wasn't in the initial list's first element\\\\\\\\n+# It means ref_doc references 'doc', but ref_doc itself might not have been a primary entry\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"][ref_doc] = [doc]\\\\\\\\n+if ref_doc not in processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"]:\\\\\\\\n+processed_data[\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\"][ref_doc] = [] # Initialize if missing\\\\\\\\n+logger.debug(f\\\\\\\\\\\\\\\"Added '{ref_doc}' to structures during second pass.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+self.report_data = processed_data\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Successfully processed {len(self.report_data['references'])} documents from checkpoint.\\\\\\\\\\\\\\\")\\\\\\\\n+return True\\\\\\\\n+\\\\\\\\n+except Exception as e:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"Error loading and processing graph data: {str(e)}\\\\\\\\\\\\\\\")\\\\\\\\n+import traceback # Added for better debugging\\\\\\\\n+logger.error(traceback.format_exc()) # Added for better debugging\\\\\\\\n+return False\\\\\\\\n+\\\\\\\\n+def build_graph(self) -> None:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Build the document graph from loaded report data.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not hasattr(self, 'report_data'):\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No report data loaded. Call load_report_data() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return\\\\\\\\n+\\\\\\\\n+references = self.report_data[\\\\\\\\\\\\\\\"references\\\\\\\\\\\\\\\"]\\\\\\\\n+referenced_by = self.report_data.get(\\\\\\\\\\\\\\\"referenced_by\\\\\\\\\\\\\\\", {})\\\\\\\\n+\\\\\\\\n+logger.info(\\\\\\\\\\\\\\\"Building documentation graph...\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Create nodes for all documents\\\\\\\\n+all_docs = set(references.keys())\\\\\\\\n+for refs in references.values():\\\\\\\\n+all_docs.update(refs)\\\\\\\\n+\\\\\\\\n+# Show progress during graph building\\\\\\\\n+total_docs = len(all_docs)\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Processing {total_docs} documents...\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+if console:\\\\\\\\n+with Progress(\\\\\\\\n+SpinnerColumn(),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"[progress.description]{task.description}\\\\\\\\\\\\\\\"),\\\\\\\\n+BarColumn(),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"[progress.percentage]{task.percentage:>3.0f}%\\\\\\\\\\\\\\\"),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"({task.completed}/{task.total})\\\\\\\\\\\\\\\"),\\\\\\\\n+TimeRemainingColumn(),\\\\\\\\n+console=console,\\\\\\\\n+transient=False,  # Keep progress visible after completion\\\\\\\\n+) as progress:\\\\\\\\n+build_task = progress.add_task(\\\\\\\\\\\\\\\"Building graph nodes and edges...\\\\\\\\\\\\\\\", total=total_docs)\\\\\\\\n+\\\\\\\\n+# Process each document as a node\\\\\\\\n+for i, doc in enumerate(all_docs):\\\\\\\\n+try:\\\\\\\\n+# Basic filtering for non-markdown files or complex paths\\\\\\\\n+if not doc.endswith(\\\\\\\\\\\\\\\".md\\\\\\\\\\\\\\\") or \\\\\\\\\\\\\\\"/\\\\\\\\\\\\\\\" in doc or r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\.\\\\\\\\\\\\\\\" in doc or \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\" in doc:\\\\\\\\n+logger.debug(f\\\\\\\\\\\\\\\"Skipping non-standard doc path: {doc}\\\\\\\\\\\\\\\")\\\\\\\\n+progress.update(build_task, advance=1)\\\\\\\\n+continue\\\\\\\\n+\\\\\\\\n+# Extract subsystem from path\\\\\\\\n+subsystem = self._get_subsystem(doc)\\\\\\\\n+self.subsystems.add(subsystem)\\\\\\\\n+\\\\\\\\n+# Add node with attributes\\\\\\\\n+node_id = doc\\\\\\\\n+node_label = os.path.basename(doc)\\\\\\\\n+# Ensure references and referenced_by are accessed safely\\\\\\\\n+doc_refs = references.get(doc, [])\\\\\\\\n+doc_referenced_by = referenced_by.get(doc, [])\\\\\\\\n+\\\\\\\\n+node_title = self._generate_node_title(doc, doc_refs, doc_referenced_by)\\\\\\\\n+node_size = self._calculate_node_size(len(doc_refs), len(doc_referenced_by))\\\\\\\\n+node_color = self.color_palette.get(subsystem, self.color_palette[\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+self.nodes.append({\\\\\\\\n+\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\": node_id,\\\\\\\\n+\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\": node_label,\\\\\\\\n+\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\": node_title,\\\\\\\\n+\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\": node_size,\\\\\\\\n+\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\": node_color,\\\\\\\\n+\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\": subsystem\\\\\\\\n+})\\\\\\\\n+\\\\\\\\n+# Add edges (references from this doc to others)\\\\\\\\n+for target_doc in doc_refs:\\\\\\\\n+# Skip self-references or invalid targets\\\\\\\\n+if target_doc == doc or not target_doc.endswith(\\\\\\\\\\\\\\\".md\\\\\\\\\\\\\\\") or \\\\\\\\\\\\\\\"/\\\\\\\\\\\\\\\" in target_doc or r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\.\\\\\\\\\\\\\\\" in target_doc or \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\" in target_doc:\\\\\\\\n+continue\\\\\\\\n+self.edges.append({\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\": node_id, \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\": target_doc, \\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"})\\\\\\\\n+\\\\\\\\n+# Update progress\\\\\\\\n+progress.update(build_task, advance=1)\\\\\\\\n+\\\\\\\\n+except Exception as node_error:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"Error processing node '{doc}': {node_error}\\\\\\\\\\\\\\\")\\\\\\\\n+# Still advance progress even if one node fails\\\\\\\\n+progress.update(build_task, advance=1)\\\\\\\\n+continue\\\\\\\\n+else:\\\\\\\\n+# Fallback for when rich is not available\\\\\\\\n+logger.info(\\\\\\\\\\\\\\\"Building graph nodes and edges (basic logging)...\\\\\\\\\\\\\\\")\\\\\\\\n {{ ... }}\\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Graph built successfully: {len(self.nodes)} nodes, {len(self.edges)} edges.\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-    def generate_pyvis_visualization(self, output_path: Optional[str] = None) -> str:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generate an interactive HTML visualization using Pyvis.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            output_path: Path to save the HTML file (optional)\\\\\\\\n-            \\\\\\\\n-        Returns:\\\\\\\\n-            Path to the generated HTML file\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not hasattr(self, 'nodes') or not self.nodes:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No graph data to visualize. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Create network\\\\\\\\n-        timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n-        output_filename = f\\\\\\\\\\\\\\\"documentation_graph_{timestamp}.html\\\\\\\\\\\\\\\"\\\\\\\\n-        \\\\\\\\n-        if output_path:\\\\\\\\n-            output_file = Path(output_path)\\\\\\\\n-        else:\\\\\\\\n-            output_file = self.visualization_path / output_filename\\\\\\\\n-            \\\\\\\\n-        # Log the start of visualization generation\\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Generating interactive visualization ({len(self.nodes)} nodes, {len(self.edges)} edges)...\\\\\\\\\\\\\\\")\\\\\\\\n-        \\\\\\\\n-        if console:\\\\\\\\n-            with Progress(\\\\\\\\n-                SpinnerColumn(),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"[progress.description]{task.description}\\\\\\\\\\\\\\\"),\\\\\\\\n-                BarColumn(),\\\\\\\\n-                TextColumn(\\\\\\\\\\\\\\\"[progress.percentage]{task.percentage:>3.0f}%\\\\\\\\\\\\\\\"),\\\\\\\\n-                TimeRemainingColumn(),\\\\\\\\n-                console=console,\\\\\\\\n-                transient=False, # Keep progress visible after completion\\\\\\\\n-            ) as progress:\\\\\\\\n-                # Create network\\\\\\\\n-                net_task = progress.add_task(\\\\\\\\\\\\\\\"Creating network...\\\\\\\\\\\\\\\", total=1)\\\\\\\\n-                net = Network(\\\\\\\\n-                    height=\\\\\\\\\\\\\\\"800px\\\\\\\\\\\\\\\", \\\\\\\\n-                    width=\\\\\\\\\\\\\\\"100%\\\\\\\\\\\\\\\", \\\\\\\\n-                    bgcolor=theme.color.white, \\\\\\\\n-                    font_color=theme.color.text_primary,\\\\\\\\n-                    heading=f\\\\\\\\\\\\\\\"EGOS Documentation Network ({timestamp})\\\\\\\\\\\\\\\"\\\\\\\\n-                )\\\\\\\\n-                progress.update(net_task, completed=1)\\\\\\\\n-                \\\\\\\\n-                # Add nodes with progress\\\\\\\\n-                node_task = progress.add_task(\\\\\\\\\\\\\\\"Adding nodes...\\\\\\\\\\\\\\\", total=len(self.nodes))\\\\\\\\n-                for node in self.nodes:\\\\\\\\n-                    net.add_node(\\\\\\\\n-                        node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"], \\\\\\\\n-                        label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"], \\\\\\\\n-                        title=node[\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\"],\\\\\\\\n-                        color=node[\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\"],\\\\\\\\n-                        size=node[\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\"],\\\\\\\\n-                        group=node[\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\"]\\\\\\\\n-                    )\\\\\\\\n-                    progress.update(node_task, advance=1)\\\\\\\\n-                \\\\\\\\n-                # Add edges with progress\\\\\\\\n-                edge_task = progress.add_task(\\\\\\\\\\\\\\\"Adding edges...\\\\\\\\\\\\\\\", total=len(self.edges))\\\\\\\\n-                for edge in self.edges:\\\\\\\\n-                    net.add_edge(\\\\\\\\n-                        edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], \\\\\\\\n-                        edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"], \\\\\\\\n-                        arrows=edge.get(\\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"),\\\\\\\\n-                        title=edge.get(\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n-                    )\\\\\\\\n-                    progress.update(edge_task, advance=1)\\\\\\\\n-                \\\\\\\\n-                # Configure physics for better readability\\\\\\\\n-                save_task = progress.add_task(\\\\\\\\\\\\\\\"Configuring and saving...\\\\\\\\\\\\\\\", total=1)\\\\\\\\n-                net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\\\\\\\n-                net.set_options(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-                {\\\\\\\\n-                  \\\\\\\\\\\\\\\"physics\\\\\\\\\\\\\\\": {\\\\\\\\n-                    \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\": {\\\\\\\\n-                      \\\\\\\\\\\\\\\"gravitationalConstant\\\\\\\\\\\\\\\": -100,\\\\\\\\n-                      \\\\\\\\\\\\\\\"centralGravity\\\\\\\\\\\\\\\": 0.01,\\\\\\\\n-                      \\\\\\\\\\\\\\\"springLength\\\\\\\\\\\\\\\": 200,\\\\\\\\n-                      \\\\\\\\\\\\\\\"springConstant\\\\\\\\\\\\\\\": 0.08\\\\\\\\n-                    },\\\\\\\\n-                    \\\\\\\\\\\\\\\"solver\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\",\\\\\\\\n-                    \\\\\\\\\\\\\\\"stabilization\\\\\\\\\\\\\\\": {\\\\\\\\n-                      \\\\\\\\\\\\\\\"iterations\\\\\\\\\\\\\\\": 100\\\\\\\\n-                    }\\\\\\\\n-                  }\\\\\\\\n-                }\\\\\\\\n-                \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n-                \\\\\\\\n-                # Save the network\\\\\\\\n-                net.save_graph(str(output_file))\\\\\\\\n-                progress.update(save_task, completed=1)\\\\\\\\n-        else:\\\\\\\\n-            # Fallback for when rich is not available\\\\\\\\n-            logger.info(\\\\\\\\\\\\\\\"Generating visualization (basic logging)...\\\\\\\\\\\\\\\")\\\\\\\\n-            net = Network(\\\\\\\\n-                height=\\\\\\\\\\\\\\\"800px\\\\\\\\\\\\\\\", \\\\\\\\n-                width=\\\\\\\\\\\\\\\"100%\\\\\\\\\\\\\\\", \\\\\\\\n-                bgcolor=theme.color.white, \\\\\\\\n-                font_color=theme.color.text_primary,\\\\\\\\n-                heading=f\\\\\\\\\\\\\\\"EGOS Documentation Network ({timestamp})\\\\\\\\\\\\\\\"\\\\\\\\n-            )\\\\\\\\n-            \\\\\\\\n-            # Add nodes\\\\\\\\n-            for node in self.nodes:\\\\\\\\n-                net.add_node(\\\\\\\\n-                    node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"], \\\\\\\\n-                    label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"], \\\\\\\\n-                    title=node[\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\"],\\\\\\\\n-                    color=node[\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\"],\\\\\\\\n-                    size=node[\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\"],\\\\\\\\n-                    group=node[\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\"]\\\\\\\\n-                )\\\\\\\\n-            \\\\\\\\n-            # Add edges\\\\\\\\n-            for edge in self.edges:\\\\\\\\n-                net.add_edge(\\\\\\\\n-                    edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], \\\\\\\\n-                    edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"], \\\\\\\\n-                    arrows=edge.get(\\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"),\\\\\\\\n-                    title=edge.get(\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n-                )\\\\\\\\n-            \\\\\\\\n-            # Configure physics for better readability\\\\\\\\n-            net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\\\\\\\n-            net.set_options(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            {\\\\\\\\n-              \\\\\\\\\\\\\\\"physics\\\\\\\\\\\\\\\": {\\\\\\\\n-                \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\": {\\\\\\\\n-                  \\\\\\\\\\\\\\\"gravitationalConstant\\\\\\\\\\\\\\\": -100,\\\\\\\\n-                  \\\\\\\\\\\\\\\"centralGravity\\\\\\\\\\\\\\\": 0.01,\\\\\\\\n-                  \\\\\\\\\\\\\\\"springLength\\\\\\\\\\\\\\\": 200,\\\\\\\\n-                  \\\\\\\\\\\\\\\"springConstant\\\\\\\\\\\\\\\": 0.08\\\\\\\\n-                },\\\\\\\\n-                \\\\\\\\\\\\\\\"solver\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\",\\\\\\\\n-                \\\\\\\\\\\\\\\"stabilization\\\\\\\\\\\\\\\": {\\\\\\\\n-                  \\\\\\\\\\\\\\\"iterations\\\\\\\\\\\\\\\": 100\\\\\\\\n-                }\\\\\\\\n-              }\\\\\\\\n-            }\\\\\\\\n-            \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n-            \\\\\\\\n-            # Save the network\\\\\\\\n-            net.save_graph(str(output_file))\\\\\\\\n-            \\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Interactive visualization saved to: {output_file}\\\\\\\\\\\\\\\")\\\\\\\\n-        return str(output_file)\\\\\\\\n-    \\\\\\\\n-    def generate_networkx_visualization(self, output_path: Optional[str] = None) -> str:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generate a static visualization using NetworkX and matplotlib.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            output_path: Path to save the PNG file (optional)\\\\\\\\n-            \\\\\\\\n-        Returns:\\\\\\\\n-            Path to the generated PNG file\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not NETWORKX_AVAILABLE:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"NetworkX is not available. Install with: pip install networkx matplotlib\\\\\\\\\\\\\\\")\\\\\\\\n-            return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        if not self.nodes:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Create graph\\\\\\\\n-        G = nx.DiGraph()\\\\\\\\n-        \\\\\\\\n-        # Add nodes with attributes\\\\\\\\n-        for node in self.nodes:\\\\\\\\n-            G.add_node(\\\\\\\\n-                node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"],\\\\\\\\n-                label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"],\\\\\\\\n-                subsystem=node.get(\\\\\\\\\\\\\\\"subsystem\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\")\\\\\\\\n-            )\\\\\\\\n-            \\\\\\\\n-        # Add edges\\\\\\\\n-        for edge in self.edges:\\\\\\\\n-            G.add_edge(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n-            \\\\\\\\n-        # Generate output filename if not provided\\\\\\\\n-        if not output_path:\\\\\\\\n-            timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n-            output_path = self.visualization_path / f\\\\\\\\\\\\\\\"documentation_graph_{timestamp}.png\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Set up figure\\\\\\\\n-        plt.figure(figsize=(20, 20))\\\\\\\\n-        \\\\\\\\n-        # Use spring layout with seed for reproducibility\\\\\\\\n-        pos = nx.spring_layout(G, seed=42)\\\\\\\\n-        \\\\\\\\n-        # Node colors based on subsystem\\\\\\\\n-        node_colors = [self.node_colors.get(node, self.color_palette[\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\"]) for node in G.nodes()]\\\\\\\\n-        \\\\\\\\n-        # Node sizes\\\\\\\\n-        node_sizes = [self.node_sizes.get(node, 10) for node in G.nodes()]\\\\\\\\n-        \\\\\\\\n-        # Draw the graph\\\\\\\\n-        nx.draw_networkx(\\\\\\\\n-            G, \\\\\\\\n-            pos=pos,\\\\\\\\n-            with_labels=False,\\\\\\\\n-            node_color=node_colors,\\\\\\\\n-            node_size=node_sizes,\\\\\\\\n-            alpha=0.8,\\\\\\\\n-            arrows=True,\\\\\\\\n-            edge_color=theme.color.medium_grey\\\\\\\\n-        )\\\\\\\\n-        \\\\\\\\n-        # Draw labels for larger nodes only\\\\\\\\n-        large_nodes = {node: data for node, data in G.nodes(data=True) \\\\\\\\n-                      if self.node_sizes.get(node, 10) > 25}\\\\\\\\n-        if large_nodes:\\\\\\\\n-            nx.draw_networkx_labels(\\\\\\\\n-                G, \\\\\\\\n-                pos=pos,\\\\\\\\n-                labels={node: data[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"] for node, data in large_nodes.items()},\\\\\\\\n-                font_size=8\\\\\\\\n-            )\\\\\\\\n-            \\\\\\\\n-        # Save the visualization\\\\\\\\n-        plt.tight_layout()\\\\\\\\n-        plt.axis(\\\\\\\\\\\\\\\"off\\\\\\\\\\\\\\\")\\\\\\\\n-        plt.savefig(str(output_path), dpi=300, bbox_inches=\\\\\\\\\\\\\\\"tight\\\\\\\\\\\\\\\")\\\\\\\\n-        plt.close()\\\\\\\\n-        \\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Static visualization saved to: {output_path}\\\\\\\\\\\\\\\")\\\\\\\\n-        return str(output_path)\\\\\\\\n-    \\\\\\\\n-    def export_graph_data(self, output_path: Optional[str] = None) -> str:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Export graph data to JSON for use in custom visualizations.\\\\\\\\n-        \\\\\\\\n-        Args:\\\\\\\\n-            output_path: Path to save the JSON file (optional)\\\\\\\\n-            \\\\\\\\n-        Returns:\\\\\\\\n-            Path to the generated JSON file\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not self.nodes:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Create graph data structure\\\\\\\\n-        graph_data = {\\\\\\\\n-            \\\\\\\\\\\\\\\"nodes\\\\\\\\\\\\\\\": self.nodes,\\\\\\\\n-            \\\\\\\\\\\\\\\"edges\\\\\\\\\\\\\\\": self.edges,\\\\\\\\n-            \\\\\\\\\\\\\\\"metadata\\\\\\\\\\\\\\\": {\\\\\\\\n-                \\\\\\\\\\\\\\\"generated\\\\\\\\\\\\\\\": datetime.now().isoformat(),\\\\\\\\n-                \\\\\\\\\\\\\\\"subsystems\\\\\\\\\\\\\\\": list(sorted(self.subsystems)),\\\\\\\\n-                \\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(self.nodes),\\\\\\\\n-                \\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(self.edges)\\\\\\\\n-            }\\\\\\\\n-        }\\\\\\\\n-        \\\\\\\\n-        # Generate output filename if not provided\\\\\\\\n-        if not output_path:\\\\\\\\n-            timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n-            output_path = self.visualization_path / f\\\\\\\\\\\\\\\"documentation_graph_data_{timestamp}.json\\\\\\\\\\\\\\\"\\\\\\\\n-            \\\\\\\\n-        # Save the graph data\\\\\\\\n-        with open(output_path, 'w', encoding='utf-8') as f:\\\\\\\\n-            json.dump(graph_data, f, indent=2)\\\\\\\\n-            \\\\\\\\n-        logger.info(f\\\\\\\\\\\\\\\"Graph data exported to: {output_path}\\\\\\\\\\\\\\\")\\\\\\\\n-        return str(output_path)\\\\\\\\n-    \\\\\\\\n-    def analyze_graph(self) -> Dict[str, Any]:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Analyze the graph structure and generate metrics.\\\\\\\\n-        \\\\\\\\n-        Returns:\\\\\\\\n-            Dictionary of graph metrics and insights\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if not self.nodes or not self.edges:\\\\\\\\n-            logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n-            return {}\\\\\\\\n-            \\\\\\\\n-        # If NetworkX is available, use it for analysis\\\\\\\\n-        if NETWORKX_AVAILABLE:\\\\\\\\n-            return self._analyze_with_networkx()\\\\\\\\n-        else:\\\\\\\\n-            return self._analyze_basic()\\\\\\\\n-    \\\\\\\\n-    def _analyze_with_networkx(self) -> Dict[str, Any]:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Analyze graph using NetworkX metrics.\\\\\\\\n-        \\\\\\\\n-        Returns:\\\\\\\\n-            Dictionary of graph metrics\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        # Create graph\\\\\\\\n-        G = nx.DiGraph()\\\\\\\\n-        \\\\\\\\n-        # Add nodes\\\\\\\\n-        for node in self.nodes:\\\\\\\\n-            G.add_node(node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"])\\\\\\\\n-            \\\\\\\\n-        # Add edges\\\\\\\\n-        for edge in self.edges:\\\\\\\\n-            G.add_edge(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n-            \\\\\\\\n-        # Calculate metrics\\\\\\\\n-        metrics = {\\\\\\\\n-            \\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(G),\\\\\\\\n-            \\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(G.edges()),\\\\\\\\n-            \\\\\\\\\\\\\\\"density\\\\\\\\\\\\\\\": nx.density(G),\\\\\\\\n-        }\\\\\\\\n-        \\\\\\\\n-        # Calculate centrality measures\\\\\\\\n-        try:\\\\\\\\n-            in_degree_centrality = nx.in_degree_centrality(G)\\\\\\\\n-            out_degree_centrality = nx.out_degree_centrality(G)\\\\\\\\n-            betweenness_centrality = nx.betweenness_centrality(G)\\\\\\\\n-            \\\\\\\\n-            # Sort by value to find highest centrality nodes\\\\\\\\n-            top_in_degree = sorted(in_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n-            top_out_degree = sorted(out_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n-            top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n-            \\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_in_degree]\\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"most_referencing_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_out_degree]\\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_betweenness]\\\\\\\\n-        except:\\\\\\\\n-            logger.warning(\\\\\\\\\\\\\\\"Error calculating centrality measures\\\\\\\\\\\\\\\")\\\\\\\\n-            \\\\\\\\n-        # Find isolated nodes (no connections)\\\\\\\\n-        isolated = list(nx.isolates(G))\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"] = isolated\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\"] = len(isolated)\\\\\\\\n-        \\\\\\\\n-        # Find strongly connected components\\\\\\\\n-        strongly_connected = list(nx.strongly_connected_components(G))\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"strongly_connected_components\\\\\\\\\\\\\\\"] = len(strongly_connected)\\\\\\\\n-        \\\\\\\\n-        # Find largest strongly connected component\\\\\\\\n-        if strongly_connected:\\\\\\\\n-            largest_component = max(strongly_connected, key=len)\\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"largest_component_size\\\\\\\\\\\\\\\"] = len(largest_component)\\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"largest_component_ratio\\\\\\\\\\\\\\\"] = len(largest_component) / len(G)\\\\\\\\n-        \\\\\\\\n-        return metrics\\\\\\\\n-    \\\\\\\\n-    def _analyze_basic(self) -> Dict[str, Any]:\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Perform basic graph analysis without NetworkX.\\\\\\\\n-        \\\\\\\\n-        Returns:\\\\\\\\n-            Dictionary of basic graph metrics\\\\\\\\n-        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        # Count nodes and edges\\\\\\\\n-        metrics = {\\\\\\\\n-            \\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(self.nodes),\\\\\\\\n-            \\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(self.edges),\\\\\\\\n-        }\\\\\\\\n-        \\\\\\\\n-        # Count nodes by subsystem\\\\\\\\n-        subsystem_counts = {}\\\\\\\\n-        for node in self.nodes:\\\\\\\\n-            subsystem = node.get(\\\\\\\\\\\\\\\"subsystem\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\")\\\\\\\\n-            subsystem_counts[subsystem] = subsystem_counts.get(subsystem, 0) + 1\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"subsystem_counts\\\\\\\\\\\\\\\"] = subsystem_counts\\\\\\\\n-        \\\\\\\\n-        # Find nodes with most connections\\\\\\\\n-        node_connections = {}\\\\\\\\n-        for edge in self.edges:\\\\\\\\n-            source = edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"]\\\\\\\\n-            target = edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"]\\\\\\\\n-            \\\\\\\\n-            # Count outgoing connections\\\\\\\\n-            node_connections.setdefault(source, {\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\": 0, \\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\": 0})\\\\\\\\n-            node_connections[source][\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\"] += 1\\\\\\\\n-            \\\\\\\\n-            # Count incoming connections\\\\\\\\n-            node_connections.setdefault(target, {\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\": 0, \\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\": 0})\\\\\\\\n-            node_connections[target][\\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\"] += 1\\\\\\\\n-            \\\\\\\\n-        # Find most referenced documents\\\\\\\\n-        most_referenced = sorted(\\\\\\\\n-            [(node, data[\\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\"]) for node, data in node_connections.items()],\\\\\\\\n-            key=lambda x: x[1],\\\\\\\\n-            reverse=True\\\\\\\\n-        )[:10]\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"count\\\\\\\\\\\\\\\": count} for doc, count in most_referenced]\\\\\\\\n-        \\\\\\\\n-        # Find most referencing documents\\\\\\\\n-        most_referencing = sorted(\\\\\\\\n-            [(node, data[\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\"]) for node, data in node_connections.items()],\\\\\\\\n-            key=lambda x: x[1],\\\\\\\\n-            reverse=True\\\\\\\\n-        )[:10]\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"most_referencing_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"count\\\\\\\\\\\\\\\": count} for doc, count in most_referencing]\\\\\\\\n-        \\\\\\\\n-        # Find isolated documents (no connections)\\\\\\\\n-        connected_nodes = set()\\\\\\\\n-        for edge in self.edges:\\\\\\\\n-            connected_nodes.add(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"])\\\\\\\\n-            connected_nodes.add(edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n-            \\\\\\\\n-        all_node_ids = set(node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"] for node in self.nodes)\\\\\\\\n-        isolated = all_node_ids - connected_nodes\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"] = list(isolated)\\\\\\\\n-        metrics[\\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\"] = len(isolated)\\\\\\\\n-        \\\\\\\\n-        return metrics\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Graph built successfully: {len(self.nodes)} nodes, {len(self.edges)} edges.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+def generate_pyvis_visualization(self, output_path: Optional[str] = None) -> str:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generate an interactive HTML visualization using Pyvis.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+output_path: Path to save the HTML file (optional)\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Path to the generated HTML file\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not hasattr(self, 'nodes') or not self.nodes:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No graph data to visualize. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Create network\\\\\\\\n+timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n+output_filename = f\\\\\\\\\\\\\\\"documentation_graph_{timestamp}.html\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+if output_path:\\\\\\\\n+output_file = Path(output_path)\\\\\\\\n+else:\\\\\\\\n+output_file = self.visualization_path / output_filename\\\\\\\\n+\\\\\\\\n+# Log the start of visualization generation\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Generating interactive visualization ({len(self.nodes)} nodes, {len(self.edges)} edges)...\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+if console:\\\\\\\\n+with Progress(\\\\\\\\n+SpinnerColumn(),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"[progress.description]{task.description}\\\\\\\\\\\\\\\"),\\\\\\\\n+BarColumn(),\\\\\\\\n+TextColumn(\\\\\\\\\\\\\\\"[progress.percentage]{task.percentage:>3.0f}%\\\\\\\\\\\\\\\"),\\\\\\\\n+TimeRemainingColumn(),\\\\\\\\n+console=console,\\\\\\\\n+transient=False, # Keep progress visible after completion\\\\\\\\n+) as progress:\\\\\\\\n+# Create network\\\\\\\\n+net_task = progress.add_task(\\\\\\\\\\\\\\\"Creating network...\\\\\\\\\\\\\\\", total=1)\\\\\\\\n+net = Network(\\\\\\\\n+height=\\\\\\\\\\\\\\\"800px\\\\\\\\\\\\\\\",\\\\\\\\n+width=\\\\\\\\\\\\\\\"100%\\\\\\\\\\\\\\\",\\\\\\\\n+bgcolor=theme.color.white,\\\\\\\\n+font_color=theme.color.text_primary,\\\\\\\\n+heading=f\\\\\\\\\\\\\\\"EGOS Documentation Network ({timestamp})\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+progress.update(net_task, completed=1)\\\\\\\\n+\\\\\\\\n+# Add nodes with progress\\\\\\\\n+node_task = progress.add_task(\\\\\\\\\\\\\\\"Adding nodes...\\\\\\\\\\\\\\\", total=len(self.nodes))\\\\\\\\n+for node in self.nodes:\\\\\\\\n+net.add_node(\\\\\\\\n+node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"],\\\\\\\\n+label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"],\\\\\\\\n+title=node[\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\"],\\\\\\\\n+color=node[\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\"],\\\\\\\\n+size=node[\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\"],\\\\\\\\n+group=node[\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\"]\\\\\\\\n+)\\\\\\\\n+progress.update(node_task, advance=1)\\\\\\\\n+\\\\\\\\n+# Add edges with progress\\\\\\\\n+edge_task = progress.add_task(\\\\\\\\\\\\\\\"Adding edges...\\\\\\\\\\\\\\\", total=len(self.edges))\\\\\\\\n+for edge in self.edges:\\\\\\\\n+net.add_edge(\\\\\\\\n+edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"],\\\\\\\\n+edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"],\\\\\\\\n+arrows=edge.get(\\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"),\\\\\\\\n+title=edge.get(\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n+)\\\\\\\\n+progress.update(edge_task, advance=1)\\\\\\\\n+\\\\\\\\n+# Configure physics for better readability\\\\\\\\n+save_task = progress.add_task(\\\\\\\\\\\\\\\"Configuring and saving...\\\\\\\\\\\\\\\", total=1)\\\\\\\\n+net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\\\\\\\n+net.set_options(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+{\\\\\\\\n+\\\\\\\\\\\\\\\"physics\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"gravitationalConstant\\\\\\\\\\\\\\\": -100,\\\\\\\\n+\\\\\\\\\\\\\\\"centralGravity\\\\\\\\\\\\\\\": 0.01,\\\\\\\\n+\\\\\\\\\\\\\\\"springLength\\\\\\\\\\\\\\\": 200,\\\\\\\\n+\\\\\\\\\\\\\\\"springConstant\\\\\\\\\\\\\\\": 0.08\\\\\\\\n+},\\\\\\\\n+\\\\\\\\\\\\\\\"solver\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\",\\\\\\\\n+\\\\\\\\\\\\\\\"stabilization\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"iterations\\\\\\\\\\\\\\\": 100\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Save the network\\\\\\\\n+net.save_graph(str(output_file))\\\\\\\\n+progress.update(save_task, completed=1)\\\\\\\\n+else:\\\\\\\\n+# Fallback for when rich is not available\\\\\\\\n+logger.info(\\\\\\\\\\\\\\\"Generating visualization (basic logging)...\\\\\\\\\\\\\\\")\\\\\\\\n+net = Network(\\\\\\\\n+height=\\\\\\\\\\\\\\\"800px\\\\\\\\\\\\\\\",\\\\\\\\n+width=\\\\\\\\\\\\\\\"100%\\\\\\\\\\\\\\\",\\\\\\\\n+bgcolor=theme.color.white,\\\\\\\\n+font_color=theme.color.text_primary,\\\\\\\\n+heading=f\\\\\\\\\\\\\\\"EGOS Documentation Network ({timestamp})\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Add nodes\\\\\\\\n+for node in self.nodes:\\\\\\\\n+net.add_node(\\\\\\\\n+node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"],\\\\\\\\n+label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"],\\\\\\\\n+title=node[\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\"],\\\\\\\\n+color=node[\\\\\\\\\\\\\\\"color\\\\\\\\\\\\\\\"],\\\\\\\\n+size=node[\\\\\\\\\\\\\\\"size\\\\\\\\\\\\\\\"],\\\\\\\\n+group=node[\\\\\\\\\\\\\\\"group\\\\\\\\\\\\\\\"]\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Add edges\\\\\\\\n+for edge in self.edges:\\\\\\\\n+net.add_edge(\\\\\\\\n+edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"],\\\\\\\\n+edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"],\\\\\\\\n+arrows=edge.get(\\\\\\\\\\\\\\\"arrows\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"),\\\\\\\\n+title=edge.get(\\\\\\\\\\\\\\\"title\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Configure physics for better readability\\\\\\\\n+net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\\\\\\\n+net.set_options(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+{\\\\\\\\n+\\\\\\\\\\\\\\\"physics\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"gravitationalConstant\\\\\\\\\\\\\\\": -100,\\\\\\\\n+\\\\\\\\\\\\\\\"centralGravity\\\\\\\\\\\\\\\": 0.01,\\\\\\\\n+\\\\\\\\\\\\\\\"springLength\\\\\\\\\\\\\\\": 200,\\\\\\\\n+\\\\\\\\\\\\\\\"springConstant\\\\\\\\\\\\\\\": 0.08\\\\\\\\n+},\\\\\\\\n+\\\\\\\\\\\\\\\"solver\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"forceAtlas2Based\\\\\\\\\\\\\\\",\\\\\\\\n+\\\\\\\\\\\\\\\"stabilization\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"iterations\\\\\\\\\\\\\\\": 100\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Save the network\\\\\\\\n+net.save_graph(str(output_file))\\\\\\\\n+\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Interactive visualization saved to: {output_file}\\\\\\\\\\\\\\\")\\\\\\\\n+return str(output_file)\\\\\\\\n+\\\\\\\\n+def generate_networkx_visualization(self, output_path: Optional[str] = None) -> str:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Generate a static visualization using NetworkX and matplotlib.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+output_path: Path to save the PNG file (optional)\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Path to the generated PNG file\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not NETWORKX_AVAILABLE:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"NetworkX is not available. Install with: pip install networkx matplotlib\\\\\\\\\\\\\\\")\\\\\\\\n+return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+if not self.nodes:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Create graph\\\\\\\\n+G = nx.DiGraph()\\\\\\\\n+\\\\\\\\n+# Add nodes with attributes\\\\\\\\n+for node in self.nodes:\\\\\\\\n+G.add_node(\\\\\\\\n+node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"],\\\\\\\\n+label=node[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"],\\\\\\\\n+subsystem=node.get(\\\\\\\\\\\\\\\"subsystem\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\")\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Add edges\\\\\\\\n+for edge in self.edges:\\\\\\\\n+G.add_edge(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+# Generate output filename if not provided\\\\\\\\n+if not output_path:\\\\\\\\n+timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n+output_path = self.visualization_path / f\\\\\\\\\\\\\\\"documentation_graph_{timestamp}.png\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Set up figure\\\\\\\\n+plt.figure(figsize=(20, 20))\\\\\\\\n+\\\\\\\\n+# Use spring layout with seed for reproducibility\\\\\\\\n+pos = nx.spring_layout(G, seed=42)\\\\\\\\n+\\\\\\\\n+# Node colors based on subsystem\\\\\\\\n+node_colors = [self.node_colors.get(node, self.color_palette[\\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\"]) for node in G.nodes()]\\\\\\\\n+\\\\\\\\n+# Node sizes\\\\\\\\n+node_sizes = [self.node_sizes.get(node, 10) for node in G.nodes()]\\\\\\\\n+\\\\\\\\n+# Draw the graph\\\\\\\\n+nx.draw_networkx(\\\\\\\\n+G,\\\\\\\\n+pos=pos,\\\\\\\\n+with_labels=False,\\\\\\\\n+node_color=node_colors,\\\\\\\\n+node_size=node_sizes,\\\\\\\\n+alpha=0.8,\\\\\\\\n+arrows=True,\\\\\\\\n+edge_color=theme.color.medium_grey\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Draw labels for larger nodes only\\\\\\\\n+large_nodes = {node: data for node, data in G.nodes(data=True)\\\\\\\\n+if self.node_sizes.get(node, 10) > 25}\\\\\\\\n+if large_nodes:\\\\\\\\n+nx.draw_networkx_labels(\\\\\\\\n+G,\\\\\\\\n+pos=pos,\\\\\\\\n+labels={node: data[\\\\\\\\\\\\\\\"label\\\\\\\\\\\\\\\"] for node, data in large_nodes.items()},\\\\\\\\n+font_size=8\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Save the visualization\\\\\\\\n+plt.tight_layout()\\\\\\\\n+plt.axis(\\\\\\\\\\\\\\\"off\\\\\\\\\\\\\\\")\\\\\\\\n+plt.savefig(str(output_path), dpi=300, bbox_inches=\\\\\\\\\\\\\\\"tight\\\\\\\\\\\\\\\")\\\\\\\\n+plt.close()\\\\\\\\n+\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Static visualization saved to: {output_path}\\\\\\\\\\\\\\\")\\\\\\\\n+return str(output_path)\\\\\\\\n+\\\\\\\\n+def export_graph_data(self, output_path: Optional[str] = None) -> str:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Export graph data to JSON for use in custom visualizations.\\\\\\\\n+\\\\\\\\n+Args:\\\\\\\\n+output_path: Path to save the JSON file (optional)\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Path to the generated JSON file\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not self.nodes:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Create graph data structure\\\\\\\\n+graph_data = {\\\\\\\\n+\\\\\\\\\\\\\\\"nodes\\\\\\\\\\\\\\\": self.nodes,\\\\\\\\n+\\\\\\\\\\\\\\\"edges\\\\\\\\\\\\\\\": self.edges,\\\\\\\\n+\\\\\\\\\\\\\\\"metadata\\\\\\\\\\\\\\\": {\\\\\\\\n+\\\\\\\\\\\\\\\"generated\\\\\\\\\\\\\\\": datetime.now().isoformat(),\\\\\\\\n+\\\\\\\\\\\\\\\"subsystems\\\\\\\\\\\\\\\": list(sorted(self.subsystems)),\\\\\\\\n+\\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(self.nodes),\\\\\\\\n+\\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(self.edges)\\\\\\\\n+}\\\\\\\\n+}\\\\\\\\n+\\\\\\\\n+# Generate output filename if not provided\\\\\\\\n+if not output_path:\\\\\\\\n+timestamp = datetime.now().strftime(\\\\\\\\\\\\\\\"%Y-%m-%d\\\\\\\\\\\\\\\")\\\\\\\\n+output_path = self.visualization_path / f\\\\\\\\\\\\\\\"documentation_graph_data_{timestamp}.json\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Save the graph data\\\\\\\\n+with open(output_path, 'w', encoding='utf-8') as f:\\\\\\\\n+json.dump(graph_data, f, indent=2)\\\\\\\\n+\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Graph data exported to: {output_path}\\\\\\\\\\\\\\\")\\\\\\\\n+return str(output_path)\\\\\\\\n+\\\\\\\\n+def analyze_graph(self) -> Dict[str, Any]:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Analyze the graph structure and generate metrics.\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Dictionary of graph metrics and insights\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+if not self.nodes or not self.edges:\\\\\\\\n+logger.error(\\\\\\\\\\\\\\\"No graph data available. Call build_graph() first.\\\\\\\\\\\\\\\")\\\\\\\\n+return {}\\\\\\\\n+\\\\\\\\n+# If NetworkX is available, use it for analysis\\\\\\\\n+if NETWORKX_AVAILABLE:\\\\\\\\n+return self._analyze_with_networkx()\\\\\\\\n+else:\\\\\\\\n+return self._analyze_basic()\\\\\\\\n+\\\\\\\\n+def _analyze_with_networkx(self) -> Dict[str, Any]:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Analyze graph using NetworkX metrics.\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Dictionary of graph metrics\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+# Create graph\\\\\\\\n+G = nx.DiGraph()\\\\\\\\n+\\\\\\\\n+# Add nodes\\\\\\\\n+for node in self.nodes:\\\\\\\\n+G.add_node(node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+# Add edges\\\\\\\\n+for edge in self.edges:\\\\\\\\n+G.add_edge(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"], edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+# Calculate metrics\\\\\\\\n+metrics = {\\\\\\\\n+\\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(G),\\\\\\\\n+\\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(G.edges()),\\\\\\\\n+\\\\\\\\\\\\\\\"density\\\\\\\\\\\\\\\": nx.density(G),\\\\\\\\n+}\\\\\\\\n+\\\\\\\\n+# Calculate centrality measures\\\\\\\\n+try:\\\\\\\\n+in_degree_centrality = nx.in_degree_centrality(G)\\\\\\\\n+out_degree_centrality = nx.out_degree_centrality(G)\\\\\\\\n+betweenness_centrality = nx.betweenness_centrality(G)\\\\\\\\n+\\\\\\\\n+# Sort by value to find highest centrality nodes\\\\\\\\n+top_in_degree = sorted(in_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n+top_out_degree = sorted(out_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n+top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\\\\\\\n+\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_in_degree]\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"most_referencing_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_out_degree]\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": round(score, 3)} for doc, score in top_betweenness]\\\\\\\\n+except:\\\\\\\\n+logger.warning(\\\\\\\\\\\\\\\"Error calculating centrality measures\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Find isolated nodes (no connections)\\\\\\\\n+isolated = list(nx.isolates(G))\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"] = isolated\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\"] = len(isolated)\\\\\\\\n+\\\\\\\\n+# Find strongly connected components\\\\\\\\n+strongly_connected = list(nx.strongly_connected_components(G))\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"strongly_connected_components\\\\\\\\\\\\\\\"] = len(strongly_connected)\\\\\\\\n+\\\\\\\\n+# Find largest strongly connected component\\\\\\\\n+if strongly_connected:\\\\\\\\n+largest_component = max(strongly_connected, key=len)\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"largest_component_size\\\\\\\\\\\\\\\"] = len(largest_component)\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"largest_component_ratio\\\\\\\\\\\\\\\"] = len(largest_component) / len(G)\\\\\\\\n+\\\\\\\\n+return metrics\\\\\\\\n+\\\\\\\\n+def _analyze_basic(self) -> Dict[str, Any]:\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Perform basic graph analysis without NetworkX.\\\\\\\\n+\\\\\\\\n+Returns:\\\\\\\\n+Dictionary of basic graph metrics\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+# Count nodes and edges\\\\\\\\n+metrics = {\\\\\\\\n+\\\\\\\\\\\\\\\"node_count\\\\\\\\\\\\\\\": len(self.nodes),\\\\\\\\n+\\\\\\\\\\\\\\\"edge_count\\\\\\\\\\\\\\\": len(self.edges),\\\\\\\\n+}\\\\\\\\n+\\\\\\\\n+# Count nodes by subsystem\\\\\\\\n+subsystem_counts = {}\\\\\\\\n+for node in self.nodes:\\\\\\\\n+subsystem = node.get(\\\\\\\\\\\\\\\"subsystem\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"default\\\\\\\\\\\\\\\")\\\\\\\\n+subsystem_counts[subsystem] = subsystem_counts.get(subsystem, 0) + 1\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"subsystem_counts\\\\\\\\\\\\\\\"] = subsystem_counts\\\\\\\\n+\\\\\\\\n+# Find nodes with most connections\\\\\\\\n+node_connections = {}\\\\\\\\n+for edge in self.edges:\\\\\\\\n+source = edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"]\\\\\\\\n+target = edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"]\\\\\\\\n+\\\\\\\\n+# Count outgoing connections\\\\\\\\n+node_connections.setdefault(source, {\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\": 0, \\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\": 0})\\\\\\\\n+node_connections[source][\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\"] += 1\\\\\\\\n+\\\\\\\\n+# Count incoming connections\\\\\\\\n+node_connections.setdefault(target, {\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\": 0, \\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\": 0})\\\\\\\\n+node_connections[target][\\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\"] += 1\\\\\\\\n+\\\\\\\\n+# Find most referenced documents\\\\\\\\n+most_referenced = sorted(\\\\\\\\n+[(node, data[\\\\\\\\\\\\\\\"incoming\\\\\\\\\\\\\\\"]) for node, data in node_connections.items()],\\\\\\\\n+key=lambda x: x[1],\\\\\\\\n+reverse=True\\\\\\\\n+)[:10]\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"count\\\\\\\\\\\\\\\": count} for doc, count in most_referenced]\\\\\\\\n+\\\\\\\\n+# Find most referencing documents\\\\\\\\n+most_referencing = sorted(\\\\\\\\n+[(node, data[\\\\\\\\\\\\\\\"outgoing\\\\\\\\\\\\\\\"]) for node, data in node_connections.items()],\\\\\\\\n+key=lambda x: x[1],\\\\\\\\n+reverse=True\\\\\\\\n+)[:10]\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"most_referencing_docs\\\\\\\\\\\\\\\"] = [{\\\\\\\\\\\\\\\"doc\\\\\\\\\\\\\\\": doc, \\\\\\\\\\\\\\\"count\\\\\\\\\\\\\\\": count} for doc, count in most_referencing]\\\\\\\\n+\\\\\\\\n+# Find isolated documents (no connections)\\\\\\\\n+connected_nodes = set()\\\\\\\\n+for edge in self.edges:\\\\\\\\n+connected_nodes.add(edge[\\\\\\\\\\\\\\\"from\\\\\\\\\\\\\\\"])\\\\\\\\n+connected_nodes.add(edge[\\\\\\\\\\\\\\\"to\\\\\\\\\\\\\\\"])\\\\\\\\n+\\\\\\\\n+all_node_ids = set(node[\\\\\\\\\\\\\\\"id\\\\\\\\\\\\\\\"] for node in self.nodes)\\\\\\\\n+isolated = all_node_ids - connected_nodes\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"] = list(isolated)\\\\\\\\n+metrics[\\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\"] = len(isolated)\\\\\\\\n+\\\\\\\\n+return metrics\\\\\\\\n \\\\\\\\n \\\\\\\\n def main():\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Main entry point for the documentation graph generator.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    parser = argparse.ArgumentParser(\\\\\\\\n-        description=\\\\\\\\\\\\\\\"Generate interactive visualization of EGOS documentation connections\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--base-path\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-b\\\\\\\\\\\\\\\", default=\\\\\\\\\\\\\\\".\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Base path of the EGOS project\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--report\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-r\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Path to the cross-reference analysis JSON report\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--output\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-o\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Path to save the visualization\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--format\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-f\\\\\\\\\\\\\\\", choices=[\\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"], default=\\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Output format (default: all available formats)\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    parser.add_argument(\\\\\\\\n-        \\\\\\\\\\\\\\\"--analyze\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-a\\\\\\\\\\\\\\\", action=\\\\\\\\\\\\\\\"store_true\\\\\\\\\\\\\\\",\\\\\\\\n-        help=\\\\\\\\\\\\\\\"Analyze graph and print metrics\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    \\\\\\\\n-    args = parser.parse_args()\\\\\\\\n-    \\\\\\\\n-    try:\\\\\\\\n-        # Create graph generator\\\\\\\\n-        generator = DocumentationGraphGenerator(args.base_path)\\\\\\\\n-        \\\\\\\\n-        # Load report data\\\\\\\\n-        if not generator.load_report_data(args.report):\\\\\\\\n-            return 1\\\\\\\\n-            \\\\\\\\n-        # --- Start of new try block for core logic ---\\\\\\\\n-        try:\\\\\\\\n-            # Build graph\\\\\\\\n-            generator.build_graph()\\\\\\\\n-            \\\\\\\\n-            # Generate visualizations based on format\\\\\\\\n-            if args.format in [\\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"] and PYVIS_AVAILABLE:\\\\\\\\n-                generator.generate_pyvis_visualization(\\\\\\\\n-                    args.output if args.format == \\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\" else None\\\\\\\\n-                )\\\\\\\\n-                \\\\\\\\n-            if args.format in [\\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"] and NETWORKX_AVAILABLE:\\\\\\\\n-                generator.generate_networkx_visualization(\\\\\\\\n-                    args.output if args.format == \\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\" else None\\\\\\\\n-                )\\\\\\\\n-                \\\\\\\\n-            if args.format in [\\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"]:\\\\\\\\n-                generator.export_graph_data(\\\\\\\\n-                    args.output if args.format == \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\" else None\\\\\\\\n-                )\\\\\\\\n-                \\\\\\\\n-            # Analyze graph if requested\\\\\\\\n-            if args.analyze:\\\\\\\\n-                analysis = generator.analyze_graph()\\\\\\\\n-                \\\\\\\\n-                # Print analysis results using rich tables if available\\\\\\\\n-                console.print(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n [bold cyan]DOCUMENTATION GRAPH ANALYSIS[/bold cyan]\\\\\\\\\\\\\\\")\\\\\\\\n-                \\\\\\\\n-                summary_table = Table(title=\\\\\\\\\\\\\\\"Graph Summary\\\\\\\\\\\\\\\", show_header=False, box=None)\\\\\\\\n-                summary_table.add_row(\\\\\\\\\\\\\\\"Nodes:\\\\\\\\\\\\\\\", str(analysis.get('node_count', 'N/A')))\\\\\\\\n-                summary_table.add_row(\\\\\\\\\\\\\\\"Edges:\\\\\\\\\\\\\\\", str(analysis.get('edge_count', 'N/A')))\\\\\\\\n-                if \\\\\\\\\\\\\\\"density\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n-                    summary_table.add_row(\\\\\\\\\\\\\\\"Graph Density:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"{analysis['density']:.4f}\\\\\\\\\\\\\\\")\\\\\\\\n-                if \\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n-                    summary_table.add_row(\\\\\\\\\\\\\\\"Isolated Documents:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"[yellow]{analysis['isolated_count']}[/yellow]\\\\\\\\\\\\\\\")\\\\\\\\n-                if \\\\\\\\\\\\\\\"strongly_connected_components\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n-                    summary_table.add_row(\\\\\\\\\\\\\\\"Strongly Connected Components:\\\\\\\\\\\\\\\", str(analysis['strongly_connected_components']))\\\\\\\\n-                if \\\\\\\\\\\\\\\"largest_component_size\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n-                    ratio = analysis.get('largest_component_ratio', 0)\\\\\\\\n-                    summary_table.add_row(\\\\\\\\\\\\\\\"Largest Component Size:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"{analysis['largest_component_size']} ({ratio:.1%})\\\\\\\\\\\\\\\")\\\\\\\\n-                console.print(summary_table)\\\\\\\\n-\\\\\\\\n-                # --- Most Referenced Documents Table ---\\\\\\\\n-                if \\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n-                    ref_table = Table(title=\\\\\\\\\\\\\\\" Most Referenced Documents (Top 5)\\\\\\\\\\\\\\\", expand=True)\\\\\\\\n-                    ref_table.add_column(\\\\\\\\\\\\\\\"Rank\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"dim\\\\\\\\\\\\\\\", width=5)\\\\\\\\n-                    ref_table.add_column(\\\\\\\\\\\\\\\"Document\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"magenta\\\\\\\\\\\\\\\")\\\\\\\\n-                    ref_table.add_column(\\\\\\\\\\\\\\\"Score/Count\\\\\\\\\\\\\\\", justify=\\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"green\\\\\\\\\\\\\\\")\\\\\\\\n-                    \\\\\\\\n-                    for i, doc_info in enumerate(analysis[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"][:5], 1):\\\\\\\\n-                        score = doc_info.get('score', doc_info.get('count', 'N/A'))\\\\\\\\n-                        score_str = f\\\\\\\\\\\\\\\"{score:.3f}\\\\\\\\\\\\\\\" if isinstance(score, float) else str(score)\\\\\\\\n-                        ref_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\\\\\\\n-                    console.print(ref_table)\\\\\\\\n-\\\\\\\\n-                # --- Bridge Documents Table ---\\\\\\\\n-                if \\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n-                    bridge_table = Table(title=\\\\\\\\\\\\\\\" Bridge Documents (High Betweenness - Top 5)\\\\\\\\\\\\\\\", expand=True)\\\\\\\\n-                    bridge_table.add_column(\\\\\\\\\\\\\\\"Rank\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"dim\\\\\\\\\\\\\\\", width=5)\\\\\\\\n-                    bridge_table.add_column(\\\\\\\\\\\\\\\"Document\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"cyan\\\\\\\\\\\\\\\")\\\\\\\\n-                    bridge_table.add_column(\\\\\\\\\\\\\\\"Score\\\\\\\\\\\\\\\", justify=\\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"blue\\\\\\\\\\\\\\\")\\\\\\\\n-                    \\\\\\\\n-                    for i, doc_info in enumerate(analysis[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"][:5], 1):\\\\\\\\n-                        score = doc_info.get('score', 'N/A')\\\\\\\\n-                        score_str = f\\\\\\\\\\\\\\\"{score:.3f}\\\\\\\\\\\\\\\" if isinstance(score, float) else str(score)\\\\\\\\n-                        bridge_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\\\\\\\n-                    console.print(bridge_table)\\\\\\\\n-\\\\\\\\n-                # --- Isolated Documents --- \\\\\\\\n-                if \\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n-                     console.print(f\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n [yellow]Isolated Documents ({analysis['isolated_count']}):[/yellow]\\\\\\\\\\\\\\\")\\\\\\\\n-                     # Print only first few for brevity\\\\\\\\n-                     for doc in analysis['isolated_docs'][:10]:\\\\\\\\n-                         console.print(f\\\\\\\\\\\\\\\"  - {os.path.basename(doc)}\\\\\\\\\\\\\\\")\\\\\\\\n-                     if len(analysis['isolated_docs']) > 10:\\\\\\\\n-                         console.print(f\\\\\\\\\\\\\\\"  ... and {len(analysis['isolated_docs']) - 10} more\\\\\\\\\\\\\\\")\\\\\\\\n-                \\\\\\\\n-                # Save analysis to JSON\\\\\\\\n-                analysis_path = generator.visualization_path / \\\\\\\\\\\\\\\"documentation_graph_analysis.json\\\\\\\\\\\\\\\"\\\\\\\\n-                with open(analysis_path, 'w', encoding='utf-8') as f:\\\\\\\\n-                    json.dump(analysis, f, indent=2)\\\\\\\\n-                logger.info(f\\\\\\\\\\\\\\\"Analysis saved to: {analysis_path}\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-        # --- End of new try block, add except block ---\\\\\\\\n-        except Exception as core_error:\\\\\\\\n-            logger.error(f\\\\\\\\\\\\\\\"!!! Critical Error during graph processing/visualization: {core_error}\\\\\\\\\\\\\\\")\\\\\\\\n-            logger.error(traceback.format_exc())\\\\\\\\n-            return 1 # Ensure exit with error code\\\\\\\\n-                \\\\\\\\n-        return 0\\\\\\\\n-            \\\\\\\\n-    # Outer except block remains for initialization/loading errors\\\\\\\\n-    except Exception as e:\\\\\\\\n-        logger.error(f\\\\\\\\\\\\\\\"Initialization or Loading Error: {str(e)}\\\\\\\\\\\\\\\")\\\\\\\\n-        # Ensure traceback is imported if needed here too\\\\\\\\n-        if 'traceback' not in sys.modules:\\\\\\\\n-            import traceback\\\\\\\\n-        logger.error(traceback.format_exc())\\\\\\\\n-        return 1\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Main entry point for the documentation graph generator.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+parser = argparse.ArgumentParser(\\\\\\\\n+description=\\\\\\\\\\\\\\\"Generate interactive visualization of EGOS documentation connections\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--base-path\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-b\\\\\\\\\\\\\\\", default=\\\\\\\\\\\\\\\".\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Base path of the EGOS project\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--report\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-r\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Path to the cross-reference analysis JSON report\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--output\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-o\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Path to save the visualization\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--format\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-f\\\\\\\\\\\\\\\", choices=[\\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"], default=\\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Output format (default: all available formats)\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+parser.add_argument(\\\\\\\\n+\\\\\\\\\\\\\\\"--analyze\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"-a\\\\\\\\\\\\\\\", action=\\\\\\\\\\\\\\\"store_true\\\\\\\\\\\\\\\",\\\\\\\\n+help=\\\\\\\\\\\\\\\"Analyze graph and print metrics\\\\\\\\\\\\\\\"\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+args = parser.parse_args()\\\\\\\\n+\\\\\\\\n+try:\\\\\\\\n+# Create graph generator\\\\\\\\n+generator = DocumentationGraphGenerator(args.base_path)\\\\\\\\n+\\\\\\\\n+# Load report data\\\\\\\\n+if not generator.load_report_data(args.report):\\\\\\\\n+return 1\\\\\\\\n+\\\\\\\\n+# --- Start of new try block for core logic ---\\\\\\\\n+try:\\\\\\\\n+# Build graph\\\\\\\\n+generator.build_graph()\\\\\\\\n+\\\\\\\\n+# Generate visualizations based on format\\\\\\\\n+if args.format in [\\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"] and PYVIS_AVAILABLE:\\\\\\\\n+generator.generate_pyvis_visualization(\\\\\\\\n+args.output if args.format == \\\\\\\\\\\\\\\"html\\\\\\\\\\\\\\\" else None\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+if args.format in [\\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"] and NETWORKX_AVAILABLE:\\\\\\\\n+generator.generate_networkx_visualization(\\\\\\\\n+args.output if args.format == \\\\\\\\\\\\\\\"png\\\\\\\\\\\\\\\" else None\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+if args.format in [\\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"all\\\\\\\\\\\\\\\"]:\\\\\\\\n+generator.export_graph_data(\\\\\\\\n+args.output if args.format == \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\" else None\\\\\\\\n+)\\\\\\\\n+\\\\\\\\n+# Analyze graph if requested\\\\\\\\n+if args.analyze:\\\\\\\\n+analysis = generator.analyze_graph()\\\\\\\\n+\\\\\\\\n+# Print analysis results using rich tables if available\\\\\\\\n+console.print(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n [bold cyan]DOCUMENTATION GRAPH ANALYSIS[/bold cyan]\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+summary_table = Table(title=\\\\\\\\\\\\\\\"Graph Summary\\\\\\\\\\\\\\\", show_header=False, box=None)\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Nodes:\\\\\\\\\\\\\\\", str(analysis.get('node_count', 'N/A')))\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Edges:\\\\\\\\\\\\\\\", str(analysis.get('edge_count', 'N/A')))\\\\\\\\n+if \\\\\\\\\\\\\\\"density\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Graph Density:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"{analysis['density']:.4f}\\\\\\\\\\\\\\\")\\\\\\\\n+if \\\\\\\\\\\\\\\"isolated_count\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Isolated Documents:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"[yellow]{analysis['isolated_count']}[/yellow]\\\\\\\\\\\\\\\")\\\\\\\\n+if \\\\\\\\\\\\\\\"strongly_connected_components\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Strongly Connected Components:\\\\\\\\\\\\\\\", str(analysis['strongly_connected_components']))\\\\\\\\n+if \\\\\\\\\\\\\\\"largest_component_size\\\\\\\\\\\\\\\" in analysis:\\\\\\\\n+ratio = analysis.get('largest_component_ratio', 0)\\\\\\\\n+summary_table.add_row(\\\\\\\\\\\\\\\"Largest Component Size:\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"{analysis['largest_component_size']} ({ratio:.1%})\\\\\\\\\\\\\\\")\\\\\\\\n+console.print(summary_table)\\\\\\\\n+\\\\\\\\n+# --- Most Referenced Documents Table ---\\\\\\\\n+if \\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n+ref_table = Table(title=\\\\\\\\\\\\\\\" Most Referenced Documents (Top 5)\\\\\\\\\\\\\\\", expand=True)\\\\\\\\n+ref_table.add_column(\\\\\\\\\\\\\\\"Rank\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"dim\\\\\\\\\\\\\\\", width=5)\\\\\\\\n+ref_table.add_column(\\\\\\\\\\\\\\\"Document\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"magenta\\\\\\\\\\\\\\\")\\\\\\\\n+ref_table.add_column(\\\\\\\\\\\\\\\"Score/Count\\\\\\\\\\\\\\\", justify=\\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"green\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+for i, doc_info in enumerate(analysis[\\\\\\\\\\\\\\\"most_referenced_docs\\\\\\\\\\\\\\\"][:5], 1):\\\\\\\\n+score = doc_info.get('score', doc_info.get('count', 'N/A'))\\\\\\\\n+score_str = f\\\\\\\\\\\\\\\"{score:.3f}\\\\\\\\\\\\\\\" if isinstance(score, float) else str(score)\\\\\\\\n+ref_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\\\\\\\n+console.print(ref_table)\\\\\\\\n+\\\\\\\\n+# --- Bridge Documents Table ---\\\\\\\\n+if \\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n+bridge_table = Table(title=\\\\\\\\\\\\\\\" Bridge Documents (High Betweenness - Top 5)\\\\\\\\\\\\\\\", expand=True)\\\\\\\\n+bridge_table.add_column(\\\\\\\\\\\\\\\"Rank\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"dim\\\\\\\\\\\\\\\", width=5)\\\\\\\\n+bridge_table.add_column(\\\\\\\\\\\\\\\"Document\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"cyan\\\\\\\\\\\\\\\")\\\\\\\\n+bridge_table.add_column(\\\\\\\\\\\\\\\"Score\\\\\\\\\\\\\\\", justify=\\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\", style=\\\\\\\\\\\\\\\"blue\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+for i, doc_info in enumerate(analysis[\\\\\\\\\\\\\\\"bridge_docs\\\\\\\\\\\\\\\"][:5], 1):\\\\\\\\n+score = doc_info.get('score', 'N/A')\\\\\\\\n+score_str = f\\\\\\\\\\\\\\\"{score:.3f}\\\\\\\\\\\\\\\" if isinstance(score, float) else str(score)\\\\\\\\n+bridge_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\\\\\\\n+console.print(bridge_table)\\\\\\\\n+\\\\\\\\n+# --- Isolated Documents ---\\\\\\\\n+if \\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\" in analysis and analysis[\\\\\\\\\\\\\\\"isolated_docs\\\\\\\\\\\\\\\"]:\\\\\\\\n+console.print(f\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n [yellow]Isolated Documents ({analysis['isolated_count']}):[/yellow]\\\\\\\\\\\\\\\")\\\\\\\\n+# Print only first few for brevity\\\\\\\\n+for doc in analysis['isolated_docs'][:10]:\\\\\\\\n+console.print(f\\\\\\\\\\\\\\\"  - {os.path.basename(doc)}\\\\\\\\\\\\\\\")\\\\\\\\n+if len(analysis['isolated_docs']) > 10:\\\\\\\\n+console.print(f\\\\\\\\\\\\\\\"  ... and {len(analysis['isolated_docs']) - 10} more\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# Save analysis to JSON\\\\\\\\n+analysis_path = generator.visualization_path / \\\\\\\\\\\\\\\"documentation_graph_analysis.json\\\\\\\\\\\\\\\"\\\\\\\\n+with open(analysis_path, 'w', encoding='utf-8') as f:\\\\\\\\n+json.dump(analysis, f, indent=2)\\\\\\\\n+logger.info(f\\\\\\\\\\\\\\\"Analysis saved to: {analysis_path}\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+# --- End of new try block, add except block ---\\\\\\\\n+except Exception as core_error:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"!!! Critical Error during graph processing/visualization: {core_error}\\\\\\\\\\\\\\\")\\\\\\\\n+logger.error(traceback.format_exc())\\\\\\\\n+return 1 # Ensure exit with error code\\\\\\\\n+\\\\\\\\n+return 0\\\\\\\\n+\\\\\\\\n+# Outer except block remains for initialization/loading errors\\\\\\\\n+except Exception as e:\\\\\\\\n+logger.error(f\\\\\\\\\\\\\\\"Initialization or Loading Error: {str(e)}\\\\\\\\\\\\\\\")\\\\\\\\n+# Ensure traceback is imported if needed here too\\\\\\\\n+if 'traceback' not in sys.modules:\\\\\\\\n+import traceback\\\\\\\\n+logger.error(traceback.format_exc())\\\\\\\\n+return 1\\\\\\\\n \\\\\\\\n \\\\\\\\n if __name__ == \\\\\\\\\\\\\\\"__main__\\\\\\\\\\\\\\\":\\\\\\\\n-    sys.exit(main())+sys.exit(main())\\\\\\\\n+\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\"\\\"\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_SeeAlso_results.json",
    "LineNumber": 5,
    "LineContent": "    \"LineContent\": \"            *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, `Related:`, `Doc:`, `Link to:`).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_Source_results.json",
    "LineNumber": 10,
    "LineContent": "    \"LineContent\": \"            *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, `Related:`, `Doc:`, `Link to:`).\""
  },
  {
    "File": "C:\\EGOS\\docs_egos\\00_project_overview\\ROADMAP.md",
    "LineNumber": 64,
    "LineContent": "*   **[MVP-DEFINE-SPARC-SVC]** **Define MVP - SPARC Service:** Define MVP for a 'SPARC-based Project Analysis & Refactoring Service', outlining core features, target outcomes, required subsystems, and potential delivery model. (`HIGH`) `Status: Planned`, `linked_doc: docs_egos/strategy/MVP_Definition.md`"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\01_core_principles\\core_materials\\historical_changelogs\\PLANO DE NEGOCIO EVA (1).txt",
    "LineNumber": 714,
    "LineContent": "- Backend ReDoc: http://localhost:8000/redoc"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\09_project_meta\\reports\\html_comment_report_20250425_092241.json",
    "LineNumber": 1559,
    "LineContent": "      \"diff\": \"--- a/generate_documentation_graph.py\\n+++ b/generate_documentation_graph.py\\n@@ -1,10 +1,10 @@\\n-<!-- \\n+\\\"\\\"\\\"\\n @references:\\n - Core References:\\n-  - [MQP.md](mdc:../../MQP.md) - Master Quantum Prompt defining EGOS principles\\n-  - [ROADMAP.md](mdc:../../ROADMAP.md) - Project roadmap and planning\\n+- [MQP.md](mdc:../../MQP.md) - Master Quantum Prompt defining EGOS principles\\n+- [ROADMAP.md](mdc:../../ROADMAP.md) - Project roadmap and planning\\n - Process Documentation:\\n-  - [system_maintenance.md](mdc:../../docs_egos/process/system_maintenance.md)\\n+- [system_maintenance.md](mdc:../../docs_egos/process/system_maintenance.md)\\n \\n \\n \\n@@ -42,809 +42,810 @@\\n \\n # Import Rich components for progress bars\\n from rich.progress import (\\n-    Progress,\\n-    SpinnerColumn,\\n-    TextColumn,\\n-    BarColumn,\\n-    TimeRemainingColumn,\\n-    TaskID,\\n+Progress,\\n+SpinnerColumn,\\n+TextColumn,\\n+BarColumn,\\n+TimeRemainingColumn,\\n+TaskID,\\n )\\n \\n # Setup logging\\n logging.basicConfig(\\n-    level=logging.INFO,\\n-    format=\\\"%(message)s\\\", # Rich handler handles time/level\\n-    datefmt=\\\"[%X]\\\",\\n-    handlers=[logging.StreamHandler()]\\n+level=logging.INFO,\\n+format=\\\"%(message)s\\\", # Rich handler handles time/level\\n+datefmt=\\\"[%X]\\\",\\n+handlers=[logging.StreamHandler()]\\n )\\n \\n logger = logging.getLogger(__name__)\\n \\n # Optional imports with fallbacks\\n try:\\n-    from pyvis.network import Network\\n-    PYVIS_AVAILABLE = True\\n+from pyvis.network import Network\\n+PYVIS_AVAILABLE = True\\n except ImportError:\\n-    logger.warning(\\\"Pyvis not installed. Will use simplified visualization.\\\")\\n-    PYVIS_AVAILABLE = False\\n+logger.warning(\\\"Pyvis not installed. Will use simplified visualization.\\\")\\n+PYVIS_AVAILABLE = False\\n \\n # NetworkX for basic/static visualization\\n try:\\n-    import networkx as nx\\n-    import matplotlib.pyplot as plt\\n-    NETWORKX_AVAILABLE = True\\n+import networkx as nx\\n+import matplotlib.pyplot as plt\\n+NETWORKX_AVAILABLE = True\\n except ImportError:\\n-    logger.warning(\\\"NetworkX not installed. Only JSON graph data will be generated.\\\")\\n-    NETWORKX_AVAILABLE = False\\n+logger.warning(\\\"NetworkX not installed. Only JSON graph data will be generated.\\\")\\n+NETWORKX_AVAILABLE = False\\n \\n \\n class DocumentationGraphGenerator:\\n-    \\\"\\\"\\\"Generates interactive visualizations of EGOS documentation connections.\\n-    \\n-    This class creates visual representations of the documentation cross-reference\\n-    network, helping identify key documents, clusters, and connection patterns.\\n-    \\\"\\\"\\\"\\n-    \\n-    def __init__(self, base_path: str = \\\".\\\"):\\n-        \\\"\\\"\\\"Initialize the graph generator.\\n-        \\n-        Args:\\n-            base_path: Base path of the EGOS project\\n-        \\\"\\\"\\\"\\n-        self.base_path = Path(base_path)\\n-        self.report_path = self.base_path / \\\"reports\\\" / \\\"documentation\\\"\\n-        self.visualization_path = self.report_path / \\\"visualizations\\\"\\n-        \\n-        # Ensure output directories exist\\n-        self.visualization_path.mkdir(parents=True, exist_ok=True)\\n-        \\n-        # Graph data\\n-        self.nodes = []\\n-        self.edges = []\\n-        self.subsystems = set()\\n-        \\n-        # Node attributes\\n-        self.node_sizes = {}\\n-        self.node_colors = {}\\n-        self.node_clusters = {}\\n-        \\n-        # Color palette for subsystems with EGOS-aligned color scheme\\n-        self.color_palette = {\\n-            \\\"koios\\\": theme.color.koios,    # Blue - Knowledge\\n-            \\\"cronos\\\": theme.color.cronos,   # Purple - Time\\n-            \\\"nexus\\\": theme.color.nexus,    # Green - Connections\\n-            \\\"ethik\\\": theme.color.ethik,    # Red - Ethics\\n-            \\\"atlas\\\": theme.color.atlas,    # Orange - Mapping\\n-            \\\"mycelium\\\": theme.color.mycelium, # Teal - Integration\\n-            \\\"harmony\\\": theme.color.harmony,  # Dark Blue - Harmony\\n-            \\\"default\\\": theme.color.default   # Gray - Default\\n-        }\\n-    \\n-    def load_report_data(self, report_path: Optional[str] = None) -> bool:\\n-        \\\"\\\"\\\"Load cross-reference report data from a JSON file.\\n-        \\n-        Args:\\n-            report_path: Path to the JSON report file (optional)\\n-            \\n-        Returns:\\n-            True if data was loaded successfully, False otherwise\\n-        \\\"\\\"\\\"\\n-        if report_path:\\n-            json_path = Path(report_path)\\n-        else:\\n-            # Look for the latest 'analysis' stage checkpoint file\\n-            checkpoint_dir = self.base_path / \\\"reports\\\" / \\\"documentation\\\" / \\\"checkpoints\\\"\\n-            analysis_checkpoints = list(checkpoint_dir.glob(\\\"xref_checkpoint_analysis_*.json\\\"))\\n-            \\n-            if not analysis_checkpoints:\\n-                # Fallback: Check for older format without timestamp\\n-                old_format_checkpoint = checkpoint_dir / \\\"xref_checkpoint_analysis.json\\\"\\n-                if old_format_checkpoint.exists():\\n-                    analysis_checkpoints.append(old_format_checkpoint)\\n-                else:\\n-                    logger.error(\\n-                        \\\"No analysis checkpoint found in reports/documentation/checkpoints/. \\\"\\n-                        \\\"Run add_cross_references.py --stage analysis first.\\\"\\n-                    )\\n-                    return False\\n-                    \\n-            # Get the most recent analysis checkpoint\\n-            json_path = max(analysis_checkpoints, key=os.path.getmtime)\\n-            logger.info(f\\\"Using latest analysis checkpoint: {json_path.name}\\\")\\n-        \\n-        logger.info(f\\\"Loading and processing graph data from: {json_path}\\\")\\n-        \\n-        try:\\n-            with open(json_path, 'r', encoding='utf-8') as f:\\n-                checkpoint_content = json.load(f)\\n-            \\n-            # Extract the core data list\\n-            raw_data = None\\n-            if isinstance(checkpoint_content, dict) and \\\"data\\\" in checkpoint_content:\\n-                if isinstance(checkpoint_content[\\\"data\\\"], list):\\n-                     raw_data = checkpoint_content[\\\"data\\\"]\\n-            elif isinstance(checkpoint_content, list):\\n-                 raw_data = checkpoint_content # Assume direct list data\\n-                 \\n-            if raw_data is None:\\n-                logger.error(f\\\"Could not extract valid list data from checkpoint {json_path.name}\\\")\\n-                return False\\n-                \\n-            # Process the list data to build the required dictionary structure\\n-            processed_data = {\\\"references\\\": {}, \\\"referenced_by\\\": {}}\\n-            all_docs = set()\\n-\\n-            # First pass: Build referenced_by and collect all docs\\n-            for item in raw_data:\\n-                # Expecting [doc_path, some_int, list_of_referencing_docs]\\n-                if len(item) == 3 and isinstance(item[0], str) and isinstance(item[2], list):\\n-                    doc = item[0]\\n-                    referencing_docs = item[2]\\n-                    all_docs.add(doc)\\n-                    processed_data[\\\"referenced_by\\\"][doc] = referencing_docs\\n-                    all_docs.update(referencing_docs) # Add referencing docs too\\n-                else:\\n-                     logger.warning(f\\\"Skipping malformed item in checkpoint data: {item}\\\")\\n-\\n-            # Initialize all docs in references dict\\n-            for doc in all_docs:\\n-                # Ensure doc path is cleaned/normalized if needed (optional)\\n-                # doc_clean = Path(doc).as_posix() # Example normalization\\n-                processed_data[\\\"references\\\"][doc] = [] \\n-                # Ensure referenced_by also has all docs initialized, even if empty initially\\n-                if doc not in processed_data[\\\"referenced_by\\\"]:\\n-                    processed_data[\\\"referenced_by\\\"][doc] = []\\n-                \\n-            # Second pass: Build references from referenced_by\\n-            for doc, referencing_docs in processed_data[\\\"referenced_by\\\"].items():\\n-                for ref_doc in referencing_docs:\\n-                    if ref_doc in processed_data[\\\"references\\\"]:\\n-                        processed_data[\\\"references\\\"][ref_doc].append(doc)\\n-                    else:\\n-                        # This case handles if a referencing doc wasn't in the initial list's first element\\n-                        # It means ref_doc references 'doc', but ref_doc itself might not have been a primary entry\\n-                        processed_data[\\\"references\\\"][ref_doc] = [doc]\\n-                        if ref_doc not in processed_data[\\\"referenced_by\\\"]:\\n-                           processed_data[\\\"referenced_by\\\"][ref_doc] = [] # Initialize if missing\\n-                        logger.debug(f\\\"Added '{ref_doc}' to structures during second pass.\\\")\\n-                        \\n-            self.report_data = processed_data\\n-            logger.info(f\\\"Successfully processed {len(self.report_data['references'])} documents from checkpoint.\\\")\\n-            return True\\n-                \\n-        except Exception as e:\\n-            logger.error(f\\\"Error loading and processing graph data: {str(e)}\\\")\\n-            import traceback # Added for better debugging\\n-            logger.error(traceback.format_exc()) # Added for better debugging\\n-            return False\\n-    \\n-    def build_graph(self) -> None:\\n-        \\\"\\\"\\\"Build the document graph from loaded report data.\\\"\\\"\\\"\\n-        if not hasattr(self, 'report_data'):\\n-            logger.error(\\\"No report data loaded. Call load_report_data() first.\\\")\\n-            return\\n-            \\n-        references = self.report_data[\\\"references\\\"]\\n-        referenced_by = self.report_data.get(\\\"referenced_by\\\", {})\\n-        \\n-        logger.info(\\\"Building documentation graph...\\\")\\n-        \\n-        # Create nodes for all documents\\n-        all_docs = set(references.keys())\\n-        for refs in references.values():\\n-            all_docs.update(refs)\\n-        \\n-        # Show progress during graph building\\n-        total_docs = len(all_docs)\\n-        logger.info(f\\\"Processing {total_docs} documents...\\\")\\n-        \\n-        if console:\\n-            with Progress(\\n-                SpinnerColumn(),\\n-                TextColumn(\\\"[progress.description]{task.description}\\\"),\\n-                BarColumn(),\\n-                TextColumn(\\\"[progress.percentage]{task.percentage:>3.0f}%\\\"),\\n-                TextColumn(\\\"({task.completed}/{task.total})\\\"),\\n-                TimeRemainingColumn(),\\n-                console=console,\\n-                transient=False,  # Keep progress visible after completion\\n-            ) as progress:\\n-                build_task = progress.add_task(\\\"Building graph nodes and edges...\\\", total=total_docs)\\n-                \\n-                # Process each document as a node\\n-                for i, doc in enumerate(all_docs):\\n-                    try: \\n-                        # Basic filtering for non-markdown files or complex paths\\n-                        if not doc.endswith(\\\".md\\\") or \\\"/\\\" in doc or r\\\"\\\\.\\\" in doc or \\\"\\\\\\\\\\\" in doc:\\n-                            logger.debug(f\\\"Skipping non-standard doc path: {doc}\\\")\\n-                            progress.update(build_task, advance=1)\\n-                            continue\\n-        \\n-                        # Extract subsystem from path\\n-                        subsystem = self._get_subsystem(doc)\\n-                        self.subsystems.add(subsystem)\\n-        \\n-                        # Add node with attributes\\n-                        node_id = doc\\n-                        node_label = os.path.basename(doc)\\n-                        # Ensure references and referenced_by are accessed safely\\n-                        doc_refs = references.get(doc, [])\\n-                        doc_referenced_by = referenced_by.get(doc, [])\\n-        \\n-                        node_title = self._generate_node_title(doc, doc_refs, doc_referenced_by) \\n-                        node_size = self._calculate_node_size(len(doc_refs), len(doc_referenced_by))\\n-                        node_color = self.color_palette.get(subsystem, self.color_palette[\\\"default\\\"])\\n-        \\n-                        self.nodes.append({\\n-                            \\\"id\\\": node_id,\\n-                            \\\"label\\\": node_label,\\n-                            \\\"title\\\": node_title,\\n-                            \\\"size\\\": node_size,\\n-                            \\\"color\\\": node_color,\\n-                            \\\"group\\\": subsystem\\n-                        })\\n-        \\n-                        # Add edges (references from this doc to others)\\n-                        for target_doc in doc_refs:\\n-                            # Skip self-references or invalid targets\\n-                            if target_doc == doc or not target_doc.endswith(\\\".md\\\") or \\\"/\\\" in target_doc or r\\\"\\\\.\\\" in target_doc or \\\"\\\\\\\\\\\" in target_doc:\\n-                                continue\\n-                            self.edges.append({\\\"from\\\": node_id, \\\"to\\\": target_doc, \\\"arrows\\\": \\\"to\\\"})\\n-        \\n-                        # Update progress\\n-                        progress.update(build_task, advance=1)\\n-        \\n-                    except Exception as node_error:\\n-                        logger.error(f\\\"Error processing node '{doc}': {node_error}\\\")\\n-                        # Still advance progress even if one node fails\\n-                        progress.update(build_task, advance=1)\\n-                        continue\\n-        else:\\n-            # Fallback for when rich is not available\\n-            logger.info(\\\"Building graph nodes and edges (basic logging)...\\\")\\n+\\\"\\\"\\\"Generates interactive visualizations of EGOS documentation connections.\\n+\\n+This class creates visual representations of the documentation cross-reference\\n+network, helping identify key documents, clusters, and connection patterns.\\n+\\\"\\\"\\\"\\n+\\n+def __init__(self, base_path: str = \\\".\\\"):\\n+\\\"\\\"\\\"Initialize the graph generator.\\n+\\n+Args:\\n+base_path: Base path of the EGOS project\\n+\\\"\\\"\\\"\\n+self.base_path = Path(base_path)\\n+self.report_path = self.base_path / \\\"reports\\\" / \\\"documentation\\\"\\n+self.visualization_path = self.report_path / \\\"visualizations\\\"\\n+\\n+# Ensure output directories exist\\n+self.visualization_path.mkdir(parents=True, exist_ok=True)\\n+\\n+# Graph data\\n+self.nodes = []\\n+self.edges = []\\n+self.subsystems = set()\\n+\\n+# Node attributes\\n+self.node_sizes = {}\\n+self.node_colors = {}\\n+self.node_clusters = {}\\n+\\n+# Color palette for subsystems with EGOS-aligned color scheme\\n+self.color_palette = {\\n+\\\"koios\\\": theme.color.koios,    # Blue - Knowledge\\n+\\\"cronos\\\": theme.color.cronos,   # Purple - Time\\n+\\\"nexus\\\": theme.color.nexus,    # Green - Connections\\n+\\\"ethik\\\": theme.color.ethik,    # Red - Ethics\\n+\\\"atlas\\\": theme.color.atlas,    # Orange - Mapping\\n+\\\"mycelium\\\": theme.color.mycelium, # Teal - Integration\\n+\\\"harmony\\\": theme.color.harmony,  # Dark Blue - Harmony\\n+\\\"default\\\": theme.color.default   # Gray - Default\\n+}\\n+\\n+def load_report_data(self, report_path: Optional[str] = None) -> bool:\\n+\\\"\\\"\\\"Load cross-reference report data from a JSON file.\\n+\\n+Args:\\n+report_path: Path to the JSON report file (optional)\\n+\\n+Returns:\\n+True if data was loaded successfully, False otherwise\\n+\\\"\\\"\\\"\\n+if report_path:\\n+json_path = Path(report_path)\\n+else:\\n+# Look for the latest 'analysis' stage checkpoint file\\n+checkpoint_dir = self.base_path / \\\"reports\\\" / \\\"documentation\\\" / \\\"checkpoints\\\"\\n+analysis_checkpoints = list(checkpoint_dir.glob(\\\"xref_checkpoint_analysis_*.json\\\"))\\n+\\n+if not analysis_checkpoints:\\n+# Fallback: Check for older format without timestamp\\n+old_format_checkpoint = checkpoint_dir / \\\"xref_checkpoint_analysis.json\\\"\\n+if old_format_checkpoint.exists():\\n+analysis_checkpoints.append(old_format_checkpoint)\\n+else:\\n+logger.error(\\n+\\\"No analysis checkpoint found in reports/documentation/checkpoints/. \\\"\\n+\\\"Run add_cross_references.py --stage analysis first.\\\"\\n+)\\n+return False\\n+\\n+# Get the most recent analysis checkpoint\\n+json_path = max(analysis_checkpoints, key=os.path.getmtime)\\n+logger.info(f\\\"Using latest analysis checkpoint: {json_path.name}\\\")\\n+\\n+logger.info(f\\\"Loading and processing graph data from: {json_path}\\\")\\n+\\n+try:\\n+with open(json_path, 'r', encoding='utf-8') as f:\\n+checkpoint_content = json.load(f)\\n+\\n+# Extract the core data list\\n+raw_data = None\\n+if isinstance(checkpoint_content, dict) and \\\"data\\\" in checkpoint_content:\\n+if isinstance(checkpoint_content[\\\"data\\\"], list):\\n+raw_data = checkpoint_content[\\\"data\\\"]\\n+elif isinstance(checkpoint_content, list):\\n+raw_data = checkpoint_content # Assume direct list data\\n+\\n+if raw_data is None:\\n+logger.error(f\\\"Could not extract valid list data from checkpoint {json_path.name}\\\")\\n+return False\\n+\\n+# Process the list data to build the required dictionary structure\\n+processed_data = {\\\"references\\\": {}, \\\"referenced_by\\\": {}}\\n+all_docs = set()\\n+\\n+# First pass: Build referenced_by and collect all docs\\n+for item in raw_data:\\n+# Expecting [doc_path, some_int, list_of_referencing_docs]\\n+if len(item) == 3 and isinstance(item[0], str) and isinstance(item[2], list):\\n+doc = item[0]\\n+referencing_docs = item[2]\\n+all_docs.add(doc)\\n+processed_data[\\\"referenced_by\\\"][doc] = referencing_docs\\n+all_docs.update(referencing_docs) # Add referencing docs too\\n+else:\\n+logger.warning(f\\\"Skipping malformed item in checkpoint data: {item}\\\")\\n+\\n+# Initialize all docs in references dict\\n+for doc in all_docs:\\n+# Ensure doc path is cleaned/normalized if needed (optional)\\n+# doc_clean = Path(doc).as_posix() # Example normalization\\n+processed_data[\\\"references\\\"][doc] = []\\n+# Ensure referenced_by also has all docs initialized, even if empty initially\\n+if doc not in processed_data[\\\"referenced_by\\\"]:\\n+processed_data[\\\"referenced_by\\\"][doc] = []\\n+\\n+# Second pass: Build references from referenced_by\\n+for doc, referencing_docs in processed_data[\\\"referenced_by\\\"].items():\\n+for ref_doc in referencing_docs:\\n+if ref_doc in processed_data[\\\"references\\\"]:\\n+processed_data[\\\"references\\\"][ref_doc].append(doc)\\n+else:\\n+# This case handles if a referencing doc wasn't in the initial list's first element\\n+# It means ref_doc references 'doc', but ref_doc itself might not have been a primary entry\\n+processed_data[\\\"references\\\"][ref_doc] = [doc]\\n+if ref_doc not in processed_data[\\\"referenced_by\\\"]:\\n+processed_data[\\\"referenced_by\\\"][ref_doc] = [] # Initialize if missing\\n+logger.debug(f\\\"Added '{ref_doc}' to structures during second pass.\\\")\\n+\\n+self.report_data = processed_data\\n+logger.info(f\\\"Successfully processed {len(self.report_data['references'])} documents from checkpoint.\\\")\\n+return True\\n+\\n+except Exception as e:\\n+logger.error(f\\\"Error loading and processing graph data: {str(e)}\\\")\\n+import traceback # Added for better debugging\\n+logger.error(traceback.format_exc()) # Added for better debugging\\n+return False\\n+\\n+def build_graph(self) -> None:\\n+\\\"\\\"\\\"Build the document graph from loaded report data.\\\"\\\"\\\"\\n+if not hasattr(self, 'report_data'):\\n+logger.error(\\\"No report data loaded. Call load_report_data() first.\\\")\\n+return\\n+\\n+references = self.report_data[\\\"references\\\"]\\n+referenced_by = self.report_data.get(\\\"referenced_by\\\", {})\\n+\\n+logger.info(\\\"Building documentation graph...\\\")\\n+\\n+# Create nodes for all documents\\n+all_docs = set(references.keys())\\n+for refs in references.values():\\n+all_docs.update(refs)\\n+\\n+# Show progress during graph building\\n+total_docs = len(all_docs)\\n+logger.info(f\\\"Processing {total_docs} documents...\\\")\\n+\\n+if console:\\n+with Progress(\\n+SpinnerColumn(),\\n+TextColumn(\\\"[progress.description]{task.description}\\\"),\\n+BarColumn(),\\n+TextColumn(\\\"[progress.percentage]{task.percentage:>3.0f}%\\\"),\\n+TextColumn(\\\"({task.completed}/{task.total})\\\"),\\n+TimeRemainingColumn(),\\n+console=console,\\n+transient=False,  # Keep progress visible after completion\\n+) as progress:\\n+build_task = progress.add_task(\\\"Building graph nodes and edges...\\\", total=total_docs)\\n+\\n+# Process each document as a node\\n+for i, doc in enumerate(all_docs):\\n+try:\\n+# Basic filtering for non-markdown files or complex paths\\n+if not doc.endswith(\\\".md\\\") or \\\"/\\\" in doc or r\\\"\\\\.\\\" in doc or \\\"\\\\\\\\\\\" in doc:\\n+logger.debug(f\\\"Skipping non-standard doc path: {doc}\\\")\\n+progress.update(build_task, advance=1)\\n+continue\\n+\\n+# Extract subsystem from path\\n+subsystem = self._get_subsystem(doc)\\n+self.subsystems.add(subsystem)\\n+\\n+# Add node with attributes\\n+node_id = doc\\n+node_label = os.path.basename(doc)\\n+# Ensure references and referenced_by are accessed safely\\n+doc_refs = references.get(doc, [])\\n+doc_referenced_by = referenced_by.get(doc, [])\\n+\\n+node_title = self._generate_node_title(doc, doc_refs, doc_referenced_by)\\n+node_size = self._calculate_node_size(len(doc_refs), len(doc_referenced_by))\\n+node_color = self.color_palette.get(subsystem, self.color_palette[\\\"default\\\"])\\n+\\n+self.nodes.append({\\n+\\\"id\\\": node_id,\\n+\\\"label\\\": node_label,\\n+\\\"title\\\": node_title,\\n+\\\"size\\\": node_size,\\n+\\\"color\\\": node_color,\\n+\\\"group\\\": subsystem\\n+})\\n+\\n+# Add edges (references from this doc to others)\\n+for target_doc in doc_refs:\\n+# Skip self-references or invalid targets\\n+if target_doc == doc or not target_doc.endswith(\\\".md\\\") or \\\"/\\\" in target_doc or r\\\"\\\\.\\\" in target_doc or \\\"\\\\\\\\\\\" in target_doc:\\n+continue\\n+self.edges.append({\\\"from\\\": node_id, \\\"to\\\": target_doc, \\\"arrows\\\": \\\"to\\\"})\\n+\\n+# Update progress\\n+progress.update(build_task, advance=1)\\n+\\n+except Exception as node_error:\\n+logger.error(f\\\"Error processing node '{doc}': {node_error}\\\")\\n+# Still advance progress even if one node fails\\n+progress.update(build_task, advance=1)\\n+continue\\n+else:\\n+# Fallback for when rich is not available\\n+logger.info(\\\"Building graph nodes and edges (basic logging)...\\\")\\n {{ ... }}\\n-        logger.info(f\\\"Graph built successfully: {len(self.nodes)} nodes, {len(self.edges)} edges.\\\")\\n-\\n-    def generate_pyvis_visualization(self, output_path: Optional[str] = None) -> str:\\n-        \\\"\\\"\\\"Generate an interactive HTML visualization using Pyvis.\\n-        \\n-        Args:\\n-            output_path: Path to save the HTML file (optional)\\n-            \\n-        Returns:\\n-            Path to the generated HTML file\\n-        \\\"\\\"\\\"\\n-        if not hasattr(self, 'nodes') or not self.nodes:\\n-            logger.error(\\\"No graph data to visualize. Call build_graph() first.\\\")\\n-            return \\\"\\\"\\n-            \\n-        # Create network\\n-        timestamp = datetime.now().strftime(\\\"%Y-%m-%d\\\")\\n-        output_filename = f\\\"documentation_graph_{timestamp}.html\\\"\\n-        \\n-        if output_path:\\n-            output_file = Path(output_path)\\n-        else:\\n-            output_file = self.visualization_path / output_filename\\n-            \\n-        # Log the start of visualization generation\\n-        logger.info(f\\\"Generating interactive visualization ({len(self.nodes)} nodes, {len(self.edges)} edges)...\\\")\\n-        \\n-        if console:\\n-            with Progress(\\n-                SpinnerColumn(),\\n-                TextColumn(\\\"[progress.description]{task.description}\\\"),\\n-                BarColumn(),\\n-                TextColumn(\\\"[progress.percentage]{task.percentage:>3.0f}%\\\"),\\n-                TimeRemainingColumn(),\\n-                console=console,\\n-                transient=False, # Keep progress visible after completion\\n-            ) as progress:\\n-                # Create network\\n-                net_task = progress.add_task(\\\"Creating network...\\\", total=1)\\n-                net = Network(\\n-                    height=\\\"800px\\\", \\n-                    width=\\\"100%\\\", \\n-                    bgcolor=theme.color.white, \\n-                    font_color=theme.color.text_primary,\\n-                    heading=f\\\"EGOS Documentation Network ({timestamp})\\\"\\n-                )\\n-                progress.update(net_task, completed=1)\\n-                \\n-                # Add nodes with progress\\n-                node_task = progress.add_task(\\\"Adding nodes...\\\", total=len(self.nodes))\\n-                for node in self.nodes:\\n-                    net.add_node(\\n-                        node[\\\"id\\\"], \\n-                        label=node[\\\"label\\\"], \\n-                        title=node[\\\"title\\\"],\\n-                        color=node[\\\"color\\\"],\\n-                        size=node[\\\"size\\\"],\\n-                        group=node[\\\"group\\\"]\\n-                    )\\n-                    progress.update(node_task, advance=1)\\n-                \\n-                # Add edges with progress\\n-                edge_task = progress.add_task(\\\"Adding edges...\\\", total=len(self.edges))\\n-                for edge in self.edges:\\n-                    net.add_edge(\\n-                        edge[\\\"from\\\"], \\n-                        edge[\\\"to\\\"], \\n-                        arrows=edge.get(\\\"arrows\\\", \\\"to\\\"),\\n-                        title=edge.get(\\\"title\\\", \\\"\\\")\\n-                    )\\n-                    progress.update(edge_task, advance=1)\\n-                \\n-                # Configure physics for better readability\\n-                save_task = progress.add_task(\\\"Configuring and saving...\\\", total=1)\\n-                net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\n-                net.set_options(\\\"\\\"\\\"\\n-                {\\n-                  \\\"physics\\\": {\\n-                    \\\"forceAtlas2Based\\\": {\\n-                      \\\"gravitationalConstant\\\": -100,\\n-                      \\\"centralGravity\\\": 0.01,\\n-                      \\\"springLength\\\": 200,\\n-                      \\\"springConstant\\\": 0.08\\n-                    },\\n-                    \\\"solver\\\": \\\"forceAtlas2Based\\\",\\n-                    \\\"stabilization\\\": {\\n-                      \\\"iterations\\\": 100\\n-                    }\\n-                  }\\n-                }\\n-                \\\"\\\"\\\")\\n-                \\n-                # Save the network\\n-                net.save_graph(str(output_file))\\n-                progress.update(save_task, completed=1)\\n-        else:\\n-            # Fallback for when rich is not available\\n-            logger.info(\\\"Generating visualization (basic logging)...\\\")\\n-            net = Network(\\n-                height=\\\"800px\\\", \\n-                width=\\\"100%\\\", \\n-                bgcolor=theme.color.white, \\n-                font_color=theme.color.text_primary,\\n-                heading=f\\\"EGOS Documentation Network ({timestamp})\\\"\\n-            )\\n-            \\n-            # Add nodes\\n-            for node in self.nodes:\\n-                net.add_node(\\n-                    node[\\\"id\\\"], \\n-                    label=node[\\\"label\\\"], \\n-                    title=node[\\\"title\\\"],\\n-                    color=node[\\\"color\\\"],\\n-                    size=node[\\\"size\\\"],\\n-                    group=node[\\\"group\\\"]\\n-                )\\n-            \\n-            # Add edges\\n-            for edge in self.edges:\\n-                net.add_edge(\\n-                    edge[\\\"from\\\"], \\n-                    edge[\\\"to\\\"], \\n-                    arrows=edge.get(\\\"arrows\\\", \\\"to\\\"),\\n-                    title=edge.get(\\\"title\\\", \\\"\\\")\\n-                )\\n-            \\n-            # Configure physics for better readability\\n-            net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\n-            net.set_options(\\\"\\\"\\\"\\n-            {\\n-              \\\"physics\\\": {\\n-                \\\"forceAtlas2Based\\\": {\\n-                  \\\"gravitationalConstant\\\": -100,\\n-                  \\\"centralGravity\\\": 0.01,\\n-                  \\\"springLength\\\": 200,\\n-                  \\\"springConstant\\\": 0.08\\n-                },\\n-                \\\"solver\\\": \\\"forceAtlas2Based\\\",\\n-                \\\"stabilization\\\": {\\n-                  \\\"iterations\\\": 100\\n-                }\\n-              }\\n-            }\\n-            \\\"\\\"\\\")\\n-            \\n-            # Save the network\\n-            net.save_graph(str(output_file))\\n-            \\n-        logger.info(f\\\"Interactive visualization saved to: {output_file}\\\")\\n-        return str(output_file)\\n-    \\n-    def generate_networkx_visualization(self, output_path: Optional[str] = None) -> str:\\n-        \\\"\\\"\\\"Generate a static visualization using NetworkX and matplotlib.\\n-        \\n-        Args:\\n-            output_path: Path to save the PNG file (optional)\\n-            \\n-        Returns:\\n-            Path to the generated PNG file\\n-        \\\"\\\"\\\"\\n-        if not NETWORKX_AVAILABLE:\\n-            logger.error(\\\"NetworkX is not available. Install with: pip install networkx matplotlib\\\")\\n-            return \\\"\\\"\\n-            \\n-        if not self.nodes:\\n-            logger.error(\\\"No graph data available. Call build_graph() first.\\\")\\n-            return \\\"\\\"\\n-            \\n-        # Create graph\\n-        G = nx.DiGraph()\\n-        \\n-        # Add nodes with attributes\\n-        for node in self.nodes:\\n-            G.add_node(\\n-                node[\\\"id\\\"],\\n-                label=node[\\\"label\\\"],\\n-                subsystem=node.get(\\\"subsystem\\\", \\\"default\\\")\\n-            )\\n-            \\n-        # Add edges\\n-        for edge in self.edges:\\n-            G.add_edge(edge[\\\"from\\\"], edge[\\\"to\\\"])\\n-            \\n-        # Generate output filename if not provided\\n-        if not output_path:\\n-            timestamp = datetime.now().strftime(\\\"%Y-%m-%d\\\")\\n-            output_path = self.visualization_path / f\\\"documentation_graph_{timestamp}.png\\\"\\n-            \\n-        # Set up figure\\n-        plt.figure(figsize=(20, 20))\\n-        \\n-        # Use spring layout with seed for reproducibility\\n-        pos = nx.spring_layout(G, seed=42)\\n-        \\n-        # Node colors based on subsystem\\n-        node_colors = [self.node_colors.get(node, self.color_palette[\\\"default\\\"]) for node in G.nodes()]\\n-        \\n-        # Node sizes\\n-        node_sizes = [self.node_sizes.get(node, 10) for node in G.nodes()]\\n-        \\n-        # Draw the graph\\n-        nx.draw_networkx(\\n-            G, \\n-            pos=pos,\\n-            with_labels=False,\\n-            node_color=node_colors,\\n-            node_size=node_sizes,\\n-            alpha=0.8,\\n-            arrows=True,\\n-            edge_color=theme.color.medium_grey\\n-        )\\n-        \\n-        # Draw labels for larger nodes only\\n-        large_nodes = {node: data for node, data in G.nodes(data=True) \\n-                      if self.node_sizes.get(node, 10) > 25}\\n-        if large_nodes:\\n-            nx.draw_networkx_labels(\\n-                G, \\n-                pos=pos,\\n-                labels={node: data[\\\"label\\\"] for node, data in large_nodes.items()},\\n-                font_size=8\\n-            )\\n-            \\n-        # Save the visualization\\n-        plt.tight_layout()\\n-        plt.axis(\\\"off\\\")\\n-        plt.savefig(str(output_path), dpi=300, bbox_inches=\\\"tight\\\")\\n-        plt.close()\\n-        \\n-        logger.info(f\\\"Static visualization saved to: {output_path}\\\")\\n-        return str(output_path)\\n-    \\n-    def export_graph_data(self, output_path: Optional[str] = None) -> str:\\n-        \\\"\\\"\\\"Export graph data to JSON for use in custom visualizations.\\n-        \\n-        Args:\\n-            output_path: Path to save the JSON file (optional)\\n-            \\n-        Returns:\\n-            Path to the generated JSON file\\n-        \\\"\\\"\\\"\\n-        if not self.nodes:\\n-            logger.error(\\\"No graph data available. Call build_graph() first.\\\")\\n-            return \\\"\\\"\\n-            \\n-        # Create graph data structure\\n-        graph_data = {\\n-            \\\"nodes\\\": self.nodes,\\n-            \\\"edges\\\": self.edges,\\n-            \\\"metadata\\\": {\\n-                \\\"generated\\\": datetime.now().isoformat(),\\n-                \\\"subsystems\\\": list(sorted(self.subsystems)),\\n-                \\\"node_count\\\": len(self.nodes),\\n-                \\\"edge_count\\\": len(self.edges)\\n-            }\\n-        }\\n-        \\n-        # Generate output filename if not provided\\n-        if not output_path:\\n-            timestamp = datetime.now().strftime(\\\"%Y-%m-%d\\\")\\n-            output_path = self.visualization_path / f\\\"documentation_graph_data_{timestamp}.json\\\"\\n-            \\n-        # Save the graph data\\n-        with open(output_path, 'w', encoding='utf-8') as f:\\n-            json.dump(graph_data, f, indent=2)\\n-            \\n-        logger.info(f\\\"Graph data exported to: {output_path}\\\")\\n-        return str(output_path)\\n-    \\n-    def analyze_graph(self) -> Dict[str, Any]:\\n-        \\\"\\\"\\\"Analyze the graph structure and generate metrics.\\n-        \\n-        Returns:\\n-            Dictionary of graph metrics and insights\\n-        \\\"\\\"\\\"\\n-        if not self.nodes or not self.edges:\\n-            logger.error(\\\"No graph data available. Call build_graph() first.\\\")\\n-            return {}\\n-            \\n-        # If NetworkX is available, use it for analysis\\n-        if NETWORKX_AVAILABLE:\\n-            return self._analyze_with_networkx()\\n-        else:\\n-            return self._analyze_basic()\\n-    \\n-    def _analyze_with_networkx(self) -> Dict[str, Any]:\\n-        \\\"\\\"\\\"Analyze graph using NetworkX metrics.\\n-        \\n-        Returns:\\n-            Dictionary of graph metrics\\n-        \\\"\\\"\\\"\\n-        # Create graph\\n-        G = nx.DiGraph()\\n-        \\n-        # Add nodes\\n-        for node in self.nodes:\\n-            G.add_node(node[\\\"id\\\"])\\n-            \\n-        # Add edges\\n-        for edge in self.edges:\\n-            G.add_edge(edge[\\\"from\\\"], edge[\\\"to\\\"])\\n-            \\n-        # Calculate metrics\\n-        metrics = {\\n-            \\\"node_count\\\": len(G),\\n-            \\\"edge_count\\\": len(G.edges()),\\n-            \\\"density\\\": nx.density(G),\\n-        }\\n-        \\n-        # Calculate centrality measures\\n-        try:\\n-            in_degree_centrality = nx.in_degree_centrality(G)\\n-            out_degree_centrality = nx.out_degree_centrality(G)\\n-            betweenness_centrality = nx.betweenness_centrality(G)\\n-            \\n-            # Sort by value to find highest centrality nodes\\n-            top_in_degree = sorted(in_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\n-            top_out_degree = sorted(out_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\n-            top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\n-            \\n-            metrics[\\\"most_referenced_docs\\\"] = [{\\\"doc\\\": doc, \\\"score\\\": round(score, 3)} for doc, score in top_in_degree]\\n-            metrics[\\\"most_referencing_docs\\\"] = [{\\\"doc\\\": doc, \\\"score\\\": round(score, 3)} for doc, score in top_out_degree]\\n-            metrics[\\\"bridge_docs\\\"] = [{\\\"doc\\\": doc, \\\"score\\\": round(score, 3)} for doc, score in top_betweenness]\\n-        except:\\n-            logger.warning(\\\"Error calculating centrality measures\\\")\\n-            \\n-        # Find isolated nodes (no connections)\\n-        isolated = list(nx.isolates(G))\\n-        metrics[\\\"isolated_docs\\\"] = isolated\\n-        metrics[\\\"isolated_count\\\"] = len(isolated)\\n-        \\n-        # Find strongly connected components\\n-        strongly_connected = list(nx.strongly_connected_components(G))\\n-        metrics[\\\"strongly_connected_components\\\"] = len(strongly_connected)\\n-        \\n-        # Find largest strongly connected component\\n-        if strongly_connected:\\n-            largest_component = max(strongly_connected, key=len)\\n-            metrics[\\\"largest_component_size\\\"] = len(largest_component)\\n-            metrics[\\\"largest_component_ratio\\\"] = len(largest_component) / len(G)\\n-        \\n-        return metrics\\n-    \\n-    def _analyze_basic(self) -> Dict[str, Any]:\\n-        \\\"\\\"\\\"Perform basic graph analysis without NetworkX.\\n-        \\n-        Returns:\\n-            Dictionary of basic graph metrics\\n-        \\\"\\\"\\\"\\n-        # Count nodes and edges\\n-        metrics = {\\n-            \\\"node_count\\\": len(self.nodes),\\n-            \\\"edge_count\\\": len(self.edges),\\n-        }\\n-        \\n-        # Count nodes by subsystem\\n-        subsystem_counts = {}\\n-        for node in self.nodes:\\n-            subsystem = node.get(\\\"subsystem\\\", \\\"default\\\")\\n-            subsystem_counts[subsystem] = subsystem_counts.get(subsystem, 0) + 1\\n-        metrics[\\\"subsystem_counts\\\"] = subsystem_counts\\n-        \\n-        # Find nodes with most connections\\n-        node_connections = {}\\n-        for edge in self.edges:\\n-            source = edge[\\\"from\\\"]\\n-            target = edge[\\\"to\\\"]\\n-            \\n-            # Count outgoing connections\\n-            node_connections.setdefault(source, {\\\"outgoing\\\": 0, \\\"incoming\\\": 0})\\n-            node_connections[source][\\\"outgoing\\\"] += 1\\n-            \\n-            # Count incoming connections\\n-            node_connections.setdefault(target, {\\\"outgoing\\\": 0, \\\"incoming\\\": 0})\\n-            node_connections[target][\\\"incoming\\\"] += 1\\n-            \\n-        # Find most referenced documents\\n-        most_referenced = sorted(\\n-            [(node, data[\\\"incoming\\\"]) for node, data in node_connections.items()],\\n-            key=lambda x: x[1],\\n-            reverse=True\\n-        )[:10]\\n-        metrics[\\\"most_referenced_docs\\\"] = [{\\\"doc\\\": doc, \\\"count\\\": count} for doc, count in most_referenced]\\n-        \\n-        # Find most referencing documents\\n-        most_referencing = sorted(\\n-            [(node, data[\\\"outgoing\\\"]) for node, data in node_connections.items()],\\n-            key=lambda x: x[1],\\n-            reverse=True\\n-        )[:10]\\n-        metrics[\\\"most_referencing_docs\\\"] = [{\\\"doc\\\": doc, \\\"count\\\": count} for doc, count in most_referencing]\\n-        \\n-        # Find isolated documents (no connections)\\n-        connected_nodes = set()\\n-        for edge in self.edges:\\n-            connected_nodes.add(edge[\\\"from\\\"])\\n-            connected_nodes.add(edge[\\\"to\\\"])\\n-            \\n-        all_node_ids = set(node[\\\"id\\\"] for node in self.nodes)\\n-        isolated = all_node_ids - connected_nodes\\n-        metrics[\\\"isolated_docs\\\"] = list(isolated)\\n-        metrics[\\\"isolated_count\\\"] = len(isolated)\\n-        \\n-        return metrics\\n+logger.info(f\\\"Graph built successfully: {len(self.nodes)} nodes, {len(self.edges)} edges.\\\")\\n+\\n+def generate_pyvis_visualization(self, output_path: Optional[str] = None) -> str:\\n+\\\"\\\"\\\"Generate an interactive HTML visualization using Pyvis.\\n+\\n+Args:\\n+output_path: Path to save the HTML file (optional)\\n+\\n+Returns:\\n+Path to the generated HTML file\\n+\\\"\\\"\\\"\\n+if not hasattr(self, 'nodes') or not self.nodes:\\n+logger.error(\\\"No graph data to visualize. Call build_graph() first.\\\")\\n+return \\\"\\\"\\n+\\n+# Create network\\n+timestamp = datetime.now().strftime(\\\"%Y-%m-%d\\\")\\n+output_filename = f\\\"documentation_graph_{timestamp}.html\\\"\\n+\\n+if output_path:\\n+output_file = Path(output_path)\\n+else:\\n+output_file = self.visualization_path / output_filename\\n+\\n+# Log the start of visualization generation\\n+logger.info(f\\\"Generating interactive visualization ({len(self.nodes)} nodes, {len(self.edges)} edges)...\\\")\\n+\\n+if console:\\n+with Progress(\\n+SpinnerColumn(),\\n+TextColumn(\\\"[progress.description]{task.description}\\\"),\\n+BarColumn(),\\n+TextColumn(\\\"[progress.percentage]{task.percentage:>3.0f}%\\\"),\\n+TimeRemainingColumn(),\\n+console=console,\\n+transient=False, # Keep progress visible after completion\\n+) as progress:\\n+# Create network\\n+net_task = progress.add_task(\\\"Creating network...\\\", total=1)\\n+net = Network(\\n+height=\\\"800px\\\",\\n+width=\\\"100%\\\",\\n+bgcolor=theme.color.white,\\n+font_color=theme.color.text_primary,\\n+heading=f\\\"EGOS Documentation Network ({timestamp})\\\"\\n+)\\n+progress.update(net_task, completed=1)\\n+\\n+# Add nodes with progress\\n+node_task = progress.add_task(\\\"Adding nodes...\\\", total=len(self.nodes))\\n+for node in self.nodes:\\n+net.add_node(\\n+node[\\\"id\\\"],\\n+label=node[\\\"label\\\"],\\n+title=node[\\\"title\\\"],\\n+color=node[\\\"color\\\"],\\n+size=node[\\\"size\\\"],\\n+group=node[\\\"group\\\"]\\n+)\\n+progress.update(node_task, advance=1)\\n+\\n+# Add edges with progress\\n+edge_task = progress.add_task(\\\"Adding edges...\\\", total=len(self.edges))\\n+for edge in self.edges:\\n+net.add_edge(\\n+edge[\\\"from\\\"],\\n+edge[\\\"to\\\"],\\n+arrows=edge.get(\\\"arrows\\\", \\\"to\\\"),\\n+title=edge.get(\\\"title\\\", \\\"\\\")\\n+)\\n+progress.update(edge_task, advance=1)\\n+\\n+# Configure physics for better readability\\n+save_task = progress.add_task(\\\"Configuring and saving...\\\", total=1)\\n+net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\n+net.set_options(\\\"\\\"\\\"\\n+{\\n+\\\"physics\\\": {\\n+\\\"forceAtlas2Based\\\": {\\n+\\\"gravitationalConstant\\\": -100,\\n+\\\"centralGravity\\\": 0.01,\\n+\\\"springLength\\\": 200,\\n+\\\"springConstant\\\": 0.08\\n+},\\n+\\\"solver\\\": \\\"forceAtlas2Based\\\",\\n+\\\"stabilization\\\": {\\n+\\\"iterations\\\": 100\\n+}\\n+}\\n+}\\n+\\\"\\\"\\\")\\n+\\n+# Save the network\\n+net.save_graph(str(output_file))\\n+progress.update(save_task, completed=1)\\n+else:\\n+# Fallback for when rich is not available\\n+logger.info(\\\"Generating visualization (basic logging)...\\\")\\n+net = Network(\\n+height=\\\"800px\\\",\\n+width=\\\"100%\\\",\\n+bgcolor=theme.color.white,\\n+font_color=theme.color.text_primary,\\n+heading=f\\\"EGOS Documentation Network ({timestamp})\\\"\\n+)\\n+\\n+# Add nodes\\n+for node in self.nodes:\\n+net.add_node(\\n+node[\\\"id\\\"],\\n+label=node[\\\"label\\\"],\\n+title=node[\\\"title\\\"],\\n+color=node[\\\"color\\\"],\\n+size=node[\\\"size\\\"],\\n+group=node[\\\"group\\\"]\\n+)\\n+\\n+# Add edges\\n+for edge in self.edges:\\n+net.add_edge(\\n+edge[\\\"from\\\"],\\n+edge[\\\"to\\\"],\\n+arrows=edge.get(\\\"arrows\\\", \\\"to\\\"),\\n+title=edge.get(\\\"title\\\", \\\"\\\")\\n+)\\n+\\n+# Configure physics for better readability\\n+net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250)\\n+net.set_options(\\\"\\\"\\\"\\n+{\\n+\\\"physics\\\": {\\n+\\\"forceAtlas2Based\\\": {\\n+\\\"gravitationalConstant\\\": -100,\\n+\\\"centralGravity\\\": 0.01,\\n+\\\"springLength\\\": 200,\\n+\\\"springConstant\\\": 0.08\\n+},\\n+\\\"solver\\\": \\\"forceAtlas2Based\\\",\\n+\\\"stabilization\\\": {\\n+\\\"iterations\\\": 100\\n+}\\n+}\\n+}\\n+\\\"\\\"\\\")\\n+\\n+# Save the network\\n+net.save_graph(str(output_file))\\n+\\n+logger.info(f\\\"Interactive visualization saved to: {output_file}\\\")\\n+return str(output_file)\\n+\\n+def generate_networkx_visualization(self, output_path: Optional[str] = None) -> str:\\n+\\\"\\\"\\\"Generate a static visualization using NetworkX and matplotlib.\\n+\\n+Args:\\n+output_path: Path to save the PNG file (optional)\\n+\\n+Returns:\\n+Path to the generated PNG file\\n+\\\"\\\"\\\"\\n+if not NETWORKX_AVAILABLE:\\n+logger.error(\\\"NetworkX is not available. Install with: pip install networkx matplotlib\\\")\\n+return \\\"\\\"\\n+\\n+if not self.nodes:\\n+logger.error(\\\"No graph data available. Call build_graph() first.\\\")\\n+return \\\"\\\"\\n+\\n+# Create graph\\n+G = nx.DiGraph()\\n+\\n+# Add nodes with attributes\\n+for node in self.nodes:\\n+G.add_node(\\n+node[\\\"id\\\"],\\n+label=node[\\\"label\\\"],\\n+subsystem=node.get(\\\"subsystem\\\", \\\"default\\\")\\n+)\\n+\\n+# Add edges\\n+for edge in self.edges:\\n+G.add_edge(edge[\\\"from\\\"], edge[\\\"to\\\"])\\n+\\n+# Generate output filename if not provided\\n+if not output_path:\\n+timestamp = datetime.now().strftime(\\\"%Y-%m-%d\\\")\\n+output_path = self.visualization_path / f\\\"documentation_graph_{timestamp}.png\\\"\\n+\\n+# Set up figure\\n+plt.figure(figsize=(20, 20))\\n+\\n+# Use spring layout with seed for reproducibility\\n+pos = nx.spring_layout(G, seed=42)\\n+\\n+# Node colors based on subsystem\\n+node_colors = [self.node_colors.get(node, self.color_palette[\\\"default\\\"]) for node in G.nodes()]\\n+\\n+# Node sizes\\n+node_sizes = [self.node_sizes.get(node, 10) for node in G.nodes()]\\n+\\n+# Draw the graph\\n+nx.draw_networkx(\\n+G,\\n+pos=pos,\\n+with_labels=False,\\n+node_color=node_colors,\\n+node_size=node_sizes,\\n+alpha=0.8,\\n+arrows=True,\\n+edge_color=theme.color.medium_grey\\n+)\\n+\\n+# Draw labels for larger nodes only\\n+large_nodes = {node: data for node, data in G.nodes(data=True)\\n+if self.node_sizes.get(node, 10) > 25}\\n+if large_nodes:\\n+nx.draw_networkx_labels(\\n+G,\\n+pos=pos,\\n+labels={node: data[\\\"label\\\"] for node, data in large_nodes.items()},\\n+font_size=8\\n+)\\n+\\n+# Save the visualization\\n+plt.tight_layout()\\n+plt.axis(\\\"off\\\")\\n+plt.savefig(str(output_path), dpi=300, bbox_inches=\\\"tight\\\")\\n+plt.close()\\n+\\n+logger.info(f\\\"Static visualization saved to: {output_path}\\\")\\n+return str(output_path)\\n+\\n+def export_graph_data(self, output_path: Optional[str] = None) -> str:\\n+\\\"\\\"\\\"Export graph data to JSON for use in custom visualizations.\\n+\\n+Args:\\n+output_path: Path to save the JSON file (optional)\\n+\\n+Returns:\\n+Path to the generated JSON file\\n+\\\"\\\"\\\"\\n+if not self.nodes:\\n+logger.error(\\\"No graph data available. Call build_graph() first.\\\")\\n+return \\\"\\\"\\n+\\n+# Create graph data structure\\n+graph_data = {\\n+\\\"nodes\\\": self.nodes,\\n+\\\"edges\\\": self.edges,\\n+\\\"metadata\\\": {\\n+\\\"generated\\\": datetime.now().isoformat(),\\n+\\\"subsystems\\\": list(sorted(self.subsystems)),\\n+\\\"node_count\\\": len(self.nodes),\\n+\\\"edge_count\\\": len(self.edges)\\n+}\\n+}\\n+\\n+# Generate output filename if not provided\\n+if not output_path:\\n+timestamp = datetime.now().strftime(\\\"%Y-%m-%d\\\")\\n+output_path = self.visualization_path / f\\\"documentation_graph_data_{timestamp}.json\\\"\\n+\\n+# Save the graph data\\n+with open(output_path, 'w', encoding='utf-8') as f:\\n+json.dump(graph_data, f, indent=2)\\n+\\n+logger.info(f\\\"Graph data exported to: {output_path}\\\")\\n+return str(output_path)\\n+\\n+def analyze_graph(self) -> Dict[str, Any]:\\n+\\\"\\\"\\\"Analyze the graph structure and generate metrics.\\n+\\n+Returns:\\n+Dictionary of graph metrics and insights\\n+\\\"\\\"\\\"\\n+if not self.nodes or not self.edges:\\n+logger.error(\\\"No graph data available. Call build_graph() first.\\\")\\n+return {}\\n+\\n+# If NetworkX is available, use it for analysis\\n+if NETWORKX_AVAILABLE:\\n+return self._analyze_with_networkx()\\n+else:\\n+return self._analyze_basic()\\n+\\n+def _analyze_with_networkx(self) -> Dict[str, Any]:\\n+\\\"\\\"\\\"Analyze graph using NetworkX metrics.\\n+\\n+Returns:\\n+Dictionary of graph metrics\\n+\\\"\\\"\\\"\\n+# Create graph\\n+G = nx.DiGraph()\\n+\\n+# Add nodes\\n+for node in self.nodes:\\n+G.add_node(node[\\\"id\\\"])\\n+\\n+# Add edges\\n+for edge in self.edges:\\n+G.add_edge(edge[\\\"from\\\"], edge[\\\"to\\\"])\\n+\\n+# Calculate metrics\\n+metrics = {\\n+\\\"node_count\\\": len(G),\\n+\\\"edge_count\\\": len(G.edges()),\\n+\\\"density\\\": nx.density(G),\\n+}\\n+\\n+# Calculate centrality measures\\n+try:\\n+in_degree_centrality = nx.in_degree_centrality(G)\\n+out_degree_centrality = nx.out_degree_centrality(G)\\n+betweenness_centrality = nx.betweenness_centrality(G)\\n+\\n+# Sort by value to find highest centrality nodes\\n+top_in_degree = sorted(in_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\n+top_out_degree = sorted(out_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\n+top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\\n+\\n+metrics[\\\"most_referenced_docs\\\"] = [{\\\"doc\\\": doc, \\\"score\\\": round(score, 3)} for doc, score in top_in_degree]\\n+metrics[\\\"most_referencing_docs\\\"] = [{\\\"doc\\\": doc, \\\"score\\\": round(score, 3)} for doc, score in top_out_degree]\\n+metrics[\\\"bridge_docs\\\"] = [{\\\"doc\\\": doc, \\\"score\\\": round(score, 3)} for doc, score in top_betweenness]\\n+except:\\n+logger.warning(\\\"Error calculating centrality measures\\\")\\n+\\n+# Find isolated nodes (no connections)\\n+isolated = list(nx.isolates(G))\\n+metrics[\\\"isolated_docs\\\"] = isolated\\n+metrics[\\\"isolated_count\\\"] = len(isolated)\\n+\\n+# Find strongly connected components\\n+strongly_connected = list(nx.strongly_connected_components(G))\\n+metrics[\\\"strongly_connected_components\\\"] = len(strongly_connected)\\n+\\n+# Find largest strongly connected component\\n+if strongly_connected:\\n+largest_component = max(strongly_connected, key=len)\\n+metrics[\\\"largest_component_size\\\"] = len(largest_component)\\n+metrics[\\\"largest_component_ratio\\\"] = len(largest_component) / len(G)\\n+\\n+return metrics\\n+\\n+def _analyze_basic(self) -> Dict[str, Any]:\\n+\\\"\\\"\\\"Perform basic graph analysis without NetworkX.\\n+\\n+Returns:\\n+Dictionary of basic graph metrics\\n+\\\"\\\"\\\"\\n+# Count nodes and edges\\n+metrics = {\\n+\\\"node_count\\\": len(self.nodes),\\n+\\\"edge_count\\\": len(self.edges),\\n+}\\n+\\n+# Count nodes by subsystem\\n+subsystem_counts = {}\\n+for node in self.nodes:\\n+subsystem = node.get(\\\"subsystem\\\", \\\"default\\\")\\n+subsystem_counts[subsystem] = subsystem_counts.get(subsystem, 0) + 1\\n+metrics[\\\"subsystem_counts\\\"] = subsystem_counts\\n+\\n+# Find nodes with most connections\\n+node_connections = {}\\n+for edge in self.edges:\\n+source = edge[\\\"from\\\"]\\n+target = edge[\\\"to\\\"]\\n+\\n+# Count outgoing connections\\n+node_connections.setdefault(source, {\\\"outgoing\\\": 0, \\\"incoming\\\": 0})\\n+node_connections[source][\\\"outgoing\\\"] += 1\\n+\\n+# Count incoming connections\\n+node_connections.setdefault(target, {\\\"outgoing\\\": 0, \\\"incoming\\\": 0})\\n+node_connections[target][\\\"incoming\\\"] += 1\\n+\\n+# Find most referenced documents\\n+most_referenced = sorted(\\n+[(node, data[\\\"incoming\\\"]) for node, data in node_connections.items()],\\n+key=lambda x: x[1],\\n+reverse=True\\n+)[:10]\\n+metrics[\\\"most_referenced_docs\\\"] = [{\\\"doc\\\": doc, \\\"count\\\": count} for doc, count in most_referenced]\\n+\\n+# Find most referencing documents\\n+most_referencing = sorted(\\n+[(node, data[\\\"outgoing\\\"]) for node, data in node_connections.items()],\\n+key=lambda x: x[1],\\n+reverse=True\\n+)[:10]\\n+metrics[\\\"most_referencing_docs\\\"] = [{\\\"doc\\\": doc, \\\"count\\\": count} for doc, count in most_referencing]\\n+\\n+# Find isolated documents (no connections)\\n+connected_nodes = set()\\n+for edge in self.edges:\\n+connected_nodes.add(edge[\\\"from\\\"])\\n+connected_nodes.add(edge[\\\"to\\\"])\\n+\\n+all_node_ids = set(node[\\\"id\\\"] for node in self.nodes)\\n+isolated = all_node_ids - connected_nodes\\n+metrics[\\\"isolated_docs\\\"] = list(isolated)\\n+metrics[\\\"isolated_count\\\"] = len(isolated)\\n+\\n+return metrics\\n \\n \\n def main():\\n-    \\\"\\\"\\\"Main entry point for the documentation graph generator.\\\"\\\"\\\"\\n-    parser = argparse.ArgumentParser(\\n-        description=\\\"Generate interactive visualization of EGOS documentation connections\\\"\\n-    )\\n-    parser.add_argument(\\n-        \\\"--base-path\\\", \\\"-b\\\", default=\\\".\\\",\\n-        help=\\\"Base path of the EGOS project\\\"\\n-    )\\n-    parser.add_argument(\\n-        \\\"--report\\\", \\\"-r\\\",\\n-        help=\\\"Path to the cross-reference analysis JSON report\\\"\\n-    )\\n-    parser.add_argument(\\n-        \\\"--output\\\", \\\"-o\\\",\\n-        help=\\\"Path to save the visualization\\\"\\n-    )\\n-    parser.add_argument(\\n-        \\\"--format\\\", \\\"-f\\\", choices=[\\\"html\\\", \\\"png\\\", \\\"json\\\", \\\"all\\\"], default=\\\"all\\\",\\n-        help=\\\"Output format (default: all available formats)\\\"\\n-    )\\n-    parser.add_argument(\\n-        \\\"--analyze\\\", \\\"-a\\\", action=\\\"store_true\\\",\\n-        help=\\\"Analyze graph and print metrics\\\"\\n-    )\\n-    \\n-    args = parser.parse_args()\\n-    \\n-    try:\\n-        # Create graph generator\\n-        generator = DocumentationGraphGenerator(args.base_path)\\n-        \\n-        # Load report data\\n-        if not generator.load_report_data(args.report):\\n-            return 1\\n-            \\n-        # --- Start of new try block for core logic ---\\n-        try:\\n-            # Build graph\\n-            generator.build_graph()\\n-            \\n-            # Generate visualizations based on format\\n-            if args.format in [\\\"html\\\", \\\"all\\\"] and PYVIS_AVAILABLE:\\n-                generator.generate_pyvis_visualization(\\n-                    args.output if args.format == \\\"html\\\" else None\\n-                )\\n-                \\n-            if args.format in [\\\"png\\\", \\\"all\\\"] and NETWORKX_AVAILABLE:\\n-                generator.generate_networkx_visualization(\\n-                    args.output if args.format == \\\"png\\\" else None\\n-                )\\n-                \\n-            if args.format in [\\\"json\\\", \\\"all\\\"]:\\n-                generator.export_graph_data(\\n-                    args.output if args.format == \\\"json\\\" else None\\n-                )\\n-                \\n-            # Analyze graph if requested\\n-            if args.analyze:\\n-                analysis = generator.analyze_graph()\\n-                \\n-                # Print analysis results using rich tables if available\\n-                console.print(\\\"\\\\n [bold cyan]DOCUMENTATION GRAPH ANALYSIS[/bold cyan]\\\")\\n-                \\n-                summary_table = Table(title=\\\"Graph Summary\\\", show_header=False, box=None)\\n-                summary_table.add_row(\\\"Nodes:\\\", str(analysis.get('node_count', 'N/A')))\\n-                summary_table.add_row(\\\"Edges:\\\", str(analysis.get('edge_count', 'N/A')))\\n-                if \\\"density\\\" in analysis:\\n-                    summary_table.add_row(\\\"Graph Density:\\\", f\\\"{analysis['density']:.4f}\\\")\\n-                if \\\"isolated_count\\\" in analysis:\\n-                    summary_table.add_row(\\\"Isolated Documents:\\\", f\\\"[yellow]{analysis['isolated_count']}[/yellow]\\\")\\n-                if \\\"strongly_connected_components\\\" in analysis:\\n-                    summary_table.add_row(\\\"Strongly Connected Components:\\\", str(analysis['strongly_connected_components']))\\n-                if \\\"largest_component_size\\\" in analysis:\\n-                    ratio = analysis.get('largest_component_ratio', 0)\\n-                    summary_table.add_row(\\\"Largest Component Size:\\\", f\\\"{analysis['largest_component_size']} ({ratio:.1%})\\\")\\n-                console.print(summary_table)\\n-\\n-                # --- Most Referenced Documents Table ---\\n-                if \\\"most_referenced_docs\\\" in analysis and analysis[\\\"most_referenced_docs\\\"]:\\n-                    ref_table = Table(title=\\\" Most Referenced Documents (Top 5)\\\", expand=True)\\n-                    ref_table.add_column(\\\"Rank\\\", style=\\\"dim\\\", width=5)\\n-                    ref_table.add_column(\\\"Document\\\", style=\\\"magenta\\\")\\n-                    ref_table.add_column(\\\"Score/Count\\\", justify=\\\"right\\\", style=\\\"green\\\")\\n-                    \\n-                    for i, doc_info in enumerate(analysis[\\\"most_referenced_docs\\\"][:5], 1):\\n-                        score = doc_info.get('score', doc_info.get('count', 'N/A'))\\n-                        score_str = f\\\"{score:.3f}\\\" if isinstance(score, float) else str(score)\\n-                        ref_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\n-                    console.print(ref_table)\\n-\\n-                # --- Bridge Documents Table ---\\n-                if \\\"bridge_docs\\\" in analysis and analysis[\\\"bridge_docs\\\"]:\\n-                    bridge_table = Table(title=\\\" Bridge Documents (High Betweenness - Top 5)\\\", expand=True)\\n-                    bridge_table.add_column(\\\"Rank\\\", style=\\\"dim\\\", width=5)\\n-                    bridge_table.add_column(\\\"Document\\\", style=\\\"cyan\\\")\\n-                    bridge_table.add_column(\\\"Score\\\", justify=\\\"right\\\", style=\\\"blue\\\")\\n-                    \\n-                    for i, doc_info in enumerate(analysis[\\\"bridge_docs\\\"][:5], 1):\\n-                        score = doc_info.get('score', 'N/A')\\n-                        score_str = f\\\"{score:.3f}\\\" if isinstance(score, float) else str(score)\\n-                        bridge_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\n-                    console.print(bridge_table)\\n-\\n-                # --- Isolated Documents --- \\n-                if \\\"isolated_docs\\\" in analysis and analysis[\\\"isolated_docs\\\"]:\\n-                     console.print(f\\\"\\\\n [yellow]Isolated Documents ({analysis['isolated_count']}):[/yellow]\\\")\\n-                     # Print only first few for brevity\\n-                     for doc in analysis['isolated_docs'][:10]:\\n-                         console.print(f\\\"  - {os.path.basename(doc)}\\\")\\n-                     if len(analysis['isolated_docs']) > 10:\\n-                         console.print(f\\\"  ... and {len(analysis['isolated_docs']) - 10} more\\\")\\n-                \\n-                # Save analysis to JSON\\n-                analysis_path = generator.visualization_path / \\\"documentation_graph_analysis.json\\\"\\n-                with open(analysis_path, 'w', encoding='utf-8') as f:\\n-                    json.dump(analysis, f, indent=2)\\n-                logger.info(f\\\"Analysis saved to: {analysis_path}\\\")\\n-\\n-        # --- End of new try block, add except block ---\\n-        except Exception as core_error:\\n-            logger.error(f\\\"!!! Critical Error during graph processing/visualization: {core_error}\\\")\\n-            logger.error(traceback.format_exc())\\n-            return 1 # Ensure exit with error code\\n-                \\n-        return 0\\n-            \\n-    # Outer except block remains for initialization/loading errors\\n-    except Exception as e:\\n-        logger.error(f\\\"Initialization or Loading Error: {str(e)}\\\")\\n-        # Ensure traceback is imported if needed here too\\n-        if 'traceback' not in sys.modules:\\n-            import traceback\\n-        logger.error(traceback.format_exc())\\n-        return 1\\n+\\\"\\\"\\\"Main entry point for the documentation graph generator.\\\"\\\"\\\"\\n+parser = argparse.ArgumentParser(\\n+description=\\\"Generate interactive visualization of EGOS documentation connections\\\"\\n+)\\n+parser.add_argument(\\n+\\\"--base-path\\\", \\\"-b\\\", default=\\\".\\\",\\n+help=\\\"Base path of the EGOS project\\\"\\n+)\\n+parser.add_argument(\\n+\\\"--report\\\", \\\"-r\\\",\\n+help=\\\"Path to the cross-reference analysis JSON report\\\"\\n+)\\n+parser.add_argument(\\n+\\\"--output\\\", \\\"-o\\\",\\n+help=\\\"Path to save the visualization\\\"\\n+)\\n+parser.add_argument(\\n+\\\"--format\\\", \\\"-f\\\", choices=[\\\"html\\\", \\\"png\\\", \\\"json\\\", \\\"all\\\"], default=\\\"all\\\",\\n+help=\\\"Output format (default: all available formats)\\\"\\n+)\\n+parser.add_argument(\\n+\\\"--analyze\\\", \\\"-a\\\", action=\\\"store_true\\\",\\n+help=\\\"Analyze graph and print metrics\\\"\\n+)\\n+\\n+args = parser.parse_args()\\n+\\n+try:\\n+# Create graph generator\\n+generator = DocumentationGraphGenerator(args.base_path)\\n+\\n+# Load report data\\n+if not generator.load_report_data(args.report):\\n+return 1\\n+\\n+# --- Start of new try block for core logic ---\\n+try:\\n+# Build graph\\n+generator.build_graph()\\n+\\n+# Generate visualizations based on format\\n+if args.format in [\\\"html\\\", \\\"all\\\"] and PYVIS_AVAILABLE:\\n+generator.generate_pyvis_visualization(\\n+args.output if args.format == \\\"html\\\" else None\\n+)\\n+\\n+if args.format in [\\\"png\\\", \\\"all\\\"] and NETWORKX_AVAILABLE:\\n+generator.generate_networkx_visualization(\\n+args.output if args.format == \\\"png\\\" else None\\n+)\\n+\\n+if args.format in [\\\"json\\\", \\\"all\\\"]:\\n+generator.export_graph_data(\\n+args.output if args.format == \\\"json\\\" else None\\n+)\\n+\\n+# Analyze graph if requested\\n+if args.analyze:\\n+analysis = generator.analyze_graph()\\n+\\n+# Print analysis results using rich tables if available\\n+console.print(\\\"\\\\n [bold cyan]DOCUMENTATION GRAPH ANALYSIS[/bold cyan]\\\")\\n+\\n+summary_table = Table(title=\\\"Graph Summary\\\", show_header=False, box=None)\\n+summary_table.add_row(\\\"Nodes:\\\", str(analysis.get('node_count', 'N/A')))\\n+summary_table.add_row(\\\"Edges:\\\", str(analysis.get('edge_count', 'N/A')))\\n+if \\\"density\\\" in analysis:\\n+summary_table.add_row(\\\"Graph Density:\\\", f\\\"{analysis['density']:.4f}\\\")\\n+if \\\"isolated_count\\\" in analysis:\\n+summary_table.add_row(\\\"Isolated Documents:\\\", f\\\"[yellow]{analysis['isolated_count']}[/yellow]\\\")\\n+if \\\"strongly_connected_components\\\" in analysis:\\n+summary_table.add_row(\\\"Strongly Connected Components:\\\", str(analysis['strongly_connected_components']))\\n+if \\\"largest_component_size\\\" in analysis:\\n+ratio = analysis.get('largest_component_ratio', 0)\\n+summary_table.add_row(\\\"Largest Component Size:\\\", f\\\"{analysis['largest_component_size']} ({ratio:.1%})\\\")\\n+console.print(summary_table)\\n+\\n+# --- Most Referenced Documents Table ---\\n+if \\\"most_referenced_docs\\\" in analysis and analysis[\\\"most_referenced_docs\\\"]:\\n+ref_table = Table(title=\\\" Most Referenced Documents (Top 5)\\\", expand=True)\\n+ref_table.add_column(\\\"Rank\\\", style=\\\"dim\\\", width=5)\\n+ref_table.add_column(\\\"Document\\\", style=\\\"magenta\\\")\\n+ref_table.add_column(\\\"Score/Count\\\", justify=\\\"right\\\", style=\\\"green\\\")\\n+\\n+for i, doc_info in enumerate(analysis[\\\"most_referenced_docs\\\"][:5], 1):\\n+score = doc_info.get('score', doc_info.get('count', 'N/A'))\\n+score_str = f\\\"{score:.3f}\\\" if isinstance(score, float) else str(score)\\n+ref_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\n+console.print(ref_table)\\n+\\n+# --- Bridge Documents Table ---\\n+if \\\"bridge_docs\\\" in analysis and analysis[\\\"bridge_docs\\\"]:\\n+bridge_table = Table(title=\\\" Bridge Documents (High Betweenness - Top 5)\\\", expand=True)\\n+bridge_table.add_column(\\\"Rank\\\", style=\\\"dim\\\", width=5)\\n+bridge_table.add_column(\\\"Document\\\", style=\\\"cyan\\\")\\n+bridge_table.add_column(\\\"Score\\\", justify=\\\"right\\\", style=\\\"blue\\\")\\n+\\n+for i, doc_info in enumerate(analysis[\\\"bridge_docs\\\"][:5], 1):\\n+score = doc_info.get('score', 'N/A')\\n+score_str = f\\\"{score:.3f}\\\" if isinstance(score, float) else str(score)\\n+bridge_table.add_row(str(i), os.path.basename(doc_info['doc']), score_str)\\n+console.print(bridge_table)\\n+\\n+# --- Isolated Documents ---\\n+if \\\"isolated_docs\\\" in analysis and analysis[\\\"isolated_docs\\\"]:\\n+console.print(f\\\"\\\\n [yellow]Isolated Documents ({analysis['isolated_count']}):[/yellow]\\\")\\n+# Print only first few for brevity\\n+for doc in analysis['isolated_docs'][:10]:\\n+console.print(f\\\"  - {os.path.basename(doc)}\\\")\\n+if len(analysis['isolated_docs']) > 10:\\n+console.print(f\\\"  ... and {len(analysis['isolated_docs']) - 10} more\\\")\\n+\\n+# Save analysis to JSON\\n+analysis_path = generator.visualization_path / \\\"documentation_graph_analysis.json\\\"\\n+with open(analysis_path, 'w', encoding='utf-8') as f:\\n+json.dump(analysis, f, indent=2)\\n+logger.info(f\\\"Analysis saved to: {analysis_path}\\\")\\n+\\n+# --- End of new try block, add except block ---\\n+except Exception as core_error:\\n+logger.error(f\\\"!!! Critical Error during graph processing/visualization: {core_error}\\\")\\n+logger.error(traceback.format_exc())\\n+return 1 # Ensure exit with error code\\n+\\n+return 0\\n+\\n+# Outer except block remains for initialization/loading errors\\n+except Exception as e:\\n+logger.error(f\\\"Initialization or Loading Error: {str(e)}\\\")\\n+# Ensure traceback is imported if needed here too\\n+if 'traceback' not in sys.modules:\\n+import traceback\\n+logger.error(traceback.format_exc())\\n+return 1\\n \\n \\n if __name__ == \\\"__main__\\\":\\n-    sys.exit(main())+sys.exit(main())\\n+\\\"\\\"\\\"\""
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_archive\\historical_changelogs_from_reference\\PLANO DE NEGOCIO EVA (1).txt",
    "LineNumber": 723,
    "LineContent": "- Backend ReDoc: http://localhost:8000/redoc"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181428\\docs\\governance\\.-summary.md",
    "LineNumber": 599,
    "LineContent": "- **TODO/FIXME:** Line 3924 - \"diff\": \"--- a/strat_core.py\\n+++ b/strat_core.py\\n@@ -1,23 +1,24 @@\\n \\\"\\\"\\\"TODO: Module docstring for strat_core.py\\\"\\\"\\\"\\n \\n-<!-- \\n+\\\"\\\"\\\"\\n @references:\\n - Core References:\\n-  - [MQP.md](../../..\\..\\MQP.md) - Master Quantum Prompt defining EGOS principles\\n-  - [ROADMAP.md](../../..\\..\\ROADMAP.md) - Project roadmap and planning\\n+- [MQP.md](../../..\\..\\MQP.md) - Master Quantum Prompt defining EGOS principles\\n+- [ROADMAP.md](../../..\\..\\ROADMAP.md) - Project roadmap and planning\\n - Subsystems:\\n-  - [STRAT/README.md](../../..\\..\\subsystems\\STRAT\\README.md)\\n+- [STRAT/README.md](../../..\\..\\subsystems\\STRAT\\README.md)\\n \\n \\n \\n def analyze_strategy(strategy_doc: str) -> dict:\\n-    \\\"\\\"\\\"Placeholder for parsing and analyzing strategic documents.\\\"\\\"\\\"\\n-    print(f\\\"STRAT: Analyzing strategy document ({len(strategy_doc)} chars)...\\\")\\n-    # TODO: Implement actual parsing and analysis logic\\n-    return {\\\"objectives\\\": [], \\\"risks\\\": [], \\\"opportunities\\\": []}\\n+\\\"\\\"\\\"Placeholder for parsing and analyzing strategic documents.\\\"\\\"\\\"\\n+print(f\\\"STRAT: Analyzing strategy document ({len(strategy_doc)} chars)...\\\")\\n+# TODO: Implement actual parsing and analysis logic\\n+return {\\\"objectives\\\": [], \\\"risks\\\": [], \\\"opportunities\\\": []}\\n \\n def generate_directives(analysis_result: dict) -> list:\\n-    \\\"\\\"\\\"Placeholder for generating actionable directives for other subsystems.\\\"\\\"\\\"\\n-    print(\\\"STRAT: Generating directives...\\\")\\n-    # TODO: Implement logic to translate analysis into tasks\\n-    return []+\\\"\\\"\\\"Placeholder for generating actionable directives for other subsystems.\\\"\\\"\\\"\\n+print(\\\"STRAT: Generating directives...\\\")\\n+# TODO: Implement logic to translate analysis into tasks\\n+return []\\n+\\\"\\\"\\\"\""
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181433\\docs\\governance\\cross_reference_management_process.md",
    "LineNumber": 56,
    "LineContent": "- Doc: [cross_reference_best_practices.md](cross_reference_best_practices.md) (KOIOS Cross-Reference Best Practices)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181438\\docs\\governance\\roadmap_standardization.md",
    "LineNumber": 196,
    "LineContent": "`linked_doc: [CAMINHO-DO-DOCUMENTO]`"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181532\\docs\\governance\\migrations\\processed\\mixed-pt-en\\PLANO DE NEGOCIO EVA (1).md",
    "LineNumber": 769,
    "LineContent": "- Backend ReDoc: http://localhost:8000/redoc"
  },
  {
    "File": "C:\\EGOS\\scripts\\cross_reference\\cross_reference_ultra_report_20250520_233235.md",
    "LineNumber": 325934,
    "LineContent": "  - Found in: `docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181433\\docs\\governance\\cross_reference_management_process.md` L58: `- Doc: [ROADMAP.md](../../ROADMAP.md) (Project Roadmap and Planning)`"
  },
  {
    "File": "C:\\EGOS\\scripts\\cross_reference\\execute_inventory_scan.py",
    "LineNumber": 43,
    "LineContent": "    {\"query\": \"Doc:\", \"output_file\": \"grep_Doc_results.json\", \"case_insensitive\": True, \"category\": \"keyword\"},"
  },
  {
    "File": "C:\\EGOS\\scripts\\cross_reference\\purge_old_references.py",
    "LineNumber": 193,
    "LineContent": "    {\"name\": \"Doc\", \"pattern\": r\"Doc:\\s\", \"context_required\": False},"
  }
]