#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Migration Executor Module for Directory Unification Tool

This module executes the migration plan generated by the consolidation planner,
handling file operations, reference updates, and creating backup copies to ensure data safety.
It integrates context analysis to make informed decisions about file migrations and
provides user decision points for critical operations.

Author: Cascade
Date: 2025-05-23
Version: 1.1.0
References:
    - C:\EGOS\docs\tools\directory_unification_tool_prd.md
    - C:\EGOS\scripts\maintenance\file_backup_manager.py
    - C:\EGOS\scripts\cross_reference\optimized_reference_fixer.py
    - C:\EGOS\scripts\cross_reference\file_reference_checker_ultra.py
    - C:\EGOS\scripts\maintenance\directory_unification\context_analyzer.py
"""
# 
# @references:
#   - .windsurfrules
#   - CODE_OF_CONDUCT.md
#   - MQP.md
#   - README.md
#   - ROADMAP.md
#   - CROSSREF_STANDARD.md

# Standard library imports
import os
import re
import json
import shutil
import logging
import tempfile
import datetime
from pathlib import Path
from typing import List, Dict, Any, Set, Tuple, Optional
import threading
import subprocess
from concurrent.futures import ThreadPoolExecutor

# Third-party imports
try:
    from colorama import Fore, Style, init
    init()  # Initialize colorama
except ImportError:
    # Define dummy colorama classes if not available
    class DummyColorama:
        def __getattr__(self, name):
            return ""
    Fore = Style = DummyColorama()

# Local imports
from .utils import setup_logger, print_banner, format_path, Timer, human_readable_size

# Constants
CONFIG = {
    "BACKUP_DIRECTORY": "backup",
    "MAX_WORKERS": 4,
    "REFERENCE_PATTERNS": [
        r'C:\\EGOS\\([^"\'\\]+)',
        r'"([^"]+)"',
        r"'([^']+)'",
        r'@references\s+([^\n]+)',
        r'- (C:\\EGOS\\[^\n]+)'
    ],
    "FILE_EXTENSIONS_TO_UPDATE": [".py", ".md", ".txt", ".json", ".yml", ".yaml", ".html", ".js", ".css", ".jsx", ".ts", ".tsx"],
    "LOG_FORMAT": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
}

# Set up logger
logger = setup_logger("migration_executor", CONFIG["LOG_FORMAT"])


class MigrationExecutor:
    """
    Class for executing the migration plan, including file operations
    and reference updates.
    """
    
    def __init__(self, args: Dict[str, Any], context: Dict[str, Any], logger: logging.Logger):
        """
        Initialize the MigrationExecutor class.
        
        Args:
            args: Command line arguments or configuration
            context: Context data from previous modules
        """
        self.logger = logger
        self.args = args
        self.context = context
        self.egos_root = Path(args.get("egos_root", os.environ.get("EGOS_ROOT", os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "..")))))
        
        # Get the keyword from arguments
        self.keyword = args.get("keyword", "")
        
        # Get the dry run flag
        self.dry_run = args.get("dry_run", False)
        
        # Get the backup flag
        self.create_backup = args.get("create_backup", True)
        
        # Ensure consolidation plan is available
        if "plan" not in context or not context["plan"]:
            raise ValueError("Consolidation plan is required for migration execution")
        
        # Extract plan data from context
        self.target_location = context["plan"].get("target_location")
        self.migration_steps = context["plan"].get("migration_steps", [])
        self.file_classifications = context["plan"].get("file_classifications", {})
        
        # Initialize migration results
        self.results = {
            "successful_operations": [],
            "failed_operations": [],
            "reference_updates": [],
            "stats": {
                "total_steps": len(self.migration_steps),
                "completed_steps": 0,
                "successful_steps": 0,
                "failed_steps": 0,
                "backup_created": False,
                "backup_location": None
            }
        }
        
        # Thread lock for updating shared resources
        self.lock = threading.Lock()
    
    def execute_migration(self) -> Dict[str, Any]:
        """
        Execute the migration plan.
        
        Returns:
            Dictionary with migration results
        """
        logger.info(f"Executing migration plan for {self.keyword}")
        
        # Initialize timer
        timer = Timer("Migration execution")
        timer.start()
        
        # Create backup if needed
        if self.create_backup and not self.dry_run:
            self._create_backup()
        
        # Prepare target directories
        if not self.dry_run:
            self._prepare_target_directories()
        
        # Execute migration steps
        self._execute_steps()
        
        # Update references
        if not self.dry_run:
            self._update_references()
        
        # Stop timer
        elapsed_time = timer.stop()
        logger.info(f"Migration execution completed in {elapsed_time:.2f} seconds")
        
        # Return results
        return self.results
    
    def _create_backup(self) -> None:
        """Create a backup of all files involved in the migration."""
        logger.info("Creating backup of files before migration")
        
        # Create backup directory
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = os.path.join(self.egos_root, CONFIG["BACKUP_DIRECTORY"], f"directory_unification_{self.keyword}_{timestamp}")
        
        try:
            # Create the backup directory structure
            os.makedirs(backup_dir, exist_ok=True)
            
            # Collect paths of all files to backup
            files_to_backup = []
            for step in self.migration_steps:
                source_path = os.path.join(self.egos_root, step["source_path"])
                if os.path.exists(source_path):
                    files_to_backup.append(step["source_path"])
            
            # Use PowerShell for backup if on Windows (handles encoding better)
            if os.name == 'nt':
                # Create a temporary file with the list of files to backup
                with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as temp_file:
                    for file_path in files_to_backup:
                        temp_file.write(f"{file_path}\n")
                    temp_file_path = temp_file.name
                
                try:
                    # Use PowerShell to copy files with their relative paths preserved
                    ps_script = f"""
                    $files = Get-Content "{temp_file_path}"
                    foreach ($file in $files) {{
                        $source = Join-Path "{self.egos_root}" $file
                        $target = Join-Path "{backup_dir}" $file
                        $targetDir = Split-Path -Parent $target
                        if (!(Test-Path $targetDir)) {{
                            New-Item -ItemType Directory -Path $targetDir -Force | Out-Null
                        }}
                        Copy-Item -Path $source -Destination $target -Force
                    }}
                    """
                    
                    subprocess.run(["powershell", "-Command", ps_script], check=True)
                    
                    # Mark backup as created
                    self.results["stats"]["backup_created"] = True
                    self.results["stats"]["backup_location"] = backup_dir
                    
                    logger.info(f"Backup created at {backup_dir}")
                    
                finally:
                    # Clean up temporary file
                    try:
                        os.unlink(temp_file_path)
                    except:
                        pass
            else:
                # Fallback to Python-based backup for non-Windows systems
                for file_path in files_to_backup:
                    source_path = os.path.join(self.egos_root, file_path)
                    target_path = os.path.join(backup_dir, file_path)
                    
                    # Create the target directory if it doesn't exist
                    os.makedirs(os.path.dirname(target_path), exist_ok=True)
                    
                    # Copy the file
                    shutil.copy2(source_path, target_path)
                
                # Mark backup as created
                self.results["stats"]["backup_created"] = True
                self.results["stats"]["backup_location"] = backup_dir
                
                logger.info(f"Backup created at {backup_dir}")
                
        except Exception as e:
            logger.error(f"Failed to create backup: {e}")
            if not self.args.get("force", False):
                raise ValueError(f"Backup creation failed. Use --force to continue without backup.")
    
    def _prepare_target_directories(self) -> None:
        """Prepare target directories for migration."""
        logger.info("Preparing target directories")
        
        # Collect all target directories
        target_dirs = set()
        for step in self.migration_steps:
            if step["action"] in ["move", "archive"] and step["target_path"]:
                target_dir = os.path.dirname(os.path.join(self.egos_root, step["target_path"]))
                target_dirs.add(target_dir)
        
        # Create all target directories
        for dir_path in target_dirs:
            try:
                os.makedirs(dir_path, exist_ok=True)
                logger.debug(f"Created directory: {dir_path}")
            except Exception as e:
                logger.error(f"Failed to create directory {dir_path}: {e}")
    
    def _execute_steps(self) -> None:
        """Execute migration steps with thread pooling for efficiency."""
        logger.info(f"Executing {len(self.migration_steps)} migration steps")
        
        # Use a thread pool for parallel execution of steps
        with ThreadPoolExecutor(max_workers=CONFIG["MAX_WORKERS"]) as executor:
            # Submit all steps to the executor
            futures = [executor.submit(self._execute_step, step) for step in self.migration_steps]
            
            # Wait for all steps to complete
            for future in futures:
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Step execution failed: {e}")
        
        # Update stats
        self.results["stats"]["completed_steps"] = self.results["stats"]["successful_steps"] + self.results["stats"]["failed_steps"]
        
        logger.info(f"Completed {self.results['stats']['completed_steps']} of {self.results['stats']['total_steps']} steps")
        if self.results["stats"]["failed_steps"] > 0:
            logger.warning(f"{self.results['stats']['failed_steps']} steps failed")
    
    def _execute_step(self, step: Dict[str, Any]) -> None:
        """
        Execute a single migration step.
        
        Args:
            step: Migration step to execute
        """
        step_id = step["id"]
        action = step["action"]
        source_path = step["source_path"]
        target_path = step["target_path"]
        
        logger.info(f"Executing step {step_id}: {action} {source_path} -> {target_path if target_path else 'N/A'}")
        
        try:
            if self.dry_run:
                # In dry run mode, just log the operation
                logger.info(f"[DRY RUN] Would {action} {source_path} to {target_path if target_path else 'N/A'}")
                
                with self.lock:
                    self.results["successful_operations"].append({
                        "step_id": step_id,
                        "action": action,
                        "source_path": source_path,
                        "target_path": target_path,
                        "dry_run": True
                    })
                    self.results["stats"]["successful_steps"] += 1
                
                return
            
            # Execute the appropriate action
            if action == "move":
                self._move_file(step)
            elif action == "archive":
                self._archive_file(step)
            elif action == "delete":
                self._delete_file(step)
            else:
                raise ValueError(f"Unknown action: {action}")
            
            # Record successful operation
            with self.lock:
                self.results["successful_operations"].append({
                    "step_id": step_id,
                    "action": action,
                    "source_path": source_path,
                    "target_path": target_path
                })
                self.results["stats"]["successful_steps"] += 1
            
        except Exception as e:
            logger.error(f"Step {step_id} failed: {e}")
            
            # Record failed operation
            with self.lock:
                self.results["failed_operations"].append({
                    "step_id": step_id,
                    "action": action,
                    "source_path": source_path,
                    "target_path": target_path,
                    "error": str(e)
                })
                self.results["stats"]["failed_steps"] += 1
    
    def _move_file(self, step: Dict[str, Any]) -> None:
        """
        Move a file to the target location.
        
        Args:
            step: Migration step containing file information
        """
        source_path = os.path.join(self.egos_root, step["source_path"])
        target_path = os.path.join(self.egos_root, step["target_path"])
        
        # Ensure target directory exists
        os.makedirs(os.path.dirname(target_path), exist_ok=True)
        
        # Check if target file already exists
        if os.path.exists(target_path):
            raise FileExistsError(f"Target file already exists: {target_path}")
        
        # Move the file
        shutil.move(source_path, target_path)
        logger.info(f"Moved {source_path} to {target_path}")
    
    def _archive_file(self, step: Dict[str, Any]) -> None:
        """
        Archive a file to the archive location.
        
        Args:
            step: Migration step containing file information
        """
        source_path = os.path.join(self.egos_root, step["source_path"])
        target_path = os.path.join(self.egos_root, step["target_path"])
        
        # Ensure target directory exists
        os.makedirs(os.path.dirname(target_path), exist_ok=True)
        
        # Check if target file already exists
        if os.path.exists(target_path):
            raise FileExistsError(f"Target file already exists: {target_path}")
        
        # Copy the file to the archive location
        shutil.copy2(source_path, target_path)
        
        # Add a comment to the file indicating it's archived
        self._mark_as_archived(target_path)
        
        # Delete the original file
        os.remove(source_path)
        
        logger.info(f"Archived {source_path} to {target_path}")
    
    def _mark_as_archived(self, file_path: str) -> None:
        """
        Add a comment to a file indicating it's archived.
        
        Args:
            file_path: Path to the file to mark
        """
        _, ext = os.path.splitext(file_path)
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        archive_notice = f"ARCHIVED: This file was archived by the Directory Unification Tool on {timestamp}. "
        archive_notice += f"It was part of the {self.keyword} consolidation process."
        
        # Don't try to modify binary files
        if ext.lower() in CONFIG["FILE_EXTENSIONS_TO_UPDATE"]:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                
                # Add the archive notice in a format appropriate for the file type
                if ext.lower() == ".py":
                    archive_notice = f"# {archive_notice}"
                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(f"{archive_notice}\n\n{content}")
                
                elif ext.lower() == ".md" or ext.lower() == ".txt":
                    archive_notice = f"<!-- {archive_notice} -->"
                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(f"{archive_notice}\n\n{content}")
                
                elif ext.lower() in [".html", ".htm", ".xml"]:
                    archive_notice = f"<!-- {archive_notice} -->"
                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(f"{archive_notice}\n\n{content}")
                
                elif ext.lower() in [".js", ".jsx", ".ts", ".tsx", ".css"]:
                    archive_notice = f"/* {archive_notice} */"
                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(f"{archive_notice}\n\n{content}")
                
                elif ext.lower() in [".json", ".yml", ".yaml"]:
                    # These formats don't typically support comments
                    # So we'll add a property or metadata if possible
                    pass
            
            except Exception as e:
                self.logger.warning(f"Failed to mark file as archived: {e}")
    
    def _delete_file(self, step: Dict[str, Any]) -> None:
        """
        Delete a file.
        
        Args:
            step: Migration step containing file information
        """
        source_path = os.path.join(self.egos_root, step["source_path"])
        
        # Delete the file
        os.remove(source_path)
        self.logger.info(f"Deleted {source_path}")
    
    def _update_references(self) -> None:
        """Update references in all files to reflect the new locations."""
        self.logger.info("Updating references in files")
        
        # Build a mapping of old paths to new paths
        path_mapping = {}
        for op in self.results["successful_operations"]:
            if op["action"] in ["move", "archive"] and op["target_path"]:
                path_mapping[op["source_path"]] = op["target_path"]
        
        # If no paths were changed, we don't need to update references
        if not path_mapping:
            self.logger.info("No path changes to update references for")
            return
        
        # Find all relevant files to update references in
        files_to_update = self._find_files_to_update()
        
        # Create a thread pool for parallel processing
        with ThreadPoolExecutor(max_workers=CONFIG["MAX_WORKERS"]) as executor:
            # Submit all files for reference updating
            futures = [executor.submit(self._update_file_references, file_path, path_mapping) for file_path in files_to_update]
            
            # Wait for all updates to complete
            for future in futures:
                try:
                    result = future.result()
                    if result:
                        with self.lock:
                            self.results["reference_updates"].append(result)
                except Exception as e:
                    self.logger.error(f"Reference update failed: {e}")
        
        self.logger.info(f"Updated references in {len(self.results['reference_updates'])} files")
    
    def _find_files_to_update(self) -> List[str]:
        """
        Find all files that might need reference updates.
        
        Returns:
            List of file paths to check for reference updates
        """
        files_to_update = []
        
        # Use recursive search to find all relevant files
        for root, _, files in os.walk(str(self.egos_root)):
            for file in files:
                # Skip backup directory
                if CONFIG["BACKUP_DIRECTORY"] in root.split(os.path.sep):
                    continue
                
                # Skip files we've just moved or deleted
                file_path = os.path.join(root, file)
                rel_path = os.path.relpath(file_path, self.egos_root)
                
                # Skip files we've already processed
                if any(op["target_path"] == rel_path for op in self.results["successful_operations"]):
                    continue
                
                # Check file extension
                _, ext = os.path.splitext(file)
                if ext.lower() in CONFIG["FILE_EXTENSIONS_TO_UPDATE"]:
                    files_to_update.append(file_path)
        
        self.logger.info(f"Found {len(files_to_update)} files to check for reference updates")
        return files_to_update
    
    def _update_file_references(self, file_path: str, path_mapping: Dict[str, str]) -> Optional[Dict[str, Any]]:
        """
        Update references in a single file.
        
        Args:
            file_path: Path to the file to update
            path_mapping: Mapping of old paths to new paths
            
        Returns:
            Dictionary with update information if changes were made, None otherwise
        """
        try:
            # Read the file content
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
            
            # Check for references to updated paths
            original_content = content
            changes_made = False
            
            for old_path, new_path in path_mapping.items():
                for pattern in CONFIG["REFERENCE_PATTERNS"]:
                    # Process the pattern to make it a valid regex
                    escaped_old_path = re.escape(old_path).replace("\\\\", "\\")
                    
                    # Try to find references to the old path
                    regex = pattern.replace(r'([^"\'\\]+)', escaped_old_path)
                    regex = regex.replace(r'([^"]+)', escaped_old_path)
                    regex = regex.replace(r"([^']+)", escaped_old_path)
                    regex = regex.replace(r'([^\n]+)', escaped_old_path)
                    regex = regex.replace(r'(C:\\EGOS\\[^\n]+)', old_path)
                    
                    # Replace references to the old path with the new path
                    new_content = re.sub(regex, lambda m: m.group(0).replace(old_path, new_path), content)
                    
                    if new_content != content:
                        content = new_content
                        changes_made = True
            
            # If changes were made, write the updated content
            if changes_made:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(content)
                
                rel_path = os.path.relpath(file_path, self.egos_root)
                self.logger.info(f"Updated references in {rel_path}")
                
                # Return information about the update
                return {
                    "file_path": rel_path,
                    "changes_made": True
                }
            
            return None
            
        except Exception as e:
            rel_path = os.path.relpath(file_path, self.egos_root)
            self.logger.error(f"Failed to update references in {rel_path}: {e}")
            return {
                "file_path": rel_path,
                "changes_made": False,
                "error": str(e)
            }


def main():
    """Main function for testing the MigrationExecutor module."""
    import argparse
    import json
    
    # Print banner
    print_banner("Migration Executor Module")
    
    # Parse arguments
    parser = argparse.ArgumentParser(description="Migration Executor Module for Directory Unification Tool")
    parser.add_argument("--plan-file", required=True, help="Path to consolidation plan JSON file")
    parser.add_argument("--keyword", required=True, help="Keyword for consolidation")
    parser.add_argument("--egos-root", help="Path to EGOS root directory")
    parser.add_argument("--dry-run", action="store_true", help="Perform a dry run without making changes")
    parser.add_argument("--no-backup", action="store_true", help="Skip creating backups")
    parser.add_argument("--force", action="store_true", help="Force execution even if backup fails")
    parser.add_argument("--output", help="Output file for results (JSON format)")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose output for testing")
    
    args = parser.parse_args()
    
    # Load consolidation plan
    try:
        with open(args.plan_file, "r", encoding="utf-8") as f:
            plan = json.load(f)
    except Exception as e:
        print(f"{Fore.RED}Error loading plan file: {e}{Style.RESET_ALL}")
        return
    
    # Convert arguments to dictionary
    args_dict = vars(args)
    args_dict["create_backup"] = not args.no_backup
    
    # Create context
    context = {
        "plan": plan
    }
    
    # Create MigrationExecutor instance
    log_format_str = CONFIG.get("LOG_FORMAT", "%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    log_level_val = logging.DEBUG if args_dict.get("verbose") else logging.INFO
    test_logger = setup_logger(
        "migration_executor_test",
        log_format_str,
        log_level=log_level_val
    )
    executor = MigrationExecutor(args_dict, context, test_logger)
    
    # Execute migration
    results = executor.execute_migration()
    
    # Display summary
    print(f"\n{Fore.CYAN}Migration Results:{Style.RESET_ALL}")
    print(f"  {Fore.GREEN}Total Steps:{Style.RESET_ALL} {results['stats']['total_steps']}")
    print(f"  {Fore.GREEN}Completed Steps:{Style.RESET_ALL} {results['stats']['completed_steps']}")
    print(f"  {Fore.GREEN}Successful Steps:{Style.RESET_ALL} {results['stats']['successful_steps']}")
    print(f"  {Fore.GREEN}Failed Steps:{Style.RESET_ALL} {results['stats']['failed_steps']}")
    
    if results["stats"]["backup_created"]:
        print(f"  {Fore.GREEN}Backup Created:{Style.RESET_ALL} {results['stats']['backup_location']}")
    elif args.dry_run:
        print(f"  {Fore.YELLOW}Backup Skipped:{Style.RESET_ALL} Dry run mode")
    else:
        print(f"  {Fore.RED}Backup Not Created{Style.RESET_ALL}")
    
    print(f"\n{Fore.CYAN}Reference Updates:{Style.RESET_ALL}")
    print(f"  {Fore.GREEN}Files Updated:{Style.RESET_ALL} {len(results['reference_updates'])}")
    
    if results["failed_operations"]:
        print(f"\n{Fore.RED}Failed Operations:{Style.RESET_ALL}")
        for op in results["failed_operations"]:
            print(f"  - {op['action']} {op['source_path']} -> {op['target_path'] if op['target_path'] else 'N/A'}")
            print(f"    Error: {op['error']}")
    
    # Output results to file if specified
    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2)
        
        print(f"\n{Fore.GREEN}Results saved to {args.output}{Style.RESET_ALL}")
    
    print(f"\n{Fore.CYAN}✧༺❀༻∞ EGOS ∞༺❀༻✧{Style.RESET_ALL}")


if __name__ == "__main__":
    main()