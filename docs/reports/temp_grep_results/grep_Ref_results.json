# @references:
#   - .windsurfrules
#   - CODE_OF_CONDUCT.md
#   - MQP.md
#   - README.md
#   - ROADMAP.md
#   - CROSSREF_STANDARD.md

[
  {
    "File": "C:\\EGOS\\MQP.md",
    "LineNumber": 132,
    "LineContent": "*   **Conscious Modularity:** Deep understanding of parts and whole (Ref: [[subsystem_boundaries.mdc](cci:7://file:///C:/EGOS/.cursor/rules/subsystem_boundaries.mdc:0:0-0:0)](mdc:../../.cursor/rules/subsystem_boundaries.mdc), [`file_modularity.mdc`](mdc:../../.cursor/rules/file_modularity.mdc))."
  },
  {
    "File": "C:\\EGOS\\ROADMAP.md",
    "LineNumber": 130,
    "LineContent": "* [IN PROGRESS] [KOIOS/NEXUS][XREF-STD-01] Implement canonical cross-reference standard across EGOS ecosystem (`HIGH`) (Ref: Systemic Cartography)"
  },
  {
    "File": "C:\\EGOS\\Work_2025-05-20_Docs_Optimization.md",
    "LineNumber": 71,
    "LineContent": "    *   Renamed `c:\\EGOS\\docs\\subsystems\\ATLAS\\ATL_description.md` to `c:\\EGOS\\docs\\subsystems\\ATLAS\\README.md`. Updated its frontmatter for clarity (Purpose: \"Systemic Cartography & Visualization\") and revised its overview section. (Ref: Step ID 1980s)"
  },
  {
    "File": "C:\\EGOS\\WORK_2025_05_21.md",
    "LineNumber": 157,
    "LineContent": "            *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, `Related:`, `Doc:`, `Link to:`)."
  },
  {
    "File": "C:\\EGOS\\docs\\core_materials\\MQP.md",
    "LineNumber": 132,
    "LineContent": "*   **Conscious Modularity:** Deep understanding of parts and whole (Ref: [[subsystem_boundaries.mdc](cci:7://file:///C:/EGOS/.cursor/rules/subsystem_boundaries.mdc:0:0-0:0)](mdc:../../.cursor/rules/subsystem_boundaries.mdc), [`file_modularity.mdc`](mdc:../../.cursor/rules/file_modularity.mdc))."
  },
  {
    "File": "C:\\EGOS\\docs\\core_materials\\historical_changelogs\\Updates system EVA.txt",
    "LineNumber": 1214,
    "LineContent": "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:"
  },
  {
    "File": "C:\\EGOS\\docs\\core_materials\\templates\\PDD_Template.yaml",
    "LineNumber": 19,
    "LineContent": "#   bias_mitigation_ref: (Optional) Link to bias docs."
  },
  {
    "File": "C:\\EGOS\\docs\\core_materials\\website\\DESIGN_GUIDE.md",
    "LineNumber": 94,
    "LineContent": "## 11. Content Strategy Notes (Ref: Task WCS-1)"
  },
  {
    "File": "C:\\EGOS\\docs\\diagnostics\\system_handover_20250512.md",
    "LineNumber": 57,
    "LineContent": "*   **`docs_egos/` Directory:** Intended as the central hub for all project documentation. Current reorganization efforts are underway to streamline this directory (Ref: `diagnosticoENIO.md`, `DOCS_DIRECTORY_DIAGNOSTIC_*.md`)."
  },
  {
    "File": "C:\\EGOS\\docs\\governance\\EGOS Trust Weave MVP.txt",
    "LineNumber": 931,
    "LineContent": "    # Operational Rules (Ref: MEMORY[user_global], MEMORY[user_3779329090916737096])"
  },
  {
    "File": "C:\\EGOS\\docs\\governance\\reports\\EGOS_Project_Diagnostic_Report.md",
    "LineNumber": 554,
    "LineContent": "- - **[CORUJA-DOCS-026]** Status: `OPEN`. Module docstring is incomplete (contains TODO placeholder) and does not conform to EGOS standard format (Ref: MEMORY[05e5435b...])."
  },
  {
    "File": "C:\\EGOS\\docs\\process\\automated_docstring_fixing.md",
    "LineNumber": 69,
    "LineContent": "    Ref: PROC-KOIOS-003"
  },
  {
    "File": "C:\\EGOS\\docs\\process\\human_ai_collaboration_guidelines.md",
    "LineNumber": 5,
    "LineContent": "These guidelines enhance the EGOS operational framework (ref: `MQP`, `.windsurfrules`, `MEMORY[user_global]`, `MEMORY[user_17200193039781666577]`) with specific principles for effective human-AI collaboration, drawing from modern software engineering research (2024-2025)."
  },
  {
    "File": "C:\\EGOS\\docs\\products\\website\\DESIGN_GUIDE.md",
    "LineNumber": 48,
    "LineContent": "## 3. Color Palette (Ref: `colors.json` - TBD, Task: `RESEARCH-COLOR`)"
  },
  {
    "File": "C:\\EGOS\\docs\\products\\website\\website_WEBSITE_ROADMAP.md",
    "LineNumber": 218,
    "LineContent": "- [x] VIS-INT-002: Fix node movement on hover while preserving zoom/pan. (Ref: `SystemGraph.tsx`)"
  },
  {
    "File": "C:\\EGOS\\docs\\project\\MQP.md",
    "LineNumber": 134,
    "LineContent": "* **Conscious Modularity:** Deep understanding of parts and whole (Ref: [subsystem_boundaries](../../../.cursor/rules/subsystem_boundaries.mdc), [file_modularity](../../../.cursor/rules/file_modularity.mdc))."
  },
  {
    "File": "C:\\EGOS\\docs\\project\\archived_project_concepts\\concepts_archive\\maiêutic_interaction.md",
    "LineNumber": 48,
    "LineContent": "This document outlines the concept and implementation strategies for the **Maiêutic Interaction Style** within the EGOS ecosystem. Inspired by the Socratic method, this style prioritizes guiding the user through questioning to foster self-discovery, critical thinking, and deeper understanding, rather than simply providing direct answers. This aligns with user preferences for intellectual exploration and autonomy (Ref: `MEMORY[761dc03c-611e-4538-8dc9-799a96573864]`, `MEMORY[User Preference for AI Maiêutic Interaction]`)."
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\cross_reference_inventory_20250521.md",
    "LineNumber": 73,
    "LineContent": "              *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, ..."
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_Doc_results.json",
    "LineNumber": 10,
    "LineContent": "    \"LineContent\": \"            *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, `Related:`, `Doc:`, `Link to:`).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_EGOS_ID_Start_results.json",
    "LineNumber": 285,
    "LineContent": "    \"LineContent\": \"    * [Planned] **Task W2.6: Formal Alternative LLM Evaluation:** Systematically evaluate 2-3 alternative models on OpenRouter based on defined criteria (cost, context, performance, reliability) using EGOS-specific tasks. Document findings (Ref: LLM Plan VIII).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_LinkTo_results.json",
    "LineNumber": 5,
    "LineContent": "    \"LineContent\": \"            *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, `Related:`, `Doc:`, `Link to:`).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_MEMORY_results.json",
    "LineNumber": 65,
    "LineContent": "    \"LineContent\": \"## 4. Limitations & Human Oversight (Ref: MEMORY[92617db1...])\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_Reference_results.json",
    "LineNumber": 10,
    "LineContent": "    \"LineContent\": \"            *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, `Related:`, `Doc:`, `Link to:`).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_Ref_results.json",
    "LineNumber": 5,
    "LineContent": "    \"LineContent\": \"*   **Conscious Modularity:** Deep understanding of parts and whole (Ref: [[subsystem_boundaries.mdc](cci:7://file:///C:/EGOS/.cursor/rules/subsystem_boundaries.mdc:0:0-0:0)](mdc:../../.cursor/rules/subsystem_boundaries.mdc), [`file_modularity.mdc`](mdc:../../.cursor/rules/file_modularity.mdc)).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_REF_XYZ_Start_results.json",
    "LineNumber": 5,
    "LineContent": "    \"LineContent\": \"            *   Structural patterns (e.g., `[[WikiLinks]]`, `(REF-...)`, `xref:...`, Markdown anchor links like `(#anchor)` if potentially used for cross-document linking).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_Related_results.json",
    "LineNumber": 5,
    "LineContent": "    \"LineContent\": \"            *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, `Related:`, `Doc:`, `Link to:`).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_RelativePathParent_results.json",
    "LineNumber": 1030,
    "LineContent": "    \"LineContent\": \"    \\\"LineContent\\\": \\\"*   **Conscious Modularity:** Deep understanding of parts and whole (Ref: [[subsystem_boundaries.mdc](cci:7://file:///C:/EGOS/.cursor/rules/subsystem_boundaries.mdc:0:0-0:0)](mdc:../../.cursor/rules/subsystem_boundaries.mdc), [`file_modularity.mdc`](mdc:../../.cursor/rules/file_modularity.mdc)).\\\"\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_RelativePathSelf_results.json",
    "LineNumber": 1110,
    "LineContent": "    \"LineContent\": \"    \\\"LineContent\\\": \\\"*   **Conscious Modularity:** Deep understanding of parts and whole (Ref: [[subsystem_boundaries.mdc](cci:7://file:///C:/EGOS/.cursor/rules/subsystem_boundaries.mdc:0:0-0:0)](mdc:../../.cursor/rules/subsystem_boundaries.mdc), [`file_modularity.mdc`](mdc:../../.cursor/rules/file_modularity.mdc)).\\\"\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_SeeAlso_results.json",
    "LineNumber": 5,
    "LineContent": "    \"LineContent\": \"            *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, `Related:`, `Doc:`, `Link to:`).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_Source_results.json",
    "LineNumber": 10,
    "LineContent": "    \"LineContent\": \"            *   Common referencing keywords (e.g., `Ref:`, `Reference:`, `Source:`, `See also:`, `Related:`, `Doc:`, `Link to:`).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_WikiLinkStart_results.json",
    "LineNumber": 10,
    "LineContent": "    \"LineContent\": \"            *   Structural patterns (e.g., `[[WikiLinks]]`, `(REF-...)`, `xref:...`, Markdown anchor links like `(#anchor)` if potentially used for cross-document linking).\""
  },
  {
    "File": "C:\\EGOS\\docs\\reports\\temp_grep_results\\grep_xref_results.json",
    "LineNumber": 5,
    "LineContent": "    \"LineContent\": \"            *   Structural patterns (e.g., `[[WikiLinks]]`, `(REF-...)`, `xref:...`, Markdown anchor links like `(#anchor)` if potentially used for cross-document linking).\""
  },
  {
    "File": "C:\\EGOS\\docs\\resources\\data\\PDD_Template.yaml",
    "LineNumber": 23,
    "LineContent": "#   bias_mitigation_ref: (Optional) Link to bias docs."
  },
  {
    "File": "C:\\EGOS\\docs\\standards\\user_profile_standard.md",
    "LineNumber": 67,
    "LineContent": "    - `interaction_style`: (e.g., 'default', 'direct', 'maiêutic', 'empathetic') - User's preferred AI interaction approach. Ref: `MEMORY[...]`, [CONCEPT-MAIEUTIC-001](cci:7://file:///c:/EGOS/docs_egos/concepts/mai%C3%AAutic_interaction.md:0:0-0:0)"
  },
  {
    "File": "C:\\EGOS\\docs\\STRATEGIC_THINKING\\meta_prompts\\strategic_analysis_prompt_v2.0.md",
    "LineNumber": 13,
    "LineContent": "* **Technology & Innovation (including AI & Human-AI Collaboration):** Deep knowledge of tech trends, AI/LLM capabilities and limitations, practical applications, human-AI interaction patterns (ref: LLM/IDE Study), security/privacy/IP risks of AI-generated code, and opportunities."
  },
  {
    "File": "C:\\EGOS\\docs\\technical\\components\\website_dev_archive\\DESIGN_GUIDE.md",
    "LineNumber": 152,
    "LineContent": "## 11. Content Strategy Notes (Ref: Task WCS-1)"
  },
  {
    "File": "C:\\EGOS\\docs\\templates\\reference_templates\\PDD_Template.yaml",
    "LineNumber": 23,
    "LineContent": "#   bias_mitigation_ref: (Optional) Link to bias docs."
  },
  {
    "File": "C:\\EGOS\\docs\\templates\\reference_templates\\prompts\\strategic_analysis_prompt.md",
    "LineNumber": 67,
    "LineContent": "* **Technology & Innovation (including AI & Human-AI Collaboration):** Deep knowledge of tech trends, AI/LLM capabilities and limitations, practical applications, human-AI interaction patterns (ref: LLM/IDE Study), security/privacy/IP risks of AI-generated code, and opportunities."
  },
  {
    "File": "C:\\EGOS\\docs\\website\\DESIGN_GUIDE.md",
    "LineNumber": 94,
    "LineContent": "## 11. Content Strategy Notes (Ref: Task WCS-1)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\00_project_overview\\ROADMAP.md",
    "LineNumber": 75,
    "LineContent": "* [DONE] [SPARC/HARMONY/KOIOS][HTA-02] Automate Git History Analysis via CI/CD (GitHub Actions) (`MEDIUM`) (Ref: Evolutionary Preservation)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\01_core_concepts\\strategic_thinking\\meta_prompts\\strategic_analysis_prompt_v2.0.md",
    "LineNumber": 13,
    "LineContent": "* **Technology & Innovation (including AI & Human-AI Collaboration):** Deep knowledge of tech trends, AI/LLM capabilities and limitations, practical applications, human-AI interaction patterns (ref: LLM/IDE Study), security/privacy/IP risks of AI-generated code, and opportunities."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\01_core_principles\\core_materials\\historical_changelogs\\Updates system EVA.txt",
    "LineNumber": 1214,
    "LineContent": "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\01_core_principles\\core_materials\\templates\\PDD_Template.yaml",
    "LineNumber": 19,
    "LineContent": "#   bias_mitigation_ref: (Optional) Link to bias docs."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\01_core_principles\\core_materials\\website\\DESIGN_GUIDE.md",
    "LineNumber": 94,
    "LineContent": "## 11. Content Strategy Notes (Ref: Task WCS-1)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\01_core_principles\\governance_documents\\EGOS Trust Weave MVP.txt",
    "LineNumber": 931,
    "LineContent": "    # Operational Rules (Ref: MEMORY[user_global], MEMORY[user_3779329090916737096])"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\01_core_principles\\governance_documents\\reports\\EGOS_Project_Diagnostic_Report.md",
    "LineNumber": 554,
    "LineContent": "- - **[CORUJA-DOCS-026]** Status: `OPEN`. Module docstring is incomplete (contains TODO placeholder) and does not conform to EGOS standard format (Ref: MEMORY[05e5435b...])."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\01_core_principles\\standards_documents\\user_profile_standard.md",
    "LineNumber": 67,
    "LineContent": "    - `interaction_style`: (e.g., 'default', 'direct', 'maiêutic', 'empathetic') - User's preferred AI interaction approach. Ref: `MEMORY[...]`, [CONCEPT-MAIEUTIC-001](cci:7://file:///c:/EGOS/docs_egos/concepts/mai%C3%AAutic_interaction.md:0:0-0:0)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\02_architecture\\technical_documents\\components\\website_dev_archive\\DESIGN_GUIDE.md",
    "LineNumber": 152,
    "LineContent": "## 11. Content Strategy Notes (Ref: Task WCS-1)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\02_koios_standards\\user_profile_standard.md",
    "LineNumber": 67,
    "LineContent": "    - `interaction_style`: (e.g., 'default', 'direct', 'maiêutic', 'empathetic') - User's preferred AI interaction approach. Ref: `MEMORY[...]`, [CONCEPT-MAIEUTIC-001](cci:7://file:///c:/EGOS/docs_egos/concepts/mai%C3%AAutic_interaction.md:0:0-0:0)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\03_processes\\ai_workflows\\human_ai_collaboration_guidelines.md",
    "LineNumber": 5,
    "LineContent": "These guidelines enhance the EGOS operational framework (ref: `MQP`, `.windsurfrules`, `MEMORY[user_global]`, `MEMORY[user_17200193039781666577]`) with specific principles for effective human-AI collaboration, drawing from modern software engineering research (2024-2025)."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\03_processes\\development\\automated_docstring_fixing.md",
    "LineNumber": 69,
    "LineContent": "    Ref: PROC-KOIOS-003"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\03_subsystems\\CORUJA\\core\\tool_registry.py",
    "LineNumber": 54,
    "LineContent": "        if not implementation_ref:"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\03_subsystems\\CORUJA\\docs\\ARCHITECTURE.md",
    "LineNumber": 225,
    "LineContent": "        implementation_ref: str"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\03_subsystems\\CORUJA\\schemas\\models.py",
    "LineNumber": 49,
    "LineContent": "    implementation_ref: str = Field(..., description=\"Reference to the tool's implementation (e.g., Python import path).\")"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\03_subsystems\\KOIOS\\schemas\\pdd_schema.py",
    "LineNumber": 97,
    "LineContent": "    bias_mitigation_ref: Optional[str] = Field("
  },
  {
    "File": "C:\\EGOS\\docs_egos\\04_products_services\\website\\DESIGN_GUIDE.md",
    "LineNumber": 94,
    "LineContent": "## 11. Content Strategy Notes (Ref: Task WCS-1)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\05_processes_and_workflows\\general_processes\\automated_docstring_fixing.md",
    "LineNumber": 69,
    "LineContent": "    Ref: PROC-KOIOS-003"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\05_processes_and_workflows\\general_processes\\human_ai_collaboration_guidelines.md",
    "LineNumber": 5,
    "LineContent": "These guidelines enhance the EGOS operational framework (ref: `MQP`, `.windsurfrules`, `MEMORY[user_global]`, `MEMORY[user_17200193039781666577]`) with specific principles for effective human-AI collaboration, drawing from modern software engineering research (2024-2025)."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\09_project_meta\\products_overview\\website\\DESIGN_GUIDE.md",
    "LineNumber": 48,
    "LineContent": "## 3. Color Palette (Ref: `colors.json` - TBD, Task: `RESEARCH-COLOR`)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\09_project_meta\\products_overview\\website\\website_WEBSITE_ROADMAP.md",
    "LineNumber": 218,
    "LineContent": "- [x] VIS-INT-002: Fix node movement on hover while preserving zoom/pan. (Ref: `SystemGraph.tsx`)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\09_project_meta\\project_documentation\\MQP.md",
    "LineNumber": 134,
    "LineContent": "* **Conscious Modularity:** Deep understanding of parts and whole (Ref: [subsystem_boundaries](../../../.cursor/rules/subsystem_boundaries.mdc), [file_modularity](../../../.cursor/rules/file_modularity.mdc))."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\09_project_meta\\project_documentation\\archived_project_concepts\\concepts_archive\\maiêutic_interaction.md",
    "LineNumber": 48,
    "LineContent": "This document outlines the concept and implementation strategies for the **Maiêutic Interaction Style** within the EGOS ecosystem. Inspired by the Socratic method, this style prioritizes guiding the user through questioning to foster self-discovery, critical thinking, and deeper understanding, rather than simply providing direct answers. This aligns with user preferences for intellectual exploration and autonomy (Ref: `MEMORY[761dc03c-611e-4538-8dc9-799a96573864]`, `MEMORY[User Preference for AI Maiêutic Interaction]`)."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\09_project_meta\\reports\\html_comment_report_20250425_092241.json",
    "LineNumber": 448,
    "LineContent": "      \"diff\": \"--- a/backup_cleanup.py\\n+++ b/backup_cleanup.py\\n@@ -1,12 +1,12 @@\\n \\\"\\\"\\\"TODO: Module docstring for backup_cleanup.py\\\"\\\"\\\"\\n \\n-<!-- \\n+\\\"\\\"\\\"\\n @references:\\n - Core References:\\n-  - [MQP.md](mdc:../../MQP.md) - Master Quantum Prompt defining EGOS principles\\n-  - [ROADMAP.md](mdc:../../ROADMAP.md) - Project roadmap and planning\\n+- [MQP.md](mdc:../../MQP.md) - Master Quantum Prompt defining EGOS principles\\n+- [ROADMAP.md](mdc:../../ROADMAP.md) - Project roadmap and planning\\n - Process Documentation:\\n-  - [system_maintenance.md](mdc:../../docs_egos/process/system_maintenance.md)\\n+- [system_maintenance.md](mdc:../../docs_egos/process/system_maintenance.md)\\n \\n \\n \\n@@ -39,14 +39,14 @@\\n # EGOS Import Resilience: Add project root to sys.path (see docs_egos/process/dynamic_import_resilience.md)\\n project_root = str(Path(__file__).resolve().parents[2])\\n if project_root not in sys.path:\\n-    sys.path.insert(0, project_root)\\n+sys.path.insert(0, project_root)\\n \\n from scripts.maintenance.status_utils import (\\n-    progress_bar,\\n-    spinner,\\n-    setup_logger,\\n-    box_message,\\n-    section_header\\n+progress_bar,\\n+spinner,\\n+setup_logger,\\n+box_message,\\n+section_header\\n )\\n from typing import List, Dict, Set, Tuple, Optional\\n \\n@@ -54,802 +54,803 @@\\n logger = setup_logger(\\\"egos_backup_cleanup\\\")\\n \\n class BackupCleaner:\\n-            Attributes:\\n-            None\\n+Attributes:\\n+None\\n \\\"\\\"\\\"Cleans unnecessary files from existing backups.\\\"\\\"\\\"\\n-    \\n-    def __init__(\\n-        self,\\n-        backup_dir: str,\\n-        exclusion_patterns: Optional[List[str]] = None,\\n-        dry_run: bool = True,\\n-        verbose: bool = False,\\n-        log_file: Optional[str] = None,\\n-        max_backups: int = 5\\n-    ):\\n-        \\\"\\\"\\\"\\n-        Initialize the backup cleaner.\\n-        \\n-        Args:\\n-            backup_dir: Directory containing backups to clean\\n-            exclusion_patterns: List of patterns to exclude (will be removed from backups)\\n-            dry_run: If True, show what would be done without actually doing it\\n-            verbose: If True, show detailed logs\\n-            log_file: Optional file to log operations\\n-            max_backups: Maximum number of backups to keep (oldest will be removed)\\n-        \\\"\\\"\\\"\\n-        self.backup_dir = backup_dir\\n-        self.dry_run = dry_run\\n-        self.verbose = verbose\\n-        self.max_backups = max_backups\\n-        \\n-        # Default exclusion patterns for unnecessary files\\n-        default_exclusions = [\\n-            \\\"node_modules\\\",\\n-            \\\"venv\\\",\\n-            \\\".venv\\\",\\n-            \\\"env\\\",\\n-            \\\".env\\\", \\n-            \\\"__pycache__\\\",\\n-            \\\".git\\\",\\n-            \\\".pytest_cache\\\",\\n-            \\\".mypy_cache\\\",\\n-            \\\"dist\\\",\\n-            \\\"build\\\",\\n-            \\\"*.egg-info\\\",\\n-            \\\".tox\\\",\\n-            \\\".coverage\\\",\\n-            \\\"htmlcov\\\",\\n-            \\\"*.pyc\\\"\\n-        ]\\n-        \\n-        self.exclusion_patterns = exclusion_patterns or default_exclusions\\n-        \\n-        # Setup file logging if requested\\n-        if log_file:\\n-            file_handler = logging.FileHandler(log_file)\\n-            file_handler.setFormatter(log_formatter)\\n-            logger.addHandler(file_handler)\\n-        \\n-        # Set logging level\\n-        if verbose:\\n-            logger.setLevel(logging.DEBUG)\\n-        \\n-        # Prepare stats\\n-        self.stats = {\\n-            \\\"backups_processed\\\": 0,\\n-            \\\"backups_removed\\\": 0,\\n-            \\\"directories_cleaned\\\": 0,\\n-            \\\"files_removed\\\": 0,\\n-            \\\"bytes_freed\\\": 0,\\n-            \\\"errors\\\": 0\\n-        }\\n-        self.total_removed_size = 0\\n-        \\n-        logger.info(f\\\"Initializing backup cleaner for {self.backup_dir}\\\")\\n-        logger.info(f\\\"Dry Run: {self.dry_run}\\\")\\n-        logger.info(f\\\"Exclusion patterns: {', '.join(self.exclusion_patterns)}\\\")\\n-    \\n-    def _find_backups(self) -> List[Tuple[str, datetime.datetime]]:\\n-        \\\"\\\"\\\"\\n-        Find all backups in the backup directory.\\n-        \\n-        Returns:\\n-            List of tuples (backup_path, creation_time)\\n-        \\\"\\\"\\\"\\n-        backups = []\\n-        \\n-        if not os.path.exists(self.backup_dir):\\n-            logger.warning(f\\\"Backup directory does not exist: {self.backup_dir}\\\")\\n-            return []\\n-        \\n-        # Find zip backups\\n-        for item in os.listdir(self.backup_dir):\\n-            item_path = os.path.join(self.backup_dir, item)\\n-            \\n-            if os.path.isfile(item_path) and item.endswith('.zip') and 'backup' in item.lower():\\n-                try:\\n-                    creation_time = datetime.datetime.fromtimestamp(os.path.getctime(item_path))\\n-                    backups.append((item_path, creation_time))\\n-                    logger.debug(f\\\"Found backup zip: {item_path} (created {creation_time})\\\")\\n-                except Exception as e:\\n-                    logger.warning(f\\\"Error processing backup {item_path}: {e}\\\")\\n-            \\n-            # Also check for uncompressed backup directories\\n-            elif os.path.isdir(item_path) and 'backup' in item.lower():\\n-                try:\\n-                    creation_time = datetime.datetime.fromtimestamp(os.path.getctime(item_path))\\n-                    backups.append((item_path, creation_time))\\n-                    logger.debug(f\\\"Found backup directory: {item_path} (created {creation_time})\\\")\\n-                except Exception as e:\\n-                    logger.warning(f\\\"Error processing backup directory {item_path}: {e}\\\")\\n-        \\n-        # Sort by creation time (newest first)\\n-        backups.sort(key=lambda x: x[1], reverse=True)\\n-        \\n-        return backups\\n-    \\n-    def _should_exclude(self, path: str) -> bool:\\n-        \\\"\\\"\\\"Check if a path matches any exclusion pattern.\\\"\\\"\\\"\\n-        normalized_path = path.replace(\\\"\\\\\\\\\\\", \\\"/\\\")\\n-        \\n-        for pattern in self.exclusion_patterns:\\n-            # Handle wildcard patterns\\n-            if \\\"*\\\" in pattern:\\n-                regex_pattern = pattern.replace(\\\".\\\", \\\"\\\\\\\\.\\\").replace(\\\"*\\\", \\\".*\\\")\\n-                if re.search(regex_pattern, normalized_path, re.IGNORECASE):\\n-                    return True\\n-            # Handle exact matches\\n-            elif pattern.lower() in normalized_path.lower():\\n-                path_parts = normalized_path.lower().split('/')\\n-                if pattern.lower() in path_parts:\\n-                    return True\\n-        \\n-        return False\\n-    \\n-    def _clean_backup_directory(self, backup_path: str) -> int:\\n-        \\\"\\\"\\\"\\n-        Clean unnecessary files from a backup directory.\\n-        \\n-        Args:\\n-            backup_path: Path to the backup directory\\n-            \\n-        Returns:\\n-            Total bytes freed\\n-        \\\"\\\"\\\"\\n-        if not os.path.isdir(backup_path):\\n-            logger.warning(f\\\"Not a directory: {backup_path}\\\")\\n-            return 0\\n-        \\n-        total_bytes_freed = 0\\n-        dirs_removed = 0\\n-        files_removed = 0\\n-        \\n-        # Count items for progress tracking\\n-        dirs_to_scan = [backup_path]\\n-        total_items = sum(len(os.listdir(dir_path)) for dir_path in dirs_to_scan \\n-                         if os.path.isdir(dir_path))\\n-        \\n-        for root, dirs, files in os.walk(backup_path, topdown=True):\\n-            # Check if current directory should be excluded\\n-            dir_name = os.path.basename(root)\\n-            if self._should_exclude(dir_name):\\n-                size = self._get_dir_size(root)\\n-                logger.info(f\\\"{'[DRY RUN] Would remove' if self.dry_run else 'Removing'} directory: {root} ({self._format_size(size)})\\\")\\n-                \\n-                if not self.dry_run:\\n-                    try:\\n-                        shutil.rmtree(root)\\n-                        total_bytes_freed += size\\n-                        dirs_removed += 1\\n-                        self.stats[\\\"directories_cleaned\\\"] += 1\\n-                        self.stats[\\\"bytes_freed\\\"] += size\\n-                        self.total_removed_size += size\\n-                    except Exception as e:\\n-                        logger.error(f\\\"Error removing directory {root}: {e}\\\")\\n-                        self.stats[\\\"errors\\\"] += 1\\n-                \\n-                # Don't process subdirectories\\n-                dirs[:] = []\\n-                continue\\n-            \\n-            # Process files\\n-            for file in files:\\n-                file_path = os.path.join(root, file)\\n-                \\n-                if self._should_exclude(file):\\n-                    try:\\n-                        size = os.path.getsize(file_path)\\n-                        logger.debug(f\\\"{'[DRY RUN] Would remove' if self.dry_run else 'Removing'} file: {file_path} ({self._format_size(size)})\\\")\\n-                        \\n-                        if not self.dry_run:\\n-                            os.remove(file_path)\\n-                            total_bytes_freed += size\\n-                            files_removed += 1\\n-                            self.stats[\\\"files_removed\\\"] += 1\\n-                            self.stats[\\\"bytes_freed\\\"] += size\\n-                            self.total_removed_size += size\\n-                    except Exception as e:\\n-                        logger.error(f\\\"Error removing file {file_path}: {e}\\\")\\n-                        self.stats[\\\"errors\\\"] += 1\\n-        \\n-        logger.info(f\\\"Cleaned backup: {backup_path} - Removed {dirs_removed} directories and {files_removed} files, freed {self._format_size(total_bytes_freed)}\\\")\\n-        return total_bytes_freed\\n-    \\n-    def _extract_and_clean_zip(self, zip_path: str) -> int:\\n-        \\\"\\\"\\\"\\n-        Extract a zip backup, clean it, and recompress it.\\n-        \\n-        Args:\\n-            zip_path: Path to the zip backup\\n-            \\n-        Returns:\\n-            Total bytes freed\\n-        \\\"\\\"\\\"\\n-        if not os.path.isfile(zip_path) or not zip_path.endswith('.zip'):\\n-            logger.warning(f\\\"Not a zip file: {zip_path}\\\")\\n-            return 0\\n-        \\n-        # Get original size\\n-        original_size = os.path.getsize(zip_path)\\n-        \\n-        # Create a temporary extraction directory\\n-        temp_dir = os.path.join(self.backup_dir, f\\\"temp_extract_{os.path.basename(zip_path).replace('.zip', '')}\\\")\\n-        if os.path.exists(temp_dir):\\n-            shutil.rmtree(temp_dir)\\n-        os.makedirs(temp_dir)\\n-        \\n-        try:\\n-            # Extract the zip\\n-            logger.info(f\\\"Extracting {zip_path} to {temp_dir}\\\")\\n-            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\\n-                zip_ref.extractall(temp_dir)\\n-            \\n-            # Clean the extracted directory\\n-            bytes_freed = self._clean_backup_directory(temp_dir)\\n-            \\n-            if not self.dry_run:\\n-                # Create a new zip file\\n-                new_zip_path = zip_path + \\\".new\\\"\\n-                logger.info(f\\\"Creating new zip file: {new_zip_path}\\\")\\n-                \\n-                with zipfile.ZipFile(new_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\\n-                    for root, dirs, files in os.walk(temp_dir):\\n-                        for file in files:\\n-                            try:\\n-                                file_path = os.path.join(root, file)\\n-                                arcname = os.path.relpath(file_path, temp_dir)\\n-                                zipf.write(file_path, arcname)\\n-                            except Exception as e:\\n-                                logger.error(f\\\"Error adding {file_path} to zip: {e}\\\")\\n-                                self.stats[\\\"errors\\\"] += 1\\n-                \\n-                # Replace the original zip with the new one\\n-                new_size = os.path.getsize(new_zip_path)\\n-                bytes_freed = original_size - new_size\\n-                \\n-                os.remove(zip_path)\\n-                os.rename(new_zip_path, zip_path)\\n-                logger.info(f\\\"Replaced {zip_path} - Old size: {self._format_size(original_size)}, New size: {self._format_size(new_size)}\\\")\\n-            \\n-            return bytes_freed\\n-        \\n-        except Exception as e:\\n-            logger.error(f\\\"Error processing zip {zip_path}: {e}\\\")\\n-            self.stats[\\\"errors\\\"] += 1\\n-            return 0\\n-        \\n-        finally:\\n-            # Clean up the temporary directory\\n-            if os.path.exists(temp_dir) and not self.dry_run:\\n-                shutil.rmtree(temp_dir)\\n-    \\n-    def _remove_old_backups(self, backups: List[Tuple[str, datetime.datetime]]) -> int:\\n-        \\\"\\\"\\\"\\n-        Remove backups exceeding the maximum number to keep.\\n-        \\n-        Args:\\n-            backups: List of backups sorted by creation time (newest first)\\n-            \\n-        Returns:\\n-            Total bytes freed\\n-        \\\"\\\"\\\"\\n-        if len(backups) <= self.max_backups:\\n-            logger.info(f\\\"Only {len(backups)} backups found, not exceeding limit of {self.max_backups}\\\")\\n-            return 0\\n-        \\n-        total_bytes_freed = 0\\n-        backups_to_remove = backups[self.max_backups:]\\n-        \\n-        for backup_path, creation_time in backups_to_remove:\\n-            try:\\n-                size = os.path.getsize(backup_path) if os.path.isfile(backup_path) else self._get_dir_size(backup_path)\\n-                logger.info(f\\\"{'[DRY RUN] Would remove' if self.dry_run else 'Removing'} old backup: {backup_path} ({self._format_size(size)}, created {creation_time})\\\")\\n-                \\n-                if not self.dry_run:\\n-                    if os.path.isfile(backup_path):\\n-                        os.remove(backup_path)\\n-                    else:\\n-                        shutil.rmtree(backup_path)\\n-                    \\n-                    total_bytes_freed += size\\n-                    self.stats[\\\"backups_removed\\\"] += 1\\n-                    self.stats[\\\"bytes_freed\\\"] += size\\n-                    self.total_removed_size += size\\n-            except Exception as e:\\n-                logger.error(f\\\"Error removing backup {backup_path}: {e}\\\")\\n-                self.stats[\\\"errors\\\"] += 1\\n-        \\n-        return total_bytes_freed\\n-    \\n-    def _get_dir_size(self, path: str) -> int:\\n-        \\\"\\\"\\\"Get the total size of a directory in bytes.\\\"\\\"\\\"\\n-        total_size = 0\\n-        for dirpath, dirnames, filenames in os.walk(path):\\n-            for f in filenames:\\n-                fp = os.path.join(dirpath, f)\\n-                if not os.path.islink(fp):\\n-                    total_size += os.path.getsize(fp)\\n-        return total_size\\n-    \\n-    def _format_size(self, size_bytes: int) -> str:\\n-        \\\"\\\"\\\"Format size in bytes to human-readable format.\\\"\\\"\\\"\\n-        for unit in ['B', 'KB', 'MB', 'GB']:\\n-            if size_bytes < 1024.0:\\n-                return f\\\"{size_bytes:.1f} {unit}\\\"\\n-            size_bytes /= 1024.0\\n-        return f\\\"{size_bytes:.1f} TB\\\"\\n-    \\n-    def _remove_item(self, item_path: str) -> int:\\n-        \\\"\\\"\\\"Remove a file or directory, handling potential errors.\\\"\\\"\\\"\\n-        item_size = self._get_item_size(item_path)\\n-        action = \\\"directory\\\" if os.path.isdir(item_path) else \\\"file\\\"\\n-        \\n-        logger.debug(f\\\"Attempting to remove {action}: {item_path} ({self._format_size(item_size)})\\\")\\n-        \\n-        if self.dry_run:\\n-            logger.info(f\\\"[Dry Run] Would remove {action}: {item_path} ({self._format_size(item_size)})\\\")\\n-            return item_size  # Simulate freeing space in dry run\\n-\\n-        try:\\n-            if os.path.isdir(item_path):\\n-                # Use onerror handler for rmtree to deal with read-only files\\n-                def remove_readonly(func, path, exc_info):\\n-                    \\\"\\\"\\\"Error handler for shutil.rmtree that tries to remove read-only files.\\\"\\\"\\\"\\n-                    # Check if the error is PermissionError\\n-                    exc_type, exc_value, tb = exc_info\\n-                    if exc_value and isinstance(exc_value, PermissionError):\\n-                        logger.warning(f\\\"PermissionError accessing {path}. Attempting to change permissions.\\\")\\n-                        try:\\n-                            os.chmod(path, 0o777) # Set permissions to read/write/execute for all\\n-                            func(path) # Retry the operation\\n-                        except Exception as retry_exc:\\n-                            logger.error(f\\\"Failed to remove {path} even after chmod: {retry_exc}\\\")\\n-                            # Re-raise the original exception if chmod/retry fails\\n-                            raise exc_value\\n-                    else:\\n-                        # If it's not a PermissionError, raise it to stop the process\\n-                        # or handle other specific errors as needed.\\n-                        raise exc_value # Re-raise the original error\\n-\\n-                shutil.rmtree(item_path, onerror=remove_readonly)\\n-                self.stats[f\\\"{action}s_removed\\\"] += 1\\n-                logger.info(f\\\"Removed {action}: {item_path}\\\")\\n-                self.total_removed_size += item_size\\n-            elif os.path.isfile(item_path):\\n-                try:\\n-                    os.remove(item_path)\\n-                except PermissionError:\\n-                    logger.warning(f\\\"PermissionError removing file {item_path}. Attempting to change permissions.\\\")\\n-                    os.chmod(item_path, 0o777)\\n-                    os.remove(item_path) # Retry removal\\n-                self.stats[f\\\"{action}s_removed\\\"] += 1\\n-                logger.info(f\\\"Removed {action}: {item_path}\\\")\\n-                self.total_removed_size += item_size\\n-            else:\\n-                logger.warning(f\\\"Item not found or not a file/directory: {item_path}\\\")\\n-                return 0 # Not found, no space freed\\n-\\n-            self.stats[\\\"bytes_freed\\\"] += item_size\\n-            return item_size\\n-\\n-        except PermissionError as e:\\n-            # This block might catch errors not handled by onerror or file removal retry\\n-            logger.error(f\\\"Final PermissionError removing {action} {item_path}: {e}. Attempting fallback...\\\")\\n-            initial_error_logged = True # Flag that we logged the initial error\\n-            self.stats[\\\"errors\\\"] += 1 # Log the initial error tentatively\\n-            \\n-            fallback_success = False\\n-            if action == \\\"directory\\\": # Only attempt fallback for directories\\n-                try:\\n-                    # Use native Windows command as fallback\\n-                    import subprocess\\n-                    # Use rmdir /s /q for forceful recursive directory removal\\n-                    # Using shell=True can be a security risk if item_path is not properly controlled/sanitized.\\n-                    # In this context, item_path comes from os.walk which is generally safe.\\n-                    logger.info(f\\\"Attempting fallback: rmdir /s /q \\\\\\\"{item_path}\\\\\\\"\\\")\\n-                    result = subprocess.run(['rmdir', '/s', '/q', item_path], \\n-                                            shell=True, # Necessary for rmdir on Windows cmd\\n-                                            check=True, # Raise CalledProcessError on failure\\n-                                            capture_output=True, # Capture stdout/stderr\\n-                                            text=True, # Decode output as text\\n-                                            encoding='utf-8', # Specify encoding explicitly\\n-                                            errors='replace') # Handle potential decoding errors\\n-                    logger.info(f\\\"Fallback removal successful for directory: {item_path}\\\")\\n-                    # If successful, we assume the space is now freed.\\n-                    # We already captured item_size earlier.\\n-                    self.stats[\\\"bytes_freed\\\"] += item_size \\n-                    # Adjust stats: decrement the error logged above, increment removal count\\n-                    self.stats[\\\"errors\\\"] -= 1 \\n-                    self.stats[f\\\"{action}s_removed\\\"] += 1\\n-                    fallback_success = True # Mark success\\n-                    return item_size # Return freed size\\n-                \\n-                except subprocess.CalledProcessError as sub_e:\\n-                    # Log stderr which often contains useful info from rmdir\\n-                    logger.error(f\\\"Fallback 'rmdir /s /q' failed for {item_path}. Exit Code: {sub_e.returncode}\\\")\\n-                    if sub_e.stderr:\\n-                        logger.error(f\\\"Stderr: {sub_e.stderr.strip()}\\\")\\n-                    else:\\n-                        logger.error(\\\"(No stderr output from rmdir)\\\")\\n-                except FileNotFoundError:\\n-                    logger.error(f\\\"Fallback command 'rmdir' not found. Ensure it's in the system PATH.\\\")\\n-                except Exception as fallback_exc:\\n-                     logger.error(f\\\"Unexpected error during fallback removal of {item_path}: {fallback_exc}\\\")\\n-\\n-            # If fallback wasn't attempted (not a directory) or failed:\\n-            if not fallback_success:\\n-                logger.error(f\\\"Could not remove {action} {item_path} even with fallback (if attempted).\\\")\\n-                if \\\".git\\\" in item_path:\\n-                     logger.warning(f\\\"  Final skip occurred on a path containing '.git'. Manual review likely required.\\\")\\n-            else:\\n-                # If fallback succeeded, ensure we don't double-log the error initially counted\\n-                pass \\n-\\n-            # Log traceback if verbose for the *original* PermissionError\\n-            if self.verbose:\\n-                import traceback\\n-                logger.debug(f\\\"Traceback for initial PermissionError '{e}':\\\\n{traceback.format_exc()}\\\")\\n-\\n-            # Return 0 bytes freed as removal ultimately failed (or wasn't applicable)\\n-            return 0\\n-            \\n-        except Exception as e:\\n-            logger.error(f\\\"General Error removing {action} {item_path}: {e}\\\")\\n-            self.stats[\\\"errors\\\"] += 1\\n-            if self.verbose:\\n-                import traceback\\n-                logger.debug(f\\\"Traceback:\\\\n{traceback.format_exc()}\\\")\\n-            return 0\\n-    \\n-    def _generate_html_report(self, title: str, message: str) -> str:\\n-        \\\"\\\"\\\"\\n-        Generates an HTML report for the backup cleanup operation.\\n-        \\n-        Args:\\n-            title: The title of the report\\n-            message: The main message/summary of the cleanup operation\\n-            \\n-        Returns:\\n-            str: Path to the generated HTML report file\\n-        \\\"\\\"\\\"\\n-        # Create reports directory if it doesn't exist\\n-        reports_dir = os.path.join(\\\"reports\\\", \\\"cleanup\\\")\\n-        os.makedirs(reports_dir, exist_ok=True)\\n-        \\n-        # Generate a timestamp for the report filename\\n-        timestamp = datetime.datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\\n-        report_path = os.path.join(reports_dir, f\\\"backup_cleanup_report_{timestamp}.html\\\")\\n-        \\n-        # Format bytes freed in human-readable format\\n-        bytes_freed = self._format_size(self.stats[\\\"bytes_freed\\\"])\\n-        \\n-        # Create HTML content\\n-        html_content = f\\\"\\\"\\\"<!DOCTYPE html>\\n+\\n+def __init__(\\n+self,\\n+backup_dir: str,\\n+exclusion_patterns: Optional[List[str]] = None,\\n+dry_run: bool = True,\\n+verbose: bool = False,\\n+log_file: Optional[str] = None,\\n+max_backups: int = 5\\n+):\\n+\\\"\\\"\\\"\\n+Initialize the backup cleaner.\\n+\\n+Args:\\n+backup_dir: Directory containing backups to clean\\n+exclusion_patterns: List of patterns to exclude (will be removed from backups)\\n+dry_run: If True, show what would be done without actually doing it\\n+verbose: If True, show detailed logs\\n+log_file: Optional file to log operations\\n+max_backups: Maximum number of backups to keep (oldest will be removed)\\n+\\\"\\\"\\\"\\n+self.backup_dir = backup_dir\\n+self.dry_run = dry_run\\n+self.verbose = verbose\\n+self.max_backups = max_backups\\n+\\n+# Default exclusion patterns for unnecessary files\\n+default_exclusions = [\\n+\\\"node_modules\\\",\\n+\\\"venv\\\",\\n+\\\".venv\\\",\\n+\\\"env\\\",\\n+\\\".env\\\",\\n+\\\"__pycache__\\\",\\n+\\\".git\\\",\\n+\\\".pytest_cache\\\",\\n+\\\".mypy_cache\\\",\\n+\\\"dist\\\",\\n+\\\"build\\\",\\n+\\\"*.egg-info\\\",\\n+\\\".tox\\\",\\n+\\\".coverage\\\",\\n+\\\"htmlcov\\\",\\n+\\\"*.pyc\\\"\\n+]\\n+\\n+self.exclusion_patterns = exclusion_patterns or default_exclusions\\n+\\n+# Setup file logging if requested\\n+if log_file:\\n+file_handler = logging.FileHandler(log_file)\\n+file_handler.setFormatter(log_formatter)\\n+logger.addHandler(file_handler)\\n+\\n+# Set logging level\\n+if verbose:\\n+logger.setLevel(logging.DEBUG)\\n+\\n+# Prepare stats\\n+self.stats = {\\n+\\\"backups_processed\\\": 0,\\n+\\\"backups_removed\\\": 0,\\n+\\\"directories_cleaned\\\": 0,\\n+\\\"files_removed\\\": 0,\\n+\\\"bytes_freed\\\": 0,\\n+\\\"errors\\\": 0\\n+}\\n+self.total_removed_size = 0\\n+\\n+logger.info(f\\\"Initializing backup cleaner for {self.backup_dir}\\\")\\n+logger.info(f\\\"Dry Run: {self.dry_run}\\\")\\n+logger.info(f\\\"Exclusion patterns: {', '.join(self.exclusion_patterns)}\\\")\\n+\\n+def _find_backups(self) -> List[Tuple[str, datetime.datetime]]:\\n+\\\"\\\"\\\"\\n+Find all backups in the backup directory.\\n+\\n+Returns:\\n+List of tuples (backup_path, creation_time)\\n+\\\"\\\"\\\"\\n+backups = []\\n+\\n+if not os.path.exists(self.backup_dir):\\n+logger.warning(f\\\"Backup directory does not exist: {self.backup_dir}\\\")\\n+return []\\n+\\n+# Find zip backups\\n+for item in os.listdir(self.backup_dir):\\n+item_path = os.path.join(self.backup_dir, item)\\n+\\n+if os.path.isfile(item_path) and item.endswith('.zip') and 'backup' in item.lower():\\n+try:\\n+creation_time = datetime.datetime.fromtimestamp(os.path.getctime(item_path))\\n+backups.append((item_path, creation_time))\\n+logger.debug(f\\\"Found backup zip: {item_path} (created {creation_time})\\\")\\n+except Exception as e:\\n+logger.warning(f\\\"Error processing backup {item_path}: {e}\\\")\\n+\\n+# Also check for uncompressed backup directories\\n+elif os.path.isdir(item_path) and 'backup' in item.lower():\\n+try:\\n+creation_time = datetime.datetime.fromtimestamp(os.path.getctime(item_path))\\n+backups.append((item_path, creation_time))\\n+logger.debug(f\\\"Found backup directory: {item_path} (created {creation_time})\\\")\\n+except Exception as e:\\n+logger.warning(f\\\"Error processing backup directory {item_path}: {e}\\\")\\n+\\n+# Sort by creation time (newest first)\\n+backups.sort(key=lambda x: x[1], reverse=True)\\n+\\n+return backups\\n+\\n+def _should_exclude(self, path: str) -> bool:\\n+\\\"\\\"\\\"Check if a path matches any exclusion pattern.\\\"\\\"\\\"\\n+normalized_path = path.replace(\\\"\\\\\\\\\\\", \\\"/\\\")\\n+\\n+for pattern in self.exclusion_patterns:\\n+# Handle wildcard patterns\\n+if \\\"*\\\" in pattern:\\n+regex_pattern = pattern.replace(\\\".\\\", \\\"\\\\\\\\.\\\").replace(\\\"*\\\", \\\".*\\\")\\n+if re.search(regex_pattern, normalized_path, re.IGNORECASE):\\n+return True\\n+# Handle exact matches\\n+elif pattern.lower() in normalized_path.lower():\\n+path_parts = normalized_path.lower().split('/')\\n+if pattern.lower() in path_parts:\\n+return True\\n+\\n+return False\\n+\\n+def _clean_backup_directory(self, backup_path: str) -> int:\\n+\\\"\\\"\\\"\\n+Clean unnecessary files from a backup directory.\\n+\\n+Args:\\n+backup_path: Path to the backup directory\\n+\\n+Returns:\\n+Total bytes freed\\n+\\\"\\\"\\\"\\n+if not os.path.isdir(backup_path):\\n+logger.warning(f\\\"Not a directory: {backup_path}\\\")\\n+return 0\\n+\\n+total_bytes_freed = 0\\n+dirs_removed = 0\\n+files_removed = 0\\n+\\n+# Count items for progress tracking\\n+dirs_to_scan = [backup_path]\\n+total_items = sum(len(os.listdir(dir_path)) for dir_path in dirs_to_scan\\n+if os.path.isdir(dir_path))\\n+\\n+for root, dirs, files in os.walk(backup_path, topdown=True):\\n+# Check if current directory should be excluded\\n+dir_name = os.path.basename(root)\\n+if self._should_exclude(dir_name):\\n+size = self._get_dir_size(root)\\n+logger.info(f\\\"{'[DRY RUN] Would remove' if self.dry_run else 'Removing'} directory: {root} ({self._format_size(size)})\\\")\\n+\\n+if not self.dry_run:\\n+try:\\n+shutil.rmtree(root)\\n+total_bytes_freed += size\\n+dirs_removed += 1\\n+self.stats[\\\"directories_cleaned\\\"] += 1\\n+self.stats[\\\"bytes_freed\\\"] += size\\n+self.total_removed_size += size\\n+except Exception as e:\\n+logger.error(f\\\"Error removing directory {root}: {e}\\\")\\n+self.stats[\\\"errors\\\"] += 1\\n+\\n+# Don't process subdirectories\\n+dirs[:] = []\\n+continue\\n+\\n+# Process files\\n+for file in files:\\n+file_path = os.path.join(root, file)\\n+\\n+if self._should_exclude(file):\\n+try:\\n+size = os.path.getsize(file_path)\\n+logger.debug(f\\\"{'[DRY RUN] Would remove' if self.dry_run else 'Removing'} file: {file_path} ({self._format_size(size)})\\\")\\n+\\n+if not self.dry_run:\\n+os.remove(file_path)\\n+total_bytes_freed += size\\n+files_removed += 1\\n+self.stats[\\\"files_removed\\\"] += 1\\n+self.stats[\\\"bytes_freed\\\"] += size\\n+self.total_removed_size += size\\n+except Exception as e:\\n+logger.error(f\\\"Error removing file {file_path}: {e}\\\")\\n+self.stats[\\\"errors\\\"] += 1\\n+\\n+logger.info(f\\\"Cleaned backup: {backup_path} - Removed {dirs_removed} directories and {files_removed} files, freed {self._format_size(total_bytes_freed)}\\\")\\n+return total_bytes_freed\\n+\\n+def _extract_and_clean_zip(self, zip_path: str) -> int:\\n+\\\"\\\"\\\"\\n+Extract a zip backup, clean it, and recompress it.\\n+\\n+Args:\\n+zip_path: Path to the zip backup\\n+\\n+Returns:\\n+Total bytes freed\\n+\\\"\\\"\\\"\\n+if not os.path.isfile(zip_path) or not zip_path.endswith('.zip'):\\n+logger.warning(f\\\"Not a zip file: {zip_path}\\\")\\n+return 0\\n+\\n+# Get original size\\n+original_size = os.path.getsize(zip_path)\\n+\\n+# Create a temporary extraction directory\\n+temp_dir = os.path.join(self.backup_dir, f\\\"temp_extract_{os.path.basename(zip_path).replace('.zip', '')}\\\")\\n+if os.path.exists(temp_dir):\\n+shutil.rmtree(temp_dir)\\n+os.makedirs(temp_dir)\\n+\\n+try:\\n+# Extract the zip\\n+logger.info(f\\\"Extracting {zip_path} to {temp_dir}\\\")\\n+with zipfile.ZipFile(zip_path, 'r') as zip_ref:\\n+zip_ref.extractall(temp_dir)\\n+\\n+# Clean the extracted directory\\n+bytes_freed = self._clean_backup_directory(temp_dir)\\n+\\n+if not self.dry_run:\\n+# Create a new zip file\\n+new_zip_path = zip_path + \\\".new\\\"\\n+logger.info(f\\\"Creating new zip file: {new_zip_path}\\\")\\n+\\n+with zipfile.ZipFile(new_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\\n+for root, dirs, files in os.walk(temp_dir):\\n+for file in files:\\n+try:\\n+file_path = os.path.join(root, file)\\n+arcname = os.path.relpath(file_path, temp_dir)\\n+zipf.write(file_path, arcname)\\n+except Exception as e:\\n+logger.error(f\\\"Error adding {file_path} to zip: {e}\\\")\\n+self.stats[\\\"errors\\\"] += 1\\n+\\n+# Replace the original zip with the new one\\n+new_size = os.path.getsize(new_zip_path)\\n+bytes_freed = original_size - new_size\\n+\\n+os.remove(zip_path)\\n+os.rename(new_zip_path, zip_path)\\n+logger.info(f\\\"Replaced {zip_path} - Old size: {self._format_size(original_size)}, New size: {self._format_size(new_size)}\\\")\\n+\\n+return bytes_freed\\n+\\n+except Exception as e:\\n+logger.error(f\\\"Error processing zip {zip_path}: {e}\\\")\\n+self.stats[\\\"errors\\\"] += 1\\n+return 0\\n+\\n+finally:\\n+# Clean up the temporary directory\\n+if os.path.exists(temp_dir) and not self.dry_run:\\n+shutil.rmtree(temp_dir)\\n+\\n+def _remove_old_backups(self, backups: List[Tuple[str, datetime.datetime]]) -> int:\\n+\\\"\\\"\\\"\\n+Remove backups exceeding the maximum number to keep.\\n+\\n+Args:\\n+backups: List of backups sorted by creation time (newest first)\\n+\\n+Returns:\\n+Total bytes freed\\n+\\\"\\\"\\\"\\n+if len(backups) <= self.max_backups:\\n+logger.info(f\\\"Only {len(backups)} backups found, not exceeding limit of {self.max_backups}\\\")\\n+return 0\\n+\\n+total_bytes_freed = 0\\n+backups_to_remove = backups[self.max_backups:]\\n+\\n+for backup_path, creation_time in backups_to_remove:\\n+try:\\n+size = os.path.getsize(backup_path) if os.path.isfile(backup_path) else self._get_dir_size(backup_path)\\n+logger.info(f\\\"{'[DRY RUN] Would remove' if self.dry_run else 'Removing'} old backup: {backup_path} ({self._format_size(size)}, created {creation_time})\\\")\\n+\\n+if not self.dry_run:\\n+if os.path.isfile(backup_path):\\n+os.remove(backup_path)\\n+else:\\n+shutil.rmtree(backup_path)\\n+\\n+total_bytes_freed += size\\n+self.stats[\\\"backups_removed\\\"] += 1\\n+self.stats[\\\"bytes_freed\\\"] += size\\n+self.total_removed_size += size\\n+except Exception as e:\\n+logger.error(f\\\"Error removing backup {backup_path}: {e}\\\")\\n+self.stats[\\\"errors\\\"] += 1\\n+\\n+return total_bytes_freed\\n+\\n+def _get_dir_size(self, path: str) -> int:\\n+\\\"\\\"\\\"Get the total size of a directory in bytes.\\\"\\\"\\\"\\n+total_size = 0\\n+for dirpath, dirnames, filenames in os.walk(path):\\n+for f in filenames:\\n+fp = os.path.join(dirpath, f)\\n+if not os.path.islink(fp):\\n+total_size += os.path.getsize(fp)\\n+return total_size\\n+\\n+def _format_size(self, size_bytes: int) -> str:\\n+\\\"\\\"\\\"Format size in bytes to human-readable format.\\\"\\\"\\\"\\n+for unit in ['B', 'KB', 'MB', 'GB']:\\n+if size_bytes < 1024.0:\\n+return f\\\"{size_bytes:.1f} {unit}\\\"\\n+size_bytes /= 1024.0\\n+return f\\\"{size_bytes:.1f} TB\\\"\\n+\\n+def _remove_item(self, item_path: str) -> int:\\n+\\\"\\\"\\\"Remove a file or directory, handling potential errors.\\\"\\\"\\\"\\n+item_size = self._get_item_size(item_path)\\n+action = \\\"directory\\\" if os.path.isdir(item_path) else \\\"file\\\"\\n+\\n+logger.debug(f\\\"Attempting to remove {action}: {item_path} ({self._format_size(item_size)})\\\")\\n+\\n+if self.dry_run:\\n+logger.info(f\\\"[Dry Run] Would remove {action}: {item_path} ({self._format_size(item_size)})\\\")\\n+return item_size  # Simulate freeing space in dry run\\n+\\n+try:\\n+if os.path.isdir(item_path):\\n+# Use onerror handler for rmtree to deal with read-only files\\n+def remove_readonly(func, path, exc_info):\\n+\\\"\\\"\\\"Error handler for shutil.rmtree that tries to remove read-only files.\\\"\\\"\\\"\\n+# Check if the error is PermissionError\\n+exc_type, exc_value, tb = exc_info\\n+if exc_value and isinstance(exc_value, PermissionError):\\n+logger.warning(f\\\"PermissionError accessing {path}. Attempting to change permissions.\\\")\\n+try:\\n+os.chmod(path, 0o777) # Set permissions to read/write/execute for all\\n+func(path) # Retry the operation\\n+except Exception as retry_exc:\\n+logger.error(f\\\"Failed to remove {path} even after chmod: {retry_exc}\\\")\\n+# Re-raise the original exception if chmod/retry fails\\n+raise exc_value\\n+else:\\n+# If it's not a PermissionError, raise it to stop the process\\n+# or handle other specific errors as needed.\\n+raise exc_value # Re-raise the original error\\n+\\n+shutil.rmtree(item_path, onerror=remove_readonly)\\n+self.stats[f\\\"{action}s_removed\\\"] += 1\\n+logger.info(f\\\"Removed {action}: {item_path}\\\")\\n+self.total_removed_size += item_size\\n+elif os.path.isfile(item_path):\\n+try:\\n+os.remove(item_path)\\n+except PermissionError:\\n+logger.warning(f\\\"PermissionError removing file {item_path}. Attempting to change permissions.\\\")\\n+os.chmod(item_path, 0o777)\\n+os.remove(item_path) # Retry removal\\n+self.stats[f\\\"{action}s_removed\\\"] += 1\\n+logger.info(f\\\"Removed {action}: {item_path}\\\")\\n+self.total_removed_size += item_size\\n+else:\\n+logger.warning(f\\\"Item not found or not a file/directory: {item_path}\\\")\\n+return 0 # Not found, no space freed\\n+\\n+self.stats[\\\"bytes_freed\\\"] += item_size\\n+return item_size\\n+\\n+except PermissionError as e:\\n+# This block might catch errors not handled by onerror or file removal retry\\n+logger.error(f\\\"Final PermissionError removing {action} {item_path}: {e}. Attempting fallback...\\\")\\n+initial_error_logged = True # Flag that we logged the initial error\\n+self.stats[\\\"errors\\\"] += 1 # Log the initial error tentatively\\n+\\n+fallback_success = False\\n+if action == \\\"directory\\\": # Only attempt fallback for directories\\n+try:\\n+# Use native Windows command as fallback\\n+import subprocess\\n+# Use rmdir /s /q for forceful recursive directory removal\\n+# Using shell=True can be a security risk if item_path is not properly controlled/sanitized.\\n+# In this context, item_path comes from os.walk which is generally safe.\\n+logger.info(f\\\"Attempting fallback: rmdir /s /q \\\\\\\"{item_path}\\\\\\\"\\\")\\n+result = subprocess.run(['rmdir', '/s', '/q', item_path],\\n+shell=True, # Necessary for rmdir on Windows cmd\\n+check=True, # Raise CalledProcessError on failure\\n+capture_output=True, # Capture stdout/stderr\\n+text=True, # Decode output as text\\n+encoding='utf-8', # Specify encoding explicitly\\n+errors='replace') # Handle potential decoding errors\\n+logger.info(f\\\"Fallback removal successful for directory: {item_path}\\\")\\n+# If successful, we assume the space is now freed.\\n+# We already captured item_size earlier.\\n+self.stats[\\\"bytes_freed\\\"] += item_size\\n+# Adjust stats: decrement the error logged above, increment removal count\\n+self.stats[\\\"errors\\\"] -= 1\\n+self.stats[f\\\"{action}s_removed\\\"] += 1\\n+fallback_success = True # Mark success\\n+return item_size # Return freed size\\n+\\n+except subprocess.CalledProcessError as sub_e:\\n+# Log stderr which often contains useful info from rmdir\\n+logger.error(f\\\"Fallback 'rmdir /s /q' failed for {item_path}. Exit Code: {sub_e.returncode}\\\")\\n+if sub_e.stderr:\\n+logger.error(f\\\"Stderr: {sub_e.stderr.strip()}\\\")\\n+else:\\n+logger.error(\\\"(No stderr output from rmdir)\\\")\\n+except FileNotFoundError:\\n+logger.error(f\\\"Fallback command 'rmdir' not found. Ensure it's in the system PATH.\\\")\\n+except Exception as fallback_exc:\\n+logger.error(f\\\"Unexpected error during fallback removal of {item_path}: {fallback_exc}\\\")\\n+\\n+# If fallback wasn't attempted (not a directory) or failed:\\n+if not fallback_success:\\n+logger.error(f\\\"Could not remove {action} {item_path} even with fallback (if attempted).\\\")\\n+if \\\".git\\\" in item_path:\\n+logger.warning(f\\\"  Final skip occurred on a path containing '.git'. Manual review likely required.\\\")\\n+else:\\n+# If fallback succeeded, ensure we don't double-log the error initially counted\\n+pass\\n+\\n+# Log traceback if verbose for the *original* PermissionError\\n+if self.verbose:\\n+import traceback\\n+logger.debug(f\\\"Traceback for initial PermissionError '{e}':\\\\n{traceback.format_exc()}\\\")\\n+\\n+# Return 0 bytes freed as removal ultimately failed (or wasn't applicable)\\n+return 0\\n+\\n+except Exception as e:\\n+logger.error(f\\\"General Error removing {action} {item_path}: {e}\\\")\\n+self.stats[\\\"errors\\\"] += 1\\n+if self.verbose:\\n+import traceback\\n+logger.debug(f\\\"Traceback:\\\\n{traceback.format_exc()}\\\")\\n+return 0\\n+\\n+def _generate_html_report(self, title: str, message: str) -> str:\\n+\\\"\\\"\\\"\\n+Generates an HTML report for the backup cleanup operation.\\n+\\n+Args:\\n+title: The title of the report\\n+message: The main message/summary of the cleanup operation\\n+\\n+Returns:\\n+str: Path to the generated HTML report file\\n+\\\"\\\"\\\"\\n+# Create reports directory if it doesn't exist\\n+reports_dir = os.path.join(\\\"reports\\\", \\\"cleanup\\\")\\n+os.makedirs(reports_dir, exist_ok=True)\\n+\\n+# Generate a timestamp for the report filename\\n+timestamp = datetime.datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\\n+report_path = os.path.join(reports_dir, f\\\"backup_cleanup_report_{timestamp}.html\\\")\\n+\\n+# Format bytes freed in human-readable format\\n+bytes_freed = self._format_size(self.stats[\\\"bytes_freed\\\"])\\n+\\n+# Create HTML content\\n+html_content = f\\\"\\\"\\\"<!DOCTYPE html>\\n <html lang=\\\"en\\\">\\n <head>\\n-    <meta charset=\\\"UTF-8\\\">\\n-    <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\">\\n-    <title>{title}</title>\\n-    <style>\\n-        body {{\\n-            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\\n-            line-height: 1.6;\\n-            color: #333;\\n-            max-width: 800px;\\n-            margin: 0 auto;\\n-            padding: 20px;\\n-            background-color: #f5f5f5;\\n-        }}\\n-        .header {{\\n-            background-color: #4a86e8;\\n-            color: white;\\n-            padding: 15px 20px;\\n-            border-radius: 5px 5px 0 0;\\n-            margin-bottom: 0;\\n-        }}\\n-        .content {{\\n-            background-color: white;\\n-            padding: 20px;\\n-            border-radius: 0 0 5px 5px;\\n-            box-shadow: 0 2px 5px rgba(0,0,0,0.1);\\n-            margin-top: 0;\\n-        }}\\n-        .summary {{\\n-            font-size: 1.1em;\\n-            margin-bottom: 20px;\\n-            padding-bottom: 15px;\\n-            border-bottom: 1px solid #eee;\\n-        }}\\n-        table {{\\n-            width: 100%;\\n-            border-collapse: collapse;\\n-            margin: 20px 0;\\n-        }}\\n-        th, td {{\\n-            padding: 12px 15px;\\n-            border-bottom: 1px solid #ddd;\\n-            text-align: left;\\n-        }}\\n-        th {{\\n-            background-color: #f8f8f8;\\n-        }}\\n-        .footer {{\\n-            margin-top: 30px;\\n-            font-size: 0.9em;\\n-            color: #777;\\n-            text-align: center;\\n-        }}\\n-        .success {{\\n-            color: #28a745;\\n-        }}\\n-        .warning {{\\n-            color: #ffc107;\\n-        }}\\n-        .error {{\\n-            color: #dc3545;\\n-        }}\\n-    </style>\\n+<meta charset=\\\"UTF-8\\\">\\n+<meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\">\\n+<title>{title}</title>\\n+<style>\\n+body {{\\n+font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\\n+line-height: 1.6;\\n+color: #333;\\n+max-width: 800px;\\n+margin: 0 auto;\\n+padding: 20px;\\n+background-color: #f5f5f5;\\n+}}\\n+.header {{\\n+background-color: #4a86e8;\\n+color: white;\\n+padding: 15px 20px;\\n+border-radius: 5px 5px 0 0;\\n+margin-bottom: 0;\\n+}}\\n+.content {{\\n+background-color: white;\\n+padding: 20px;\\n+border-radius: 0 0 5px 5px;\\n+box-shadow: 0 2px 5px rgba(0,0,0,0.1);\\n+margin-top: 0;\\n+}}\\n+.summary {{\\n+font-size: 1.1em;\\n+margin-bottom: 20px;\\n+padding-bottom: 15px;\\n+border-bottom: 1px solid #eee;\\n+}}\\n+table {{\\n+width: 100%;\\n+border-collapse: collapse;\\n+margin: 20px 0;\\n+}}\\n+th, td {{\\n+padding: 12px 15px;\\n+border-bottom: 1px solid #ddd;\\n+text-align: left;\\n+}}\\n+th {{\\n+background-color: #f8f8f8;\\n+}}\\n+.footer {{\\n+margin-top: 30px;\\n+font-size: 0.9em;\\n+color: #777;\\n+text-align: center;\\n+}}\\n+.success {{\\n+color: #28a745;\\n+}}\\n+.warning {{\\n+color: #ffc107;\\n+}}\\n+.error {{\\n+color: #dc3545;\\n+}}\\n+</style>\\n </head>\\n <body>\\n-    <div class=\\\"header\\\">\\n-        <h1>{title}</h1>\\n-    </div>\\n-    <div class=\\\"content\\\">\\n-        <div class=\\\"summary\\\">\\n-            <p>{message}</p>\\n-        </div>\\n-        \\n-        <h2>Cleanup Details</h2>\\n-        <table>\\n-            <tr>\\n-                <th>Metric</th>\\n-                <th>Value</th>\\n-            </tr>\\n-            <tr>\\n-                <td>Backups Processed</td>\\n-                <td>{self.stats[\\\"backups_processed\\\"]}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Backups Removed</td>\\n-                <td>{self.stats[\\\"backups_removed\\\"]}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Directories Cleaned</td>\\n-                <td>{self.stats[\\\"directories_cleaned\\\"]}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Files Removed</td>\\n-                <td>{self.stats[\\\"files_removed\\\"]}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Space Freed</td>\\n-                <td>{bytes_freed}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Errors</td>\\n-                <td class=\\\"{(\\\"success\\\" if self.stats[\\\"errors\\\"] == 0 else \\\"error\\\")}\\\">{self.stats[\\\"errors\\\"]}</td>\\n-            </tr>\\n-        </table>\\n-        \\n-        <h2>Operation Information</h2>\\n-        <table>\\n-            <tr>\\n-                <th>Property</th>\\n-                <th>Value</th>\\n-            </tr>\\n-            <tr>\\n-                <td>Backup Directory</td>\\n-                <td>{self.backup_dir}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Dry Run</td>\\n-                <td>{str(self.dry_run)}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Max Backups Retained</td>\\n-                <td>{self.max_backups}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Timestamp</td>\\n-                <td>{datetime.datetime.now().strftime(\\\"%Y-%m-%d %H:%M:%S\\\")}</td>\\n-            </tr>\\n-        </table>\\n-        \\n-        <div class=\\\"footer\\\">\\n-            <p>Generated by EGOS Backup Cleanup Utility</p>\\n-        </div>\\n-    </div>\\n+<div class=\\\"header\\\">\\n+<h1>{title}</h1>\\n+</div>\\n+<div class=\\\"content\\\">\\n+<div class=\\\"summary\\\">\\n+<p>{message}</p>\\n+</div>\\n+\\n+<h2>Cleanup Details</h2>\\n+<table>\\n+<tr>\\n+<th>Metric</th>\\n+<th>Value</th>\\n+</tr>\\n+<tr>\\n+<td>Backups Processed</td>\\n+<td>{self.stats[\\\"backups_processed\\\"]}</td>\\n+</tr>\\n+<tr>\\n+<td>Backups Removed</td>\\n+<td>{self.stats[\\\"backups_removed\\\"]}</td>\\n+</tr>\\n+<tr>\\n+<td>Directories Cleaned</td>\\n+<td>{self.stats[\\\"directories_cleaned\\\"]}</td>\\n+</tr>\\n+<tr>\\n+<td>Files Removed</td>\\n+<td>{self.stats[\\\"files_removed\\\"]}</td>\\n+</tr>\\n+<tr>\\n+<td>Space Freed</td>\\n+<td>{bytes_freed}</td>\\n+</tr>\\n+<tr>\\n+<td>Errors</td>\\n+<td class=\\\"{(\\\"success\\\" if self.stats[\\\"errors\\\"] == 0 else \\\"error\\\")}\\\">{self.stats[\\\"errors\\\"]}</td>\\n+</tr>\\n+</table>\\n+\\n+<h2>Operation Information</h2>\\n+<table>\\n+<tr>\\n+<th>Property</th>\\n+<th>Value</th>\\n+</tr>\\n+<tr>\\n+<td>Backup Directory</td>\\n+<td>{self.backup_dir}</td>\\n+</tr>\\n+<tr>\\n+<td>Dry Run</td>\\n+<td>{str(self.dry_run)}</td>\\n+</tr>\\n+<tr>\\n+<td>Max Backups Retained</td>\\n+<td>{self.max_backups}</td>\\n+</tr>\\n+<tr>\\n+<td>Timestamp</td>\\n+<td>{datetime.datetime.now().strftime(\\\"%Y-%m-%d %H:%M:%S\\\")}</td>\\n+</tr>\\n+</table>\\n+\\n+<div class=\\\"footer\\\">\\n+<p>Generated by EGOS Backup Cleanup Utility</p>\\n+</div>\\n+</div>\\n </body>\\n </html>\\\"\\\"\\\"\\n-        \\n-        # Write HTML content to file\\n-        with open(report_path, 'w', encoding='utf-8') as f:\\n-            f.write(html_content)\\n-        \\n-        logger.info(f\\\"Generated HTML report at {report_path}\\\")\\n-        return report_path\\n-    \\n-    def _show_notification(self, title: str, message: str):\\n-        \\\"\\\"\\\"\\n-        Shows a notification about the backup cleanup operation.\\n-        Generates an HTML report and opens it in the default browser.\\n-        \\n-        Args:\\n-            title: The title of the notification\\n-            message: The message to display\\n-        \\\"\\\"\\\"\\n-        try:\\n-            # Generate HTML report\\n-            report_path = self._generate_html_report(title, message)\\n-            \\n-            # Open the report in the default browser\\n-            report_url = f\\\"file:///{report_path.replace(os.sep, '/')}\\\"\\n-            logger.info(f\\\"Opening report in browser: {report_url}\\\")\\n-            \\n-            # Use appropriate command based on platform\\n-            if sys.platform == 'win32':\\n-                os.startfile(report_path)\\n-            elif sys.platform == 'darwin':  # macOS\\n-                subprocess.run(['open', report_path], check=False)\\n-            else:  # Linux and others\\n-                subprocess.run(['xdg-open', report_path], check=False)\\n-                \\n-            logger.info(\\\"Opened cleanup report in default browser\\\")\\n-        except Exception as e:\\n-            logger.error(f\\\"Error showing notification: {e}\\\")\\n-    \\n-    def execute_cleanup(self) -> Dict:\\n-        \\\"\\\"\\\"\\n-        Execute the backup cleanup process.\\n-        \\n-        Returns:\\n-            dict: Cleanup statistics\\n-        \\\"\\\"\\\"\\n-        print(box_message(\\\"EGOS Backup Cleanup\\\", [\\n-            f\\\"Target Directory: {self.backup_dir}\\\",\\n-            f\\\"Dry Run: {self.dry_run}\\\",\\n-            f\\\"Max Backups: {self.max_backups}\\\",\\n-            f\\\"Started: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\"\\n-        ]))\\n-        print()\\n-        \\n-        # Find all backups\\n-        with spinner(\\\"Scanning for backups\\\") as spin:\\n-            backups = self._find_backups()\\n-            spin.finish(f\\\"Found {len(backups)} backups\\\")\\n-        \\n-        print(section_header(\\\"Processing Backups\\\"))\\n-        \\n-        # Process each backup\\n-        for idx, (backup_path, creation_time) in enumerate(backups):\\n-            backup_name = os.path.basename(backup_path)\\n-            creation_str = creation_time.strftime(\\\"%Y-%m-%d %H:%M\\\")\\n-            \\n-            print(f\\\"\\\\n{idx+1}/{len(backups)} \\u2022 {backup_name} (created {creation_str})\\\")\\n-            self.stats[\\\"backups_processed\\\"] += 1\\n-            \\n-            if os.path.isfile(backup_path) and backup_path.endswith('.zip'):\\n-                with spinner(f\\\"Processing zip backup\\\") as spin:\\n-                    try:\\n-                        freed = self._extract_and_clean_zip(backup_path)\\n-                        spin.finish(f\\\"Processed zip backup \\u2022 {self._format_size(freed)} freed\\\")\\n-                    except Exception as e:\\n-                        spin.finish(f\\\"Error processing zip backup\\\")\\n-                        logger.error(f\\\"Error details: {e}\\\")\\n-                        self.stats[\\\"errors\\\"] += 1\\n-            elif os.path.isdir(backup_path):\\n-                # For directories, we can show more granular progress\\n-                logger.info(f\\\"Cleaning directory backup: {backup_path}\\\")\\n-                freed = self._clean_backup_directory(backup_path)\\n-                logger.info(f\\\"Cleaned directory \\u2022 {self._format_size(freed)} freed\\\")\\n-        \\n-        # Remove old backups if exceeding maximum\\n-        if len(backups) > self.max_backups:\\n-            print(\\\"\\\\n\\\" + section_header(\\\"Removing Old Backups\\\"))\\n-            with spinner(f\\\"Removing {len(backups) - self.max_backups} old backups\\\") as spin:\\n-                freed = self._remove_old_backups(backups)\\n-                spin.finish(f\\\"Removed old backups \\u2022 {self._format_size(freed)} freed\\\")\\n-        \\n-        # Show final results\\n-        print(\\\"\\\\n\\\" + section_header(\\\"Cleanup Results\\\"))\\n-        \\n-        results = [\\n-            f\\\"Backups Processed: {self.stats['backups_processed']}\\\",\\n-            f\\\"Backups Removed: {self.stats['backups_removed']}\\\",\\n-            f\\\"Directories Cleaned: {self.stats['directories_cleaned']}\\\",\\n-            f\\\"Files Removed: {self.stats['files_removed']}\\\",\\n-            f\\\"Total Space Freed: {self._format_size(self.stats['bytes_freed'])}\\\",\\n-            f\\\"Errors: {self.stats['errors']}\\\"\\n-        ]\\n-        \\n-        for result in results:\\n-            logger.info(result)\\n-        \\n-        print(\\\"\\\\n\\\" + box_message(\\\"Cleanup Completed\\\", \\n-            f\\\"Total Space {'Would be Freed' if self.dry_run else 'Freed'}: {self._format_size(self.stats['bytes_freed'])}\\\"))\\n-        \\n-        # Send notification\\n-        error_count = self.stats[\\\"errors\\\"]\\n-        title = \\\"EGOS Backup Cleanup Complete\\\"\\n-        size_mb = self.stats[\\\"bytes_freed\\\"] / (1024 * 1024)\\n-        message = f\\\"Removed {self.stats['backups_removed']} backups, {self.stats['directories_cleaned']} directories, and {self.stats['files_removed']} files ({size_mb:.2f} MB). \\\"\\n-        if error_count > 0:\\n-            message += f\\\"{error_count} errors encountered.\\\"\\n-        else:\\n-            message += \\\"No errors.\\\"\\n-        self._show_notification(title, message)\\n-        \\n-        return self.stats\\n+\\n+# Write HTML content to file\\n+with open(report_path, 'w', encoding='utf-8') as f:\\n+f.write(html_content)\\n+\\n+logger.info(f\\\"Generated HTML report at {report_path}\\\")\\n+return report_path\\n+\\n+def _show_notification(self, title: str, message: str):\\n+\\\"\\\"\\\"\\n+Shows a notification about the backup cleanup operation.\\n+Generates an HTML report and opens it in the default browser.\\n+\\n+Args:\\n+title: The title of the notification\\n+message: The message to display\\n+\\\"\\\"\\\"\\n+try:\\n+# Generate HTML report\\n+report_path = self._generate_html_report(title, message)\\n+\\n+# Open the report in the default browser\\n+report_url = f\\\"file:///{report_path.replace(os.sep, '/')}\\\"\\n+logger.info(f\\\"Opening report in browser: {report_url}\\\")\\n+\\n+# Use appropriate command based on platform\\n+if sys.platform == 'win32':\\n+os.startfile(report_path)\\n+elif sys.platform == 'darwin':  # macOS\\n+subprocess.run(['open', report_path], check=False)\\n+else:  # Linux and others\\n+subprocess.run(['xdg-open', report_path], check=False)\\n+\\n+logger.info(\\\"Opened cleanup report in default browser\\\")\\n+except Exception as e:\\n+logger.error(f\\\"Error showing notification: {e}\\\")\\n+\\n+def execute_cleanup(self) -> Dict:\\n+\\\"\\\"\\\"\\n+Execute the backup cleanup process.\\n+\\n+Returns:\\n+dict: Cleanup statistics\\n+\\\"\\\"\\\"\\n+print(box_message(\\\"EGOS Backup Cleanup\\\", [\\n+f\\\"Target Directory: {self.backup_dir}\\\",\\n+f\\\"Dry Run: {self.dry_run}\\\",\\n+f\\\"Max Backups: {self.max_backups}\\\",\\n+f\\\"Started: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\"\\n+]))\\n+print()\\n+\\n+# Find all backups\\n+with spinner(\\\"Scanning for backups\\\") as spin:\\n+backups = self._find_backups()\\n+spin.finish(f\\\"Found {len(backups)} backups\\\")\\n+\\n+print(section_header(\\\"Processing Backups\\\"))\\n+\\n+# Process each backup\\n+for idx, (backup_path, creation_time) in enumerate(backups):\\n+backup_name = os.path.basename(backup_path)\\n+creation_str = creation_time.strftime(\\\"%Y-%m-%d %H:%M\\\")\\n+\\n+print(f\\\"\\\\n{idx+1}/{len(backups)} \\u2022 {backup_name} (created {creation_str})\\\")\\n+self.stats[\\\"backups_processed\\\"] += 1\\n+\\n+if os.path.isfile(backup_path) and backup_path.endswith('.zip'):\\n+with spinner(f\\\"Processing zip backup\\\") as spin:\\n+try:\\n+freed = self._extract_and_clean_zip(backup_path)\\n+spin.finish(f\\\"Processed zip backup \\u2022 {self._format_size(freed)} freed\\\")\\n+except Exception as e:\\n+spin.finish(f\\\"Error processing zip backup\\\")\\n+logger.error(f\\\"Error details: {e}\\\")\\n+self.stats[\\\"errors\\\"] += 1\\n+elif os.path.isdir(backup_path):\\n+# For directories, we can show more granular progress\\n+logger.info(f\\\"Cleaning directory backup: {backup_path}\\\")\\n+freed = self._clean_backup_directory(backup_path)\\n+logger.info(f\\\"Cleaned directory \\u2022 {self._format_size(freed)} freed\\\")\\n+\\n+# Remove old backups if exceeding maximum\\n+if len(backups) > self.max_backups:\\n+print(\\\"\\\\n\\\" + section_header(\\\"Removing Old Backups\\\"))\\n+with spinner(f\\\"Removing {len(backups) - self.max_backups} old backups\\\") as spin:\\n+freed = self._remove_old_backups(backups)\\n+spin.finish(f\\\"Removed old backups \\u2022 {self._format_size(freed)} freed\\\")\\n+\\n+# Show final results\\n+print(\\\"\\\\n\\\" + section_header(\\\"Cleanup Results\\\"))\\n+\\n+results = [\\n+f\\\"Backups Processed: {self.stats['backups_processed']}\\\",\\n+f\\\"Backups Removed: {self.stats['backups_removed']}\\\",\\n+f\\\"Directories Cleaned: {self.stats['directories_cleaned']}\\\",\\n+f\\\"Files Removed: {self.stats['files_removed']}\\\",\\n+f\\\"Total Space Freed: {self._format_size(self.stats['bytes_freed'])}\\\",\\n+f\\\"Errors: {self.stats['errors']}\\\"\\n+]\\n+\\n+for result in results:\\n+logger.info(result)\\n+\\n+print(\\\"\\\\n\\\" + box_message(\\\"Cleanup Completed\\\",\\n+f\\\"Total Space {'Would be Freed' if self.dry_run else 'Freed'}: {self._format_size(self.stats['bytes_freed'])}\\\"))\\n+\\n+# Send notification\\n+error_count = self.stats[\\\"errors\\\"]\\n+title = \\\"EGOS Backup Cleanup Complete\\\"\\n+size_mb = self.stats[\\\"bytes_freed\\\"] / (1024 * 1024)\\n+message = f\\\"Removed {self.stats['backups_removed']} backups, {self.stats['directories_cleaned']} directories, and {self.stats['files_removed']} files ({size_mb:.2f} MB). \\\"\\n+if error_count > 0:\\n+message += f\\\"{error_count} errors encountered.\\\"\\n+else:\\n+message += \\\"No errors.\\\"\\n+self._show_notification(title, message)\\n+\\n+return self.stats\\n \\n \\n def main():\\n-    \\\"\\\"\\\"Main function for command-line execution.\\\"\\\"\\\"\\n-    parser = argparse.ArgumentParser(description=\\\"EGOS Backup Cleanup Script\\\")\\n-    parser.add_argument(\\\"--backup-dir\\\", default=\\\"./backups\\\", help=\\\"Directory containing backups to clean\\\")\\n-    parser.add_argument(\\\"--exclude\\\", nargs=\\\"+\\\", help=\\\"Additional patterns to exclude\\\")\\n-    parser.add_argument(\\\"--max-backups\\\", type=int, default=5, help=\\\"Maximum number of backups to keep\\\")\\n-    parser.add_argument(\\\"--log-file\\\", help=\\\"File to log operations\\\")\\n-    parser.add_argument(\\\"--dry-run\\\", action=\\\"store_true\\\", help=\\\"Show what would be done without actually doing it\\\")\\n-    parser.add_argument(\\\"--verbose\\\", action=\\\"store_true\\\", help=\\\"Show detailed logs\\\")\\n-    parser.add_argument(\\\"--output\\\", help=\\\"Path to save detailed cleanup report JSON\\\")\\n-    \\n-    args = parser.parse_args()\\n-    \\n-    # Configure log file if specified\\n-    if args.log_file:\\n-        file_handler = logging.FileHandler(args.log_file)\\n-        file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\\n-        file_handler.setFormatter(file_formatter)\\n-        logger.addHandler(file_handler)\\n-    \\n-    # Set log level based on verbosity\\n-    if args.verbose:\\n-        logger.setLevel(logging.DEBUG)\\n-    \\n-    try:\\n-        # Create backup cleaner\\n-        cleaner = BackupCleaner(\\n-            backup_dir=args.backup_dir,\\n-            exclusion_patterns=args.exclude,\\n-            dry_run=args.dry_run,\\n-            verbose=args.verbose,\\n-            log_file=args.log_file,\\n-            max_backups=args.max_backups\\n-        )\\n-        \\n-        # Execute cleanup\\n-        stats = cleaner.execute_cleanup()\\n-        \\n-        # Save detailed report if requested\\n-        if args.output:\\n-            report = {\\n-                \\\"timestamp\\\": datetime.datetime.now().isoformat(),\\n-                \\\"configuration\\\": {\\n-                    \\\"backup_dir\\\": args.backup_dir,\\n-                    \\\"max_backups\\\": args.max_backups,\\n-                    \\\"dry_run\\\": args.dry_run\\n-                },\\n-                \\\"results\\\": stats\\n-            }\\n-            \\n-            # Create directory for output file if it doesn't exist\\n-            output_dir = os.path.dirname(args.output)\\n-            if output_dir:\\n-                os.makedirs(output_dir, exist_ok=True)\\n-\\n-            with open(args.output, 'w', encoding='utf-8') as f:\\n-                json.dump(report, f, indent=2)\\n-                \\n-            logger.info(f\\\"Detailed report saved to {args.output}\\\")\\n-    \\n-    except KeyboardInterrupt:\\n-        print(\\\"\\\\n\\\\nOperation cancelled by user.\\\")\\n-        sys.exit(1)\\n-    except Exception as e:\\n-        logger.critical(f\\\"Unexpected error: {e}\\\")\\n-        sys.exit(1)\\n+\\\"\\\"\\\"Main function for command-line execution.\\\"\\\"\\\"\\n+parser = argparse.ArgumentParser(description=\\\"EGOS Backup Cleanup Script\\\")\\n+parser.add_argument(\\\"--backup-dir\\\", default=\\\"./backups\\\", help=\\\"Directory containing backups to clean\\\")\\n+parser.add_argument(\\\"--exclude\\\", nargs=\\\"+\\\", help=\\\"Additional patterns to exclude\\\")\\n+parser.add_argument(\\\"--max-backups\\\", type=int, default=5, help=\\\"Maximum number of backups to keep\\\")\\n+parser.add_argument(\\\"--log-file\\\", help=\\\"File to log operations\\\")\\n+parser.add_argument(\\\"--dry-run\\\", action=\\\"store_true\\\", help=\\\"Show what would be done without actually doing it\\\")\\n+parser.add_argument(\\\"--verbose\\\", action=\\\"store_true\\\", help=\\\"Show detailed logs\\\")\\n+parser.add_argument(\\\"--output\\\", help=\\\"Path to save detailed cleanup report JSON\\\")\\n+\\n+args = parser.parse_args()\\n+\\n+# Configure log file if specified\\n+if args.log_file:\\n+file_handler = logging.FileHandler(args.log_file)\\n+file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\\n+file_handler.setFormatter(file_formatter)\\n+logger.addHandler(file_handler)\\n+\\n+# Set log level based on verbosity\\n+if args.verbose:\\n+logger.setLevel(logging.DEBUG)\\n+\\n+try:\\n+# Create backup cleaner\\n+cleaner = BackupCleaner(\\n+backup_dir=args.backup_dir,\\n+exclusion_patterns=args.exclude,\\n+dry_run=args.dry_run,\\n+verbose=args.verbose,\\n+log_file=args.log_file,\\n+max_backups=args.max_backups\\n+)\\n+\\n+# Execute cleanup\\n+stats = cleaner.execute_cleanup()\\n+\\n+# Save detailed report if requested\\n+if args.output:\\n+report = {\\n+\\\"timestamp\\\": datetime.datetime.now().isoformat(),\\n+\\\"configuration\\\": {\\n+\\\"backup_dir\\\": args.backup_dir,\\n+\\\"max_backups\\\": args.max_backups,\\n+\\\"dry_run\\\": args.dry_run\\n+},\\n+\\\"results\\\": stats\\n+}\\n+\\n+# Create directory for output file if it doesn't exist\\n+output_dir = os.path.dirname(args.output)\\n+if output_dir:\\n+os.makedirs(output_dir, exist_ok=True)\\n+\\n+with open(args.output, 'w', encoding='utf-8') as f:\\n+json.dump(report, f, indent=2)\\n+\\n+logger.info(f\\\"Detailed report saved to {args.output}\\\")\\n+\\n+except KeyboardInterrupt:\\n+print(\\\"\\\\n\\\\nOperation cancelled by user.\\\")\\n+sys.exit(1)\\n+except Exception as e:\\n+logger.critical(f\\\"Unexpected error: {e}\\\")\\n+sys.exit(1)\\n \\n \\n if __name__ == \\\"__main__\\\":\\n-    main()+main()\\n+\\\"\\\"\\\"\""
  },
  {
    "File": "C:\\EGOS\\docs_egos\\10_system_health\\diagnostics\\system_handover_20250512.md",
    "LineNumber": 57,
    "LineContent": "*   **`docs_egos/` Directory:** Intended as the central hub for all project documentation. Current reorganization efforts are underway to streamline this directory (Ref: `diagnosticoENIO.md`, `DOCS_DIRECTORY_DIAGNOSTIC_*.md`)."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\11_resources_and_assets\\misc_resources\\data\\PDD_Template.yaml",
    "LineNumber": 23,
    "LineContent": "#   bias_mitigation_ref: (Optional) Link to bias docs."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\12_website_and_public_docs\\website_content_legacy\\DESIGN_GUIDE.md",
    "LineNumber": 94,
    "LineContent": "## 11. Content Strategy Notes (Ref: Task WCS-1)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\templates\\reference_templates\\PDD_Template.yaml",
    "LineNumber": 23,
    "LineContent": "#   bias_mitigation_ref: (Optional) Link to bias docs."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\templates\\reference_templates\\prompts\\strategic_analysis_prompt.md",
    "LineNumber": 67,
    "LineContent": "* **Technology & Innovation (including AI & Human-AI Collaboration):** Deep knowledge of tech trends, AI/LLM capabilities and limitations, practical applications, human-AI interaction patterns (ref: LLM/IDE Study), security/privacy/IP risks of AI-generated code, and opportunities."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_archive\\historical_changelogs_from_reference\\Updates system EVA.txt",
    "LineNumber": 1223,
    "LineContent": "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181428\\docs\\governance\\.-summary.md",
    "LineNumber": 390,
    "LineContent": "- **TODO/FIXME:** Line 448 - \"diff\": \"--- a/backup_cleanup.py\\n+++ b/backup_cleanup.py\\n@@ -1,12 +1,12 @@\\n \\\"\\\"\\\"TODO: Module docstring for backup_cleanup.py\\\"\\\"\\\"\\n \\n-<!-- \\n+\\\"\\\"\\\"\\n @references:\\n - Core References:\\n-  - [MQP.md](../../..\\..\\MQP.md) - Master Quantum Prompt defining EGOS principles\\n-  - [ROADMAP.md](../../..\\..\\ROADMAP.md) - Project roadmap and planning\\n+- [MQP.md](../../..\\..\\MQP.md) - Master Quantum Prompt defining EGOS principles\\n+- [ROADMAP.md](../../..\\..\\ROADMAP.md) - Project roadmap and planning\\n - Process Documentation:\\n-  - [system_maintenance.md](../../..\\..\\docs\\process\\system_maintenance.md)\\n+- [system_maintenance.md](../../..\\..\\docs\\process\\system_maintenance.md)\\n \\n \\n \\n@@ -39,14 +39,14 @@\\n # EGOS Import Resilience: Add project root to sys.path (see docs_egos/process/dynamic_import_resilience.md)\\n project_root = str(Path(__file__).resolve().parents[2])\\n if project_root not in sys.path:\\n-    sys.path.insert(0, project_root)\\n+sys.path.insert(0, project_root)\\n \\n from scripts.maintenance.status_utils import (\\n-    progress_bar,\\n-    spinner,\\n-    setup_logger,\\n-    box_message,\\n-    section_header\\n+progress_bar,\\n+spinner,\\n+setup_logger,\\n+box_message,\\n+section_header\\n )\\n from typing import List, Dict, Set, Tuple, Optional\\n \\n@@ -54,802 +54,803 @@\\n logger = setup_logger(\\\"egos_backup_cleanup\\\")\\n \\n class BackupCleaner:\\n-            Attributes:\\n-            None\\n+Attributes:\\n+None\\n \\\"\\\"\\\"Cleans unnecessary files from existing backups.\\\"\\\"\\\"\\n-    \\n-    def __init__(\\n-        self,\\n-        backup_dir: str,\\n-        exclusion_patterns: Optional[List[str]] = None,\\n-        dry_run: bool = True,\\n-        verbose: bool = False,\\n-        log_file: Optional[str] = None,\\n-        max_backups: int = 5\\n-    ):\\n-        \\\"\\\"\\\"\\n-        Initialize the backup cleaner.\\n-        \\n-        Args:\\n-            backup_dir: Directory containing backups to clean\\n-            exclusion_patterns: List of patterns to exclude (will be removed from backups)\\n-            dry_run: If True, show what would be done without actually doing it\\n-            verbose: If True, show detailed logs\\n-            log_file: Optional file to log operations\\n-            max_backups: Maximum number of backups to keep (oldest will be removed)\\n-        \\\"\\\"\\\"\\n-        self.backup_dir = backup_dir\\n-        self.dry_run = dry_run\\n-        self.verbose = verbose\\n-        self.max_backups = max_backups\\n-        \\n-        # Default exclusion patterns for unnecessary files\\n-        default_exclusions = [\\n-            \\\"node_modules\\\",\\n-            \\\"venv\\\",\\n-            \\\".venv\\\",\\n-            \\\"env\\\",\\n-            \\\".env\\\", \\n-            \\\"__pycache__\\\",\\n-            \\\".git\\\",\\n-            \\\".pytest_cache\\\",\\n-            \\\".mypy_cache\\\",\\n-            \\\"dist\\\",\\n-            \\\"build\\\",\\n-            \\\"*.egg-info\\\",\\n-            \\\".tox\\\",\\n-            \\\".coverage\\\",\\n-            \\\"htmlcov\\\",\\n-            \\\"*.pyc\\\"\\n-        ]\\n-        \\n-        self.exclusion_patterns = exclusion_patterns or default_exclusions\\n-        \\n-        # Setup file logging if requested\\n-        if log_file:\\n-            file_handler = logging.FileHandler(log_file)\\n-            file_handler.setFormatter(log_formatter)\\n-            logger.addHandler(file_handler)\\n-        \\n-        # Set logging level\\n-        if verbose:\\n-            logger.setLevel(logging.DEBUG)\\n-        \\n-        # Prepare stats\\n-        self.stats = {\\n-            \\\"backups_processed\\\": 0,\\n-            \\\"backups_removed\\\": 0,\\n-            \\\"directories_cleaned\\\": 0,\\n-            \\\"files_removed\\\": 0,\\n-            \\\"bytes_freed\\\": 0,\\n-            \\\"errors\\\": 0\\n-        }\\n-        self.total_removed_size = 0\\n-        \\n-        logger.info(f\\\"Initializing backup cleaner for {self.backup_dir}\\\")\\n-        logger.info(f\\\"Dry Run: {self.dry_run}\\\")\\n-        logger.info(f\\\"Exclusion patterns: {', '.join(self.exclusion_patterns)}\\\")\\n-    \\n-    def _find_backups(self) -> List[Tuple[str, datetime.datetime]]:\\n-        \\\"\\\"\\\"\\n-        Find all backups in the backup directory.\\n-        \\n-        Returns:\\n-            List of tuples (backup_path, creation_time)\\n-        \\\"\\\"\\\"\\n-        backups = []\\n-        \\n-        if not os.path.exists(self.backup_dir):\\n-            logger.warning(f\\\"Backup directory does not exist: {self.backup_dir}\\\")\\n-            return []\\n-        \\n-        # Find zip backups\\n-        for item in os.listdir(self.backup_dir):\\n-            item_path = os.path.join(self.backup_dir, item)\\n-            \\n-            if os.path.isfile(item_path) and item.endswith('.zip') and 'backup' in item.lower():\\n-                try:\\n-                    creation_time = datetime.datetime.fromtimestamp(os.path.getctime(item_path))\\n-                    backups.append((item_path, creation_time))\\n-                    logger.debug(f\\\"Found backup zip: {item_path} (created {creation_time})\\\")\\n-                except Exception as e:\\n-                    logger.warning(f\\\"Error processing backup {item_path}: {e}\\\")\\n-            \\n-            # Also check for uncompressed backup directories\\n-            elif os.path.isdir(item_path) and 'backup' in item.lower():\\n-                try:\\n-                    creation_time = datetime.datetime.fromtimestamp(os.path.getctime(item_path))\\n-                    backups.append((item_path, creation_time))\\n-                    logger.debug(f\\\"Found backup directory: {item_path} (created {creation_time})\\\")\\n-                except Exception as e:\\n-                    logger.warning(f\\\"Error processing backup directory {item_path}: {e}\\\")\\n-        \\n-        # Sort by creation time (newest first)\\n-        backups.sort(key=lambda x: x[1], reverse=True)\\n-        \\n-        return backups\\n-    \\n-    def _should_exclude(self, path: str) -> bool:\\n-        \\\"\\\"\\\"Check if a path matches any exclusion pattern.\\\"\\\"\\\"\\n-        normalized_path = path.replace(\\\"\\\\\\\\\\\", \\\"/\\\")\\n-        \\n-        for pattern in self.exclusion_patterns:\\n-            # Handle wildcard patterns\\n-            if \\\"*\\\" in pattern:\\n-                regex_pattern = pattern.replace(\\\".\\\", \\\"\\\\\\\\.\\\").replace(\\\"*\\\", \\\".*\\\")\\n-                if re.search(regex_pattern, normalized_path, re.IGNORECASE):\\n-                    return True\\n-            # Handle exact matches\\n-            elif pattern.lower() in normalized_path.lower():\\n-                path_parts = normalized_path.lower().split('/')\\n-                if pattern.lower() in path_parts:\\n-                    return True\\n-        \\n-        return False\\n-    \\n-    def _clean_backup_directory(self, backup_path: str) -> int:\\n-        \\\"\\\"\\\"\\n-        Clean unnecessary files from a backup directory.\\n-        \\n-        Args:\\n-            backup_path: Path to the backup directory\\n-            \\n-        Returns:\\n-            Total bytes freed\\n-        \\\"\\\"\\\"\\n-        if not os.path.isdir(backup_path):\\n-            logger.warning(f\\\"Not a directory: {backup_path}\\\")\\n-            return 0\\n-        \\n-        total_bytes_freed = 0\\n-        dirs_removed = 0\\n-        files_removed = 0\\n-        \\n-        # Count items for progress tracking\\n-        dirs_to_scan = [backup_path]\\n-        total_items = sum(len(os.listdir(dir_path)) for dir_path in dirs_to_scan \\n-                         if os.path.isdir(dir_path))\\n-        \\n-        for root, dirs, files in os.walk(backup_path, topdown=True):\\n-            # Check if current directory should be excluded\\n-            dir_name = os.path.basename(root)\\n-            if self._should_exclude(dir_name):\\n-                size = self._get_dir_size(root)\\n-                logger.info(f\\\"{'[DRY RUN] Would remove' if self.dry_run else 'Removing'} directory: {root} ({self._format_size(size)})\\\")\\n-                \\n-                if not self.dry_run:\\n-                    try:\\n-                        shutil.rmtree(root)\\n-                        total_bytes_freed += size\\n-                        dirs_removed += 1\\n-                        self.stats[\\\"directories_cleaned\\\"] += 1\\n-                        self.stats[\\\"bytes_freed\\\"] += size\\n-                        self.total_removed_size += size\\n-                    except Exception as e:\\n-                        logger.error(f\\\"Error removing directory {root}: {e}\\\")\\n-                        self.stats[\\\"errors\\\"] += 1\\n-                \\n-                # Don't process subdirectories\\n-                dirs[:] = []\\n-                continue\\n-            \\n-            # Process files\\n-            for file in files:\\n-                file_path = os.path.join(root, file)\\n-                \\n-                if self._should_exclude(file):\\n-                    try:\\n-                        size = os.path.getsize(file_path)\\n-                        logger.debug(f\\\"{'[DRY RUN] Would remove' if self.dry_run else 'Removing'} file: {file_path} ({self._format_size(size)})\\\")\\n-                        \\n-                        if not self.dry_run:\\n-                            os.remove(file_path)\\n-                            total_bytes_freed += size\\n-                            files_removed += 1\\n-                            self.stats[\\\"files_removed\\\"] += 1\\n-                            self.stats[\\\"bytes_freed\\\"] += size\\n-                            self.total_removed_size += size\\n-                    except Exception as e:\\n-                        logger.error(f\\\"Error removing file {file_path}: {e}\\\")\\n-                        self.stats[\\\"errors\\\"] += 1\\n-        \\n-        logger.info(f\\\"Cleaned backup: {backup_path} - Removed {dirs_removed} directories and {files_removed} files, freed {self._format_size(total_bytes_freed)}\\\")\\n-        return total_bytes_freed\\n-    \\n-    def _extract_and_clean_zip(self, zip_path: str) -> int:\\n-        \\\"\\\"\\\"\\n-        Extract a zip backup, clean it, and recompress it.\\n-        \\n-        Args:\\n-            zip_path: Path to the zip backup\\n-            \\n-        Returns:\\n-            Total bytes freed\\n-        \\\"\\\"\\\"\\n-        if not os.path.isfile(zip_path) or not zip_path.endswith('.zip'):\\n-            logger.warning(f\\\"Not a zip file: {zip_path}\\\")\\n-            return 0\\n-        \\n-        # Get original size\\n-        original_size = os.path.getsize(zip_path)\\n-        \\n-        # Create a temporary extraction directory\\n-        temp_dir = os.path.join(self.backup_dir, f\\\"temp_extract_{os.path.basename(zip_path).replace('.zip', '')}\\\")\\n-        if os.path.exists(temp_dir):\\n-            shutil.rmtree(temp_dir)\\n-        os.makedirs(temp_dir)\\n-        \\n-        try:\\n-            # Extract the zip\\n-            logger.info(f\\\"Extracting {zip_path} to {temp_dir}\\\")\\n-            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\\n-                zip_ref.extractall(temp_dir)\\n-            \\n-            # Clean the extracted directory\\n-            bytes_freed = self._clean_backup_directory(temp_dir)\\n-            \\n-            if not self.dry_run:\\n-                # Create a new zip file\\n-                new_zip_path = zip_path + \\\".new\\\"\\n-                logger.info(f\\\"Creating new zip file: {new_zip_path}\\\")\\n-                \\n-                with zipfile.ZipFile(new_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\\n-                    for root, dirs, files in os.walk(temp_dir):\\n-                        for file in files:\\n-                            try:\\n-                                file_path = os.path.join(root, file)\\n-                                arcname = os.path.relpath(file_path, temp_dir)\\n-                                zipf.write(file_path, arcname)\\n-                            except Exception as e:\\n-                                logger.error(f\\\"Error adding {file_path} to zip: {e}\\\")\\n-                                self.stats[\\\"errors\\\"] += 1\\n-                \\n-                # Replace the original zip with the new one\\n-                new_size = os.path.getsize(new_zip_path)\\n-                bytes_freed = original_size - new_size\\n-                \\n-                os.remove(zip_path)\\n-                os.rename(new_zip_path, zip_path)\\n-                logger.info(f\\\"Replaced {zip_path} - Old size: {self._format_size(original_size)}, New size: {self._format_size(new_size)}\\\")\\n-            \\n-            return bytes_freed\\n-        \\n-        except Exception as e:\\n-            logger.error(f\\\"Error processing zip {zip_path}: {e}\\\")\\n-            self.stats[\\\"errors\\\"] += 1\\n-            return 0\\n-        \\n-        finally:\\n-            # Clean up the temporary directory\\n-            if os.path.exists(temp_dir) and not self.dry_run:\\n-                shutil.rmtree(temp_dir)\\n-    \\n-    def _remove_old_backups(self, backups: List[Tuple[str, datetime.datetime]]) -> int:\\n-        \\\"\\\"\\\"\\n-        Remove backups exceeding the maximum number to keep.\\n-        \\n-        Args:\\n-            backups: List of backups sorted by creation time (newest first)\\n-            \\n-        Returns:\\n-            Total bytes freed\\n-        \\\"\\\"\\\"\\n-        if len(backups) <= self.max_backups:\\n-            logger.info(f\\\"Only {len(backups)} backups found, not exceeding limit of {self.max_backups}\\\")\\n-            return 0\\n-        \\n-        total_bytes_freed = 0\\n-        backups_to_remove = backups[self.max_backups:]\\n-        \\n-        for backup_path, creation_time in backups_to_remove:\\n-            try:\\n-                size = os.path.getsize(backup_path) if os.path.isfile(backup_path) else self._get_dir_size(backup_path)\\n-                logger.info(f\\\"{'[DRY RUN] Would remove' if self.dry_run else 'Removing'} old backup: {backup_path} ({self._format_size(size)}, created {creation_time})\\\")\\n-                \\n-                if not self.dry_run:\\n-                    if os.path.isfile(backup_path):\\n-                        os.remove(backup_path)\\n-                    else:\\n-                        shutil.rmtree(backup_path)\\n-                    \\n-                    total_bytes_freed += size\\n-                    self.stats[\\\"backups_removed\\\"] += 1\\n-                    self.stats[\\\"bytes_freed\\\"] += size\\n-                    self.total_removed_size += size\\n-            except Exception as e:\\n-                logger.error(f\\\"Error removing backup {backup_path}: {e}\\\")\\n-                self.stats[\\\"errors\\\"] += 1\\n-        \\n-        return total_bytes_freed\\n-    \\n-    def _get_dir_size(self, path: str) -> int:\\n-        \\\"\\\"\\\"Get the total size of a directory in bytes.\\\"\\\"\\\"\\n-        total_size = 0\\n-        for dirpath, dirnames, filenames in os.walk(path):\\n-            for f in filenames:\\n-                fp = os.path.join(dirpath, f)\\n-                if not os.path.islink(fp):\\n-                    total_size += os.path.getsize(fp)\\n-        return total_size\\n-    \\n-    def _format_size(self, size_bytes: int) -> str:\\n-        \\\"\\\"\\\"Format size in bytes to human-readable format.\\\"\\\"\\\"\\n-        for unit in ['B', 'KB', 'MB', 'GB']:\\n-            if size_bytes < 1024.0:\\n-                return f\\\"{size_bytes:.1f} {unit}\\\"\\n-            size_bytes /= 1024.0\\n-        return f\\\"{size_bytes:.1f} TB\\\"\\n-    \\n-    def _remove_item(self, item_path: str) -> int:\\n-        \\\"\\\"\\\"Remove a file or directory, handling potential errors.\\\"\\\"\\\"\\n-        item_size = self._get_item_size(item_path)\\n-        action = \\\"directory\\\" if os.path.isdir(item_path) else \\\"file\\\"\\n-        \\n-        logger.debug(f\\\"Attempting to remove {action}: {item_path} ({self._format_size(item_size)})\\\")\\n-        \\n-        if self.dry_run:\\n-            logger.info(f\\\"[Dry Run] Would remove {action}: {item_path} ({self._format_size(item_size)})\\\")\\n-            return item_size  # Simulate freeing space in dry run\\n-\\n-        try:\\n-            if os.path.isdir(item_path):\\n-                # Use onerror handler for rmtree to deal with read-only files\\n-                def remove_readonly(func, path, exc_info):\\n-                    \\\"\\\"\\\"Error handler for shutil.rmtree that tries to remove read-only files.\\\"\\\"\\\"\\n-                    # Check if the error is PermissionError\\n-                    exc_type, exc_value, tb = exc_info\\n-                    if exc_value and isinstance(exc_value, PermissionError):\\n-                        logger.warning(f\\\"PermissionError accessing {path}. Attempting to change permissions.\\\")\\n-                        try:\\n-                            os.chmod(path, 0o777) # Set permissions to read/write/execute for all\\n-                            func(path) # Retry the operation\\n-                        except Exception as retry_exc:\\n-                            logger.error(f\\\"Failed to remove {path} even after chmod: {retry_exc}\\\")\\n-                            # Re-raise the original exception if chmod/retry fails\\n-                            raise exc_value\\n-                    else:\\n-                        # If it's not a PermissionError, raise it to stop the process\\n-                        # or handle other specific errors as needed.\\n-                        raise exc_value # Re-raise the original error\\n-\\n-                shutil.rmtree(item_path, onerror=remove_readonly)\\n-                self.stats[f\\\"{action}s_removed\\\"] += 1\\n-                logger.info(f\\\"Removed {action}: {item_path}\\\")\\n-                self.total_removed_size += item_size\\n-            elif os.path.isfile(item_path):\\n-                try:\\n-                    os.remove(item_path)\\n-                except PermissionError:\\n-                    logger.warning(f\\\"PermissionError removing file {item_path}. Attempting to change permissions.\\\")\\n-                    os.chmod(item_path, 0o777)\\n-                    os.remove(item_path) # Retry removal\\n-                self.stats[f\\\"{action}s_removed\\\"] += 1\\n-                logger.info(f\\\"Removed {action}: {item_path}\\\")\\n-                self.total_removed_size += item_size\\n-            else:\\n-                logger.warning(f\\\"Item not found or not a file/directory: {item_path}\\\")\\n-                return 0 # Not found, no space freed\\n-\\n-            self.stats[\\\"bytes_freed\\\"] += item_size\\n-            return item_size\\n-\\n-        except PermissionError as e:\\n-            # This block might catch errors not handled by onerror or file removal retry\\n-            logger.error(f\\\"Final PermissionError removing {action} {item_path}: {e}. Attempting fallback...\\\")\\n-            initial_error_logged = True # Flag that we logged the initial error\\n-            self.stats[\\\"errors\\\"] += 1 # Log the initial error tentatively\\n-            \\n-            fallback_success = False\\n-            if action == \\\"directory\\\": # Only attempt fallback for directories\\n-                try:\\n-                    # Use native Windows command as fallback\\n-                    import subprocess\\n-                    # Use rmdir /s /q for forceful recursive directory removal\\n-                    # Using shell=True can be a security risk if item_path is not properly controlled/sanitized.\\n-                    # In this context, item_path comes from os.walk which is generally safe.\\n-                    logger.info(f\\\"Attempting fallback: rmdir /s /q \\\\\\\"{item_path}\\\\\\\"\\\")\\n-                    result = subprocess.run(['rmdir', '/s', '/q', item_path], \\n-                                            shell=True, # Necessary for rmdir on Windows cmd\\n-                                            check=True, # Raise CalledProcessError on failure\\n-                                            capture_output=True, # Capture stdout/stderr\\n-                                            text=True, # Decode output as text\\n-                                            encoding='utf-8', # Specify encoding explicitly\\n-                                            errors='replace') # Handle potential decoding errors\\n-                    logger.info(f\\\"Fallback removal successful for directory: {item_path}\\\")\\n-                    # If successful, we assume the space is now freed.\\n-                    # We already captured item_size earlier.\\n-                    self.stats[\\\"bytes_freed\\\"] += item_size \\n-                    # Adjust stats: decrement the error logged above, increment removal count\\n-                    self.stats[\\\"errors\\\"] -= 1 \\n-                    self.stats[f\\\"{action}s_removed\\\"] += 1\\n-                    fallback_success = True # Mark success\\n-                    return item_size # Return freed size\\n-                \\n-                except subprocess.CalledProcessError as sub_e:\\n-                    # Log stderr which often contains useful info from rmdir\\n-                    logger.error(f\\\"Fallback 'rmdir /s /q' failed for {item_path}. Exit Code: {sub_e.returncode}\\\")\\n-                    if sub_e.stderr:\\n-                        logger.error(f\\\"Stderr: {sub_e.stderr.strip()}\\\")\\n-                    else:\\n-                        logger.error(\\\"(No stderr output from rmdir)\\\")\\n-                except FileNotFoundError:\\n-                    logger.error(f\\\"Fallback command 'rmdir' not found. Ensure it's in the system PATH.\\\")\\n-                except Exception as fallback_exc:\\n-                     logger.error(f\\\"Unexpected error during fallback removal of {item_path}: {fallback_exc}\\\")\\n-\\n-            # If fallback wasn't attempted (not a directory) or failed:\\n-            if not fallback_success:\\n-                logger.error(f\\\"Could not remove {action} {item_path} even with fallback (if attempted).\\\")\\n-                if \\\".git\\\" in item_path:\\n-                     logger.warning(f\\\"  Final skip occurred on a path containing '.git'. Manual review likely required.\\\")\\n-            else:\\n-                # If fallback succeeded, ensure we don't double-log the error initially counted\\n-                pass \\n-\\n-            # Log traceback if verbose for the *original* PermissionError\\n-            if self.verbose:\\n-                import traceback\\n-                logger.debug(f\\\"Traceback for initial PermissionError '{e}':\\\\n{traceback.format_exc()}\\\")\\n-\\n-            # Return 0 bytes freed as removal ultimately failed (or wasn't applicable)\\n-            return 0\\n-            \\n-        except Exception as e:\\n-            logger.error(f\\\"General Error removing {action} {item_path}: {e}\\\")\\n-            self.stats[\\\"errors\\\"] += 1\\n-            if self.verbose:\\n-                import traceback\\n-                logger.debug(f\\\"Traceback:\\\\n{traceback.format_exc()}\\\")\\n-            return 0\\n-    \\n-    def _generate_html_report(self, title: str, message: str) -> str:\\n-        \\\"\\\"\\\"\\n-        Generates an HTML report for the backup cleanup operation.\\n-        \\n-        Args:\\n-            title: The title of the report\\n-            message: The main message/summary of the cleanup operation\\n-            \\n-        Returns:\\n-            str: Path to the generated HTML report file\\n-        \\\"\\\"\\\"\\n-        # Create reports directory if it doesn't exist\\n-        reports_dir = os.path.join(\\\"reports\\\", \\\"cleanup\\\")\\n-        os.makedirs(reports_dir, exist_ok=True)\\n-        \\n-        # Generate a timestamp for the report filename\\n-        timestamp = datetime.datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\\n-        report_path = os.path.join(reports_dir, f\\\"backup_cleanup_report_{timestamp}.html\\\")\\n-        \\n-        # Format bytes freed in human-readable format\\n-        bytes_freed = self._format_size(self.stats[\\\"bytes_freed\\\"])\\n-        \\n-        # Create HTML content\\n-        html_content = f\\\"\\\"\\\"<!DOCTYPE html>\\n+\\n+def __init__(\\n+self,\\n+backup_dir: str,\\n+exclusion_patterns: Optional[List[str]] = None,\\n+dry_run: bool = True,\\n+verbose: bool = False,\\n+log_file: Optional[str] = None,\\n+max_backups: int = 5\\n+):\\n+\\\"\\\"\\\"\\n+Initialize the backup cleaner.\\n+\\n+Args:\\n+backup_dir: Directory containing backups to clean\\n+exclusion_patterns: List of patterns to exclude (will be removed from backups)\\n+dry_run: If True, show what would be done without actually doing it\\n+verbose: If True, show detailed logs\\n+log_file: Optional file to log operations\\n+max_backups: Maximum number of backups to keep (oldest will be removed)\\n+\\\"\\\"\\\"\\n+self.backup_dir = backup_dir\\n+self.dry_run = dry_run\\n+self.verbose = verbose\\n+self.max_backups = max_backups\\n+\\n+# Default exclusion patterns for unnecessary files\\n+default_exclusions = [\\n+\\\"node_modules\\\",\\n+\\\"venv\\\",\\n+\\\".venv\\\",\\n+\\\"env\\\",\\n+\\\".env\\\",\\n+\\\"__pycache__\\\",\\n+\\\".git\\\",\\n+\\\".pytest_cache\\\",\\n+\\\".mypy_cache\\\",\\n+\\\"dist\\\",\\n+\\\"build\\\",\\n+\\\"*.egg-info\\\",\\n+\\\".tox\\\",\\n+\\\".coverage\\\",\\n+\\\"htmlcov\\\",\\n+\\\"*.pyc\\\"\\n+]\\n+\\n+self.exclusion_patterns = exclusion_patterns or default_exclusions\\n+\\n+# Setup file logging if requested\\n+if log_file:\\n+file_handler = logging.FileHandler(log_file)\\n+file_handler.setFormatter(log_formatter)\\n+logger.addHandler(file_handler)\\n+\\n+# Set logging level\\n+if verbose:\\n+logger.setLevel(logging.DEBUG)\\n+\\n+# Prepare stats\\n+self.stats = {\\n+\\\"backups_processed\\\": 0,\\n+\\\"backups_removed\\\": 0,\\n+\\\"directories_cleaned\\\": 0,\\n+\\\"files_removed\\\": 0,\\n+\\\"bytes_freed\\\": 0,\\n+\\\"errors\\\": 0\\n+}\\n+self.total_removed_size = 0\\n+\\n+logger.info(f\\\"Initializing backup cleaner for {self.backup_dir}\\\")\\n+logger.info(f\\\"Dry Run: {self.dry_run}\\\")\\n+logger.info(f\\\"Exclusion patterns: {', '.join(self.exclusion_patterns)}\\\")\\n+\\n+def _find_backups(self) -> List[Tuple[str, datetime.datetime]]:\\n+\\\"\\\"\\\"\\n+Find all backups in the backup directory.\\n+\\n+Returns:\\n+List of tuples (backup_path, creation_time)\\n+\\\"\\\"\\\"\\n+backups = []\\n+\\n+if not os.path.exists(self.backup_dir):\\n+logger.warning(f\\\"Backup directory does not exist: {self.backup_dir}\\\")\\n+return []\\n+\\n+# Find zip backups\\n+for item in os.listdir(self.backup_dir):\\n+item_path = os.path.join(self.backup_dir, item)\\n+\\n+if os.path.isfile(item_path) and item.endswith('.zip') and 'backup' in item.lower():\\n+try:\\n+creation_time = datetime.datetime.fromtimestamp(os.path.getctime(item_path))\\n+backups.append((item_path, creation_time))\\n+logger.debug(f\\\"Found backup zip: {item_path} (created {creation_time})\\\")\\n+except Exception as e:\\n+logger.warning(f\\\"Error processing backup {item_path}: {e}\\\")\\n+\\n+# Also check for uncompressed backup directories\\n+elif os.path.isdir(item_path) and 'backup' in item.lower():\\n+try:\\n+creation_time = datetime.datetime.fromtimestamp(os.path.getctime(item_path))\\n+backups.append((item_path, creation_time))\\n+logger.debug(f\\\"Found backup directory: {item_path} (created {creation_time})\\\")\\n+except Exception as e:\\n+logger.warning(f\\\"Error processing backup directory {item_path}: {e}\\\")\\n+\\n+# Sort by creation time (newest first)\\n+backups.sort(key=lambda x: x[1], reverse=True)\\n+\\n+return backups\\n+\\n+def _should_exclude(self, path: str) -> bool:\\n+\\\"\\\"\\\"Check if a path matches any exclusion pattern.\\\"\\\"\\\"\\n+normalized_path = path.replace(\\\"\\\\\\\\\\\", \\\"/\\\")\\n+\\n+for pattern in self.exclusion_patterns:\\n+# Handle wildcard patterns\\n+if \\\"*\\\" in pattern:\\n+regex_pattern = pattern.replace(\\\".\\\", \\\"\\\\\\\\.\\\").replace(\\\"*\\\", \\\".*\\\")\\n+if re.search(regex_pattern, normalized_path, re.IGNORECASE):\\n+return True\\n+# Handle exact matches\\n+elif pattern.lower() in normalized_path.lower():\\n+path_parts = normalized_path.lower().split('/')\\n+if pattern.lower() in path_parts:\\n+return True\\n+\\n+return False\\n+\\n+def _clean_backup_directory(self, backup_path: str) -> int:\\n+\\\"\\\"\\\"\\n+Clean unnecessary files from a backup directory.\\n+\\n+Args:\\n+backup_path: Path to the backup directory\\n+\\n+Returns:\\n+Total bytes freed\\n+\\\"\\\"\\\"\\n+if not os.path.isdir(backup_path):\\n+logger.warning(f\\\"Not a directory: {backup_path}\\\")\\n+return 0\\n+\\n+total_bytes_freed = 0\\n+dirs_removed = 0\\n+files_removed = 0\\n+\\n+# Count items for progress tracking\\n+dirs_to_scan = [backup_path]\\n+total_items = sum(len(os.listdir(dir_path)) for dir_path in dirs_to_scan\\n+if os.path.isdir(dir_path))\\n+\\n+for root, dirs, files in os.walk(backup_path, topdown=True):\\n+# Check if current directory should be excluded\\n+dir_name = os.path.basename(root)\\n+if self._should_exclude(dir_name):\\n+size = self._get_dir_size(root)\\n+logger.info(f\\\"{'[DRY RUN] Would remove' if self.dry_run else 'Removing'} directory: {root} ({self._format_size(size)})\\\")\\n+\\n+if not self.dry_run:\\n+try:\\n+shutil.rmtree(root)\\n+total_bytes_freed += size\\n+dirs_removed += 1\\n+self.stats[\\\"directories_cleaned\\\"] += 1\\n+self.stats[\\\"bytes_freed\\\"] += size\\n+self.total_removed_size += size\\n+except Exception as e:\\n+logger.error(f\\\"Error removing directory {root}: {e}\\\")\\n+self.stats[\\\"errors\\\"] += 1\\n+\\n+# Don't process subdirectories\\n+dirs[:] = []\\n+continue\\n+\\n+# Process files\\n+for file in files:\\n+file_path = os.path.join(root, file)\\n+\\n+if self._should_exclude(file):\\n+try:\\n+size = os.path.getsize(file_path)\\n+logger.debug(f\\\"{'[DRY RUN] Would remove' if self.dry_run else 'Removing'} file: {file_path} ({self._format_size(size)})\\\")\\n+\\n+if not self.dry_run:\\n+os.remove(file_path)\\n+total_bytes_freed += size\\n+files_removed += 1\\n+self.stats[\\\"files_removed\\\"] += 1\\n+self.stats[\\\"bytes_freed\\\"] += size\\n+self.total_removed_size += size\\n+except Exception as e:\\n+logger.error(f\\\"Error removing file {file_path}: {e}\\\")\\n+self.stats[\\\"errors\\\"] += 1\\n+\\n+logger.info(f\\\"Cleaned backup: {backup_path} - Removed {dirs_removed} directories and {files_removed} files, freed {self._format_size(total_bytes_freed)}\\\")\\n+return total_bytes_freed\\n+\\n+def _extract_and_clean_zip(self, zip_path: str) -> int:\\n+\\\"\\\"\\\"\\n+Extract a zip backup, clean it, and recompress it.\\n+\\n+Args:\\n+zip_path: Path to the zip backup\\n+\\n+Returns:\\n+Total bytes freed\\n+\\\"\\\"\\\"\\n+if not os.path.isfile(zip_path) or not zip_path.endswith('.zip'):\\n+logger.warning(f\\\"Not a zip file: {zip_path}\\\")\\n+return 0\\n+\\n+# Get original size\\n+original_size = os.path.getsize(zip_path)\\n+\\n+# Create a temporary extraction directory\\n+temp_dir = os.path.join(self.backup_dir, f\\\"temp_extract_{os.path.basename(zip_path).replace('.zip', '')}\\\")\\n+if os.path.exists(temp_dir):\\n+shutil.rmtree(temp_dir)\\n+os.makedirs(temp_dir)\\n+\\n+try:\\n+# Extract the zip\\n+logger.info(f\\\"Extracting {zip_path} to {temp_dir}\\\")\\n+with zipfile.ZipFile(zip_path, 'r') as zip_ref:\\n+zip_ref.extractall(temp_dir)\\n+\\n+# Clean the extracted directory\\n+bytes_freed = self._clean_backup_directory(temp_dir)\\n+\\n+if not self.dry_run:\\n+# Create a new zip file\\n+new_zip_path = zip_path + \\\".new\\\"\\n+logger.info(f\\\"Creating new zip file: {new_zip_path}\\\")\\n+\\n+with zipfile.ZipFile(new_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\\n+for root, dirs, files in os.walk(temp_dir):\\n+for file in files:\\n+try:\\n+file_path = os.path.join(root, file)\\n+arcname = os.path.relpath(file_path, temp_dir)\\n+zipf.write(file_path, arcname)\\n+except Exception as e:\\n+logger.error(f\\\"Error adding {file_path} to zip: {e}\\\")\\n+self.stats[\\\"errors\\\"] += 1\\n+\\n+# Replace the original zip with the new one\\n+new_size = os.path.getsize(new_zip_path)\\n+bytes_freed = original_size - new_size\\n+\\n+os.remove(zip_path)\\n+os.rename(new_zip_path, zip_path)\\n+logger.info(f\\\"Replaced {zip_path} - Old size: {self._format_size(original_size)}, New size: {self._format_size(new_size)}\\\")\\n+\\n+return bytes_freed\\n+\\n+except Exception as e:\\n+logger.error(f\\\"Error processing zip {zip_path}: {e}\\\")\\n+self.stats[\\\"errors\\\"] += 1\\n+return 0\\n+\\n+finally:\\n+# Clean up the temporary directory\\n+if os.path.exists(temp_dir) and not self.dry_run:\\n+shutil.rmtree(temp_dir)\\n+\\n+def _remove_old_backups(self, backups: List[Tuple[str, datetime.datetime]]) -> int:\\n+\\\"\\\"\\\"\\n+Remove backups exceeding the maximum number to keep.\\n+\\n+Args:\\n+backups: List of backups sorted by creation time (newest first)\\n+\\n+Returns:\\n+Total bytes freed\\n+\\\"\\\"\\\"\\n+if len(backups) <= self.max_backups:\\n+logger.info(f\\\"Only {len(backups)} backups found, not exceeding limit of {self.max_backups}\\\")\\n+return 0\\n+\\n+total_bytes_freed = 0\\n+backups_to_remove = backups[self.max_backups:]\\n+\\n+for backup_path, creation_time in backups_to_remove:\\n+try:\\n+size = os.path.getsize(backup_path) if os.path.isfile(backup_path) else self._get_dir_size(backup_path)\\n+logger.info(f\\\"{'[DRY RUN] Would remove' if self.dry_run else 'Removing'} old backup: {backup_path} ({self._format_size(size)}, created {creation_time})\\\")\\n+\\n+if not self.dry_run:\\n+if os.path.isfile(backup_path):\\n+os.remove(backup_path)\\n+else:\\n+shutil.rmtree(backup_path)\\n+\\n+total_bytes_freed += size\\n+self.stats[\\\"backups_removed\\\"] += 1\\n+self.stats[\\\"bytes_freed\\\"] += size\\n+self.total_removed_size += size\\n+except Exception as e:\\n+logger.error(f\\\"Error removing backup {backup_path}: {e}\\\")\\n+self.stats[\\\"errors\\\"] += 1\\n+\\n+return total_bytes_freed\\n+\\n+def _get_dir_size(self, path: str) -> int:\\n+\\\"\\\"\\\"Get the total size of a directory in bytes.\\\"\\\"\\\"\\n+total_size = 0\\n+for dirpath, dirnames, filenames in os.walk(path):\\n+for f in filenames:\\n+fp = os.path.join(dirpath, f)\\n+if not os.path.islink(fp):\\n+total_size += os.path.getsize(fp)\\n+return total_size\\n+\\n+def _format_size(self, size_bytes: int) -> str:\\n+\\\"\\\"\\\"Format size in bytes to human-readable format.\\\"\\\"\\\"\\n+for unit in ['B', 'KB', 'MB', 'GB']:\\n+if size_bytes < 1024.0:\\n+return f\\\"{size_bytes:.1f} {unit}\\\"\\n+size_bytes /= 1024.0\\n+return f\\\"{size_bytes:.1f} TB\\\"\\n+\\n+def _remove_item(self, item_path: str) -> int:\\n+\\\"\\\"\\\"Remove a file or directory, handling potential errors.\\\"\\\"\\\"\\n+item_size = self._get_item_size(item_path)\\n+action = \\\"directory\\\" if os.path.isdir(item_path) else \\\"file\\\"\\n+\\n+logger.debug(f\\\"Attempting to remove {action}: {item_path} ({self._format_size(item_size)})\\\")\\n+\\n+if self.dry_run:\\n+logger.info(f\\\"[Dry Run] Would remove {action}: {item_path} ({self._format_size(item_size)})\\\")\\n+return item_size  # Simulate freeing space in dry run\\n+\\n+try:\\n+if os.path.isdir(item_path):\\n+# Use onerror handler for rmtree to deal with read-only files\\n+def remove_readonly(func, path, exc_info):\\n+\\\"\\\"\\\"Error handler for shutil.rmtree that tries to remove read-only files.\\\"\\\"\\\"\\n+# Check if the error is PermissionError\\n+exc_type, exc_value, tb = exc_info\\n+if exc_value and isinstance(exc_value, PermissionError):\\n+logger.warning(f\\\"PermissionError accessing {path}. Attempting to change permissions.\\\")\\n+try:\\n+os.chmod(path, 0o777) # Set permissions to read/write/execute for all\\n+func(path) # Retry the operation\\n+except Exception as retry_exc:\\n+logger.error(f\\\"Failed to remove {path} even after chmod: {retry_exc}\\\")\\n+# Re-raise the original exception if chmod/retry fails\\n+raise exc_value\\n+else:\\n+# If it's not a PermissionError, raise it to stop the process\\n+# or handle other specific errors as needed.\\n+raise exc_value # Re-raise the original error\\n+\\n+shutil.rmtree(item_path, onerror=remove_readonly)\\n+self.stats[f\\\"{action}s_removed\\\"] += 1\\n+logger.info(f\\\"Removed {action}: {item_path}\\\")\\n+self.total_removed_size += item_size\\n+elif os.path.isfile(item_path):\\n+try:\\n+os.remove(item_path)\\n+except PermissionError:\\n+logger.warning(f\\\"PermissionError removing file {item_path}. Attempting to change permissions.\\\")\\n+os.chmod(item_path, 0o777)\\n+os.remove(item_path) # Retry removal\\n+self.stats[f\\\"{action}s_removed\\\"] += 1\\n+logger.info(f\\\"Removed {action}: {item_path}\\\")\\n+self.total_removed_size += item_size\\n+else:\\n+logger.warning(f\\\"Item not found or not a file/directory: {item_path}\\\")\\n+return 0 # Not found, no space freed\\n+\\n+self.stats[\\\"bytes_freed\\\"] += item_size\\n+return item_size\\n+\\n+except PermissionError as e:\\n+# This block might catch errors not handled by onerror or file removal retry\\n+logger.error(f\\\"Final PermissionError removing {action} {item_path}: {e}. Attempting fallback...\\\")\\n+initial_error_logged = True # Flag that we logged the initial error\\n+self.stats[\\\"errors\\\"] += 1 # Log the initial error tentatively\\n+\\n+fallback_success = False\\n+if action == \\\"directory\\\": # Only attempt fallback for directories\\n+try:\\n+# Use native Windows command as fallback\\n+import subprocess\\n+# Use rmdir /s /q for forceful recursive directory removal\\n+# Using shell=True can be a security risk if item_path is not properly controlled/sanitized.\\n+# In this context, item_path comes from os.walk which is generally safe.\\n+logger.info(f\\\"Attempting fallback: rmdir /s /q \\\\\\\"{item_path}\\\\\\\"\\\")\\n+result = subprocess.run(['rmdir', '/s', '/q', item_path],\\n+shell=True, # Necessary for rmdir on Windows cmd\\n+check=True, # Raise CalledProcessError on failure\\n+capture_output=True, # Capture stdout/stderr\\n+text=True, # Decode output as text\\n+encoding='utf-8', # Specify encoding explicitly\\n+errors='replace') # Handle potential decoding errors\\n+logger.info(f\\\"Fallback removal successful for directory: {item_path}\\\")\\n+# If successful, we assume the space is now freed.\\n+# We already captured item_size earlier.\\n+self.stats[\\\"bytes_freed\\\"] += item_size\\n+# Adjust stats: decrement the error logged above, increment removal count\\n+self.stats[\\\"errors\\\"] -= 1\\n+self.stats[f\\\"{action}s_removed\\\"] += 1\\n+fallback_success = True # Mark success\\n+return item_size # Return freed size\\n+\\n+except subprocess.CalledProcessError as sub_e:\\n+# Log stderr which often contains useful info from rmdir\\n+logger.error(f\\\"Fallback 'rmdir /s /q' failed for {item_path}. Exit Code: {sub_e.returncode}\\\")\\n+if sub_e.stderr:\\n+logger.error(f\\\"Stderr: {sub_e.stderr.strip()}\\\")\\n+else:\\n+logger.error(\\\"(No stderr output from rmdir)\\\")\\n+except FileNotFoundError:\\n+logger.error(f\\\"Fallback command 'rmdir' not found. Ensure it's in the system PATH.\\\")\\n+except Exception as fallback_exc:\\n+logger.error(f\\\"Unexpected error during fallback removal of {item_path}: {fallback_exc}\\\")\\n+\\n+# If fallback wasn't attempted (not a directory) or failed:\\n+if not fallback_success:\\n+logger.error(f\\\"Could not remove {action} {item_path} even with fallback (if attempted).\\\")\\n+if \\\".git\\\" in item_path:\\n+logger.warning(f\\\"  Final skip occurred on a path containing '.git'. Manual review likely required.\\\")\\n+else:\\n+# If fallback succeeded, ensure we don't double-log the error initially counted\\n+pass\\n+\\n+# Log traceback if verbose for the *original* PermissionError\\n+if self.verbose:\\n+import traceback\\n+logger.debug(f\\\"Traceback for initial PermissionError '{e}':\\\\n{traceback.format_exc()}\\\")\\n+\\n+# Return 0 bytes freed as removal ultimately failed (or wasn't applicable)\\n+return 0\\n+\\n+except Exception as e:\\n+logger.error(f\\\"General Error removing {action} {item_path}: {e}\\\")\\n+self.stats[\\\"errors\\\"] += 1\\n+if self.verbose:\\n+import traceback\\n+logger.debug(f\\\"Traceback:\\\\n{traceback.format_exc()}\\\")\\n+return 0\\n+\\n+def _generate_html_report(self, title: str, message: str) -> str:\\n+\\\"\\\"\\\"\\n+Generates an HTML report for the backup cleanup operation.\\n+\\n+Args:\\n+title: The title of the report\\n+message: The main message/summary of the cleanup operation\\n+\\n+Returns:\\n+str: Path to the generated HTML report file\\n+\\\"\\\"\\\"\\n+# Create reports directory if it doesn't exist\\n+reports_dir = os.path.join(\\\"reports\\\", \\\"cleanup\\\")\\n+os.makedirs(reports_dir, exist_ok=True)\\n+\\n+# Generate a timestamp for the report filename\\n+timestamp = datetime.datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\\n+report_path = os.path.join(reports_dir, f\\\"backup_cleanup_report_{timestamp}.html\\\")\\n+\\n+# Format bytes freed in human-readable format\\n+bytes_freed = self._format_size(self.stats[\\\"bytes_freed\\\"])\\n+\\n+# Create HTML content\\n+html_content = f\\\"\\\"\\\"<!DOCTYPE html>\\n <html lang=\\\"en\\\">\\n <head>\\n-    <meta charset=\\\"UTF-8\\\">\\n-    <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\">\\n-    <title>{title}</title>\\n-    <style>\\n-        body {{\\n-            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\\n-            line-height: 1.6;\\n-            color: #333;\\n-            max-width: 800px;\\n-            margin: 0 auto;\\n-            padding: 20px;\\n-            background-color: #f5f5f5;\\n-        }}\\n-        .header {{\\n-            background-color: #4a86e8;\\n-            color: white;\\n-            padding: 15px 20px;\\n-            border-radius: 5px 5px 0 0;\\n-            margin-bottom: 0;\\n-        }}\\n-        .content {{\\n-            background-color: white;\\n-            padding: 20px;\\n-            border-radius: 0 0 5px 5px;\\n-            box-shadow: 0 2px 5px rgba(0,0,0,0.1);\\n-            margin-top: 0;\\n-        }}\\n-        .summary {{\\n-            font-size: 1.1em;\\n-            margin-bottom: 20px;\\n-            padding-bottom: 15px;\\n-            border-bottom: 1px solid #eee;\\n-        }}\\n-        table {{\\n-            width: 100%;\\n-            border-collapse: collapse;\\n-            margin: 20px 0;\\n-        }}\\n-        th, td {{\\n-            padding: 12px 15px;\\n-            border-bottom: 1px solid #ddd;\\n-            text-align: left;\\n-        }}\\n-        th {{\\n-            background-color: #f8f8f8;\\n-        }}\\n-        .footer {{\\n-            margin-top: 30px;\\n-            font-size: 0.9em;\\n-            color: #777;\\n-            text-align: center;\\n-        }}\\n-        .success {{\\n-            color: #28a745;\\n-        }}\\n-        .warning {{\\n-            color: #ffc107;\\n-        }}\\n-        .error {{\\n-            color: #dc3545;\\n-        }}\\n-    </style>\\n+<meta charset=\\\"UTF-8\\\">\\n+<meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\">\\n+<title>{title}</title>\\n+<style>\\n+body {{\\n+font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\\n+line-height: 1.6;\\n+color: #333;\\n+max-width: 800px;\\n+margin: 0 auto;\\n+padding: 20px;\\n+background-color: #f5f5f5;\\n+}}\\n+.header {{\\n+background-color: #4a86e8;\\n+color: white;\\n+padding: 15px 20px;\\n+border-radius: 5px 5px 0 0;\\n+margin-bottom: 0;\\n+}}\\n+.content {{\\n+background-color: white;\\n+padding: 20px;\\n+border-radius: 0 0 5px 5px;\\n+box-shadow: 0 2px 5px rgba(0,0,0,0.1);\\n+margin-top: 0;\\n+}}\\n+.summary {{\\n+font-size: 1.1em;\\n+margin-bottom: 20px;\\n+padding-bottom: 15px;\\n+border-bottom: 1px solid #eee;\\n+}}\\n+table {{\\n+width: 100%;\\n+border-collapse: collapse;\\n+margin: 20px 0;\\n+}}\\n+th, td {{\\n+padding: 12px 15px;\\n+border-bottom: 1px solid #ddd;\\n+text-align: left;\\n+}}\\n+th {{\\n+background-color: #f8f8f8;\\n+}}\\n+.footer {{\\n+margin-top: 30px;\\n+font-size: 0.9em;\\n+color: #777;\\n+text-align: center;\\n+}}\\n+.success {{\\n+color: #28a745;\\n+}}\\n+.warning {{\\n+color: #ffc107;\\n+}}\\n+.error {{\\n+color: #dc3545;\\n+}}\\n+</style>\\n </head>\\n <body>\\n-    <div class=\\\"header\\\">\\n-        <h1>{title}</h1>\\n-    </div>\\n-    <div class=\\\"content\\\">\\n-        <div class=\\\"summary\\\">\\n-            <p>{message}</p>\\n-        </div>\\n-        \\n-        <h2>Cleanup Details</h2>\\n-        <table>\\n-            <tr>\\n-                <th>Metric</th>\\n-                <th>Value</th>\\n-            </tr>\\n-            <tr>\\n-                <td>Backups Processed</td>\\n-                <td>{self.stats[\\\"backups_processed\\\"]}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Backups Removed</td>\\n-                <td>{self.stats[\\\"backups_removed\\\"]}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Directories Cleaned</td>\\n-                <td>{self.stats[\\\"directories_cleaned\\\"]}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Files Removed</td>\\n-                <td>{self.stats[\\\"files_removed\\\"]}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Space Freed</td>\\n-                <td>{bytes_freed}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Errors</td>\\n-                <td class=\\\"{(\\\"success\\\" if self.stats[\\\"errors\\\"] == 0 else \\\"error\\\")}\\\">{self.stats[\\\"errors\\\"]}</td>\\n-            </tr>\\n-        </table>\\n-        \\n-        <h2>Operation Information</h2>\\n-        <table>\\n-            <tr>\\n-                <th>Property</th>\\n-                <th>Value</th>\\n-            </tr>\\n-            <tr>\\n-                <td>Backup Directory</td>\\n-                <td>{self.backup_dir}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Dry Run</td>\\n-                <td>{str(self.dry_run)}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Max Backups Retained</td>\\n-                <td>{self.max_backups}</td>\\n-            </tr>\\n-            <tr>\\n-                <td>Timestamp</td>\\n-                <td>{datetime.datetime.now().strftime(\\\"%Y-%m-%d %H:%M:%S\\\")}</td>\\n-            </tr>\\n-        </table>\\n-        \\n-        <div class=\\\"footer\\\">\\n-            <p>Generated by EGOS Backup Cleanup Utility</p>\\n-        </div>\\n-    </div>\\n+<div class=\\\"header\\\">\\n+<h1>{title}</h1>\\n+</div>\\n+<div class=\\\"content\\\">\\n+<div class=\\\"summary\\\">\\n+<p>{message}</p>\\n+</div>\\n+\\n+<h2>Cleanup Details</h2>\\n+<table>\\n+<tr>\\n+<th>Metric</th>\\n+<th>Value</th>\\n+</tr>\\n+<tr>\\n+<td>Backups Processed</td>\\n+<td>{self.stats[\\\"backups_processed\\\"]}</td>\\n+</tr>\\n+<tr>\\n+<td>Backups Removed</td>\\n+<td>{self.stats[\\\"backups_removed\\\"]}</td>\\n+</tr>\\n+<tr>\\n+<td>Directories Cleaned</td>\\n+<td>{self.stats[\\\"directories_cleaned\\\"]}</td>\\n+</tr>\\n+<tr>\\n+<td>Files Removed</td>\\n+<td>{self.stats[\\\"files_removed\\\"]}</td>\\n+</tr>\\n+<tr>\\n+<td>Space Freed</td>\\n+<td>{bytes_freed}</td>\\n+</tr>\\n+<tr>\\n+<td>Errors</td>\\n+<td class=\\\"{(\\\"success\\\" if self.stats[\\\"errors\\\"] == 0 else \\\"error\\\")}\\\">{self.stats[\\\"errors\\\"]}</td>\\n+</tr>\\n+</table>\\n+\\n+<h2>Operation Information</h2>\\n+<table>\\n+<tr>\\n+<th>Property</th>\\n+<th>Value</th>\\n+</tr>\\n+<tr>\\n+<td>Backup Directory</td>\\n+<td>{self.backup_dir}</td>\\n+</tr>\\n+<tr>\\n+<td>Dry Run</td>\\n+<td>{str(self.dry_run)}</td>\\n+</tr>\\n+<tr>\\n+<td>Max Backups Retained</td>\\n+<td>{self.max_backups}</td>\\n+</tr>\\n+<tr>\\n+<td>Timestamp</td>\\n+<td>{datetime.datetime.now().strftime(\\\"%Y-%m-%d %H:%M:%S\\\")}</td>\\n+</tr>\\n+</table>\\n+\\n+<div class=\\\"footer\\\">\\n+<p>Generated by EGOS Backup Cleanup Utility</p>\\n+</div>\\n+</div>\\n </body>\\n </html>\\\"\\\"\\\"\\n-        \\n-        # Write HTML content to file\\n-        with open(report_path, 'w', encoding='utf-8') as f:\\n-            f.write(html_content)\\n-        \\n-        logger.info(f\\\"Generated HTML report at {report_path}\\\")\\n-        return report_path\\n-    \\n-    def _show_notification(self, title: str, message: str):\\n-        \\\"\\\"\\\"\\n-        Shows a notification about the backup cleanup operation.\\n-        Generates an HTML report and opens it in the default browser.\\n-        \\n-        Args:\\n-            title: The title of the notification\\n-            message: The message to display\\n-        \\\"\\\"\\\"\\n-        try:\\n-            # Generate HTML report\\n-            report_path = self._generate_html_report(title, message)\\n-            \\n-            # Open the report in the default browser\\n-            report_url = f\\\"file:///{report_path.replace(os.sep, '/')}\\\"\\n-            logger.info(f\\\"Opening report in browser: {report_url}\\\")\\n-            \\n-            # Use appropriate command based on platform\\n-            if sys.platform == 'win32':\\n-                os.startfile(report_path)\\n-            elif sys.platform == 'darwin':  # macOS\\n-                subprocess.run(['open', report_path], check=False)\\n-            else:  # Linux and others\\n-                subprocess.run(['xdg-open', report_path], check=False)\\n-                \\n-            logger.info(\\\"Opened cleanup report in default browser\\\")\\n-        except Exception as e:\\n-            logger.error(f\\\"Error showing notification: {e}\\\")\\n-    \\n-    def execute_cleanup(self) -> Dict:\\n-        \\\"\\\"\\\"\\n-        Execute the backup cleanup process.\\n-        \\n-        Returns:\\n-            dict: Cleanup statistics\\n-        \\\"\\\"\\\"\\n-        print(box_message(\\\"EGOS Backup Cleanup\\\", [\\n-            f\\\"Target Directory: {self.backup_dir}\\\",\\n-            f\\\"Dry Run: {self.dry_run}\\\",\\n-            f\\\"Max Backups: {self.max_backups}\\\",\\n-            f\\\"Started: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\"\\n-        ]))\\n-        print()\\n-        \\n-        # Find all backups\\n-        with spinner(\\\"Scanning for backups\\\") as spin:\\n-            backups = self._find_backups()\\n-            spin.finish(f\\\"Found {len(backups)} backups\\\")\\n-        \\n-        print(section_header(\\\"Processing Backups\\\"))\\n-        \\n-        # Process each backup\\n-        for idx, (backup_path, creation_time) in enumerate(backups):\\n-            backup_name = os.path.basename(backup_path)\\n-            creation_str = creation_time.strftime(\\\"%Y-%m-%d %H:%M\\\")\\n-            \\n-            print(f\\\"\\\\n{idx+1}/{len(backups)} \\u2022 {backup_name} (created {creation_str})\\\")\\n-            self.stats[\\\"backups_processed\\\"] += 1\\n-            \\n-            if os.path.isfile(backup_path) and backup_path.endswith('.zip'):\\n-                with spinner(f\\\"Processing zip backup\\\") as spin:\\n-                    try:\\n-                        freed = self._extract_and_clean_zip(backup_path)\\n-                        spin.finish(f\\\"Processed zip backup \\u2022 {self._format_size(freed)} freed\\\")\\n-                    except Exception as e:\\n-                        spin.finish(f\\\"Error processing zip backup\\\")\\n-                        logger.error(f\\\"Error details: {e}\\\")\\n-                        self.stats[\\\"errors\\\"] += 1\\n-            elif os.path.isdir(backup_path):\\n-                # For directories, we can show more granular progress\\n-                logger.info(f\\\"Cleaning directory backup: {backup_path}\\\")\\n-                freed = self._clean_backup_directory(backup_path)\\n-                logger.info(f\\\"Cleaned directory \\u2022 {self._format_size(freed)} freed\\\")\\n-        \\n-        # Remove old backups if exceeding maximum\\n-        if len(backups) > self.max_backups:\\n-            print(\\\"\\\\n\\\" + section_header(\\\"Removing Old Backups\\\"))\\n-            with spinner(f\\\"Removing {len(backups) - self.max_backups} old backups\\\") as spin:\\n-                freed = self._remove_old_backups(backups)\\n-                spin.finish(f\\\"Removed old backups \\u2022 {self._format_size(freed)} freed\\\")\\n-        \\n-        # Show final results\\n-        print(\\\"\\\\n\\\" + section_header(\\\"Cleanup Results\\\"))\\n-        \\n-        results = [\\n-            f\\\"Backups Processed: {self.stats['backups_processed']}\\\",\\n-            f\\\"Backups Removed: {self.stats['backups_removed']}\\\",\\n-            f\\\"Directories Cleaned: {self.stats['directories_cleaned']}\\\",\\n-            f\\\"Files Removed: {self.stats['files_removed']}\\\",\\n-            f\\\"Total Space Freed: {self._format_size(self.stats['bytes_freed'])}\\\",\\n-            f\\\"Errors: {self.stats['errors']}\\\"\\n-        ]\\n-        \\n-        for result in results:\\n-            logger.info(result)\\n-        \\n-        print(\\\"\\\\n\\\" + box_message(\\\"Cleanup Completed\\\", \\n-            f\\\"Total Space {'Would be Freed' if self.dry_run else 'Freed'}: {self._format_size(self.stats['bytes_freed'])}\\\"))\\n-        \\n-        # Send notification\\n-        error_count = self.stats[\\\"errors\\\"]\\n-        title = \\\"EGOS Backup Cleanup Complete\\\"\\n-        size_mb = self.stats[\\\"bytes_freed\\\"] / (1024 * 1024)\\n-        message = f\\\"Removed {self.stats['backups_removed']} backups, {self.stats['directories_cleaned']} directories, and {self.stats['files_removed']} files ({size_mb:.2f} MB). \\\"\\n-        if error_count > 0:\\n-            message += f\\\"{error_count} errors encountered.\\\"\\n-        else:\\n-            message += \\\"No errors.\\\"\\n-        self._show_notification(title, message)\\n-        \\n-        return self.stats\\n+\\n+# Write HTML content to file\\n+with open(report_path, 'w', encoding='utf-8') as f:\\n+f.write(html_content)\\n+\\n+logger.info(f\\\"Generated HTML report at {report_path}\\\")\\n+return report_path\\n+\\n+def _show_notification(self, title: str, message: str):\\n+\\\"\\\"\\\"\\n+Shows a notification about the backup cleanup operation.\\n+Generates an HTML report and opens it in the default browser.\\n+\\n+Args:\\n+title: The title of the notification\\n+message: The message to display\\n+\\\"\\\"\\\"\\n+try:\\n+# Generate HTML report\\n+report_path = self._generate_html_report(title, message)\\n+\\n+# Open the report in the default browser\\n+report_url = f\\\"file:///{report_path.replace(os.sep, '/')}\\\"\\n+logger.info(f\\\"Opening report in browser: {report_url}\\\")\\n+\\n+# Use appropriate command based on platform\\n+if sys.platform == 'win32':\\n+os.startfile(report_path)\\n+elif sys.platform == 'darwin':  # macOS\\n+subprocess.run(['open', report_path], check=False)\\n+else:  # Linux and others\\n+subprocess.run(['xdg-open', report_path], check=False)\\n+\\n+logger.info(\\\"Opened cleanup report in default browser\\\")\\n+except Exception as e:\\n+logger.error(f\\\"Error showing notification: {e}\\\")\\n+\\n+def execute_cleanup(self) -> Dict:\\n+\\\"\\\"\\\"\\n+Execute the backup cleanup process.\\n+\\n+Returns:\\n+dict: Cleanup statistics\\n+\\\"\\\"\\\"\\n+print(box_message(\\\"EGOS Backup Cleanup\\\", [\\n+f\\\"Target Directory: {self.backup_dir}\\\",\\n+f\\\"Dry Run: {self.dry_run}\\\",\\n+f\\\"Max Backups: {self.max_backups}\\\",\\n+f\\\"Started: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\"\\n+]))\\n+print()\\n+\\n+# Find all backups\\n+with spinner(\\\"Scanning for backups\\\") as spin:\\n+backups = self._find_backups()\\n+spin.finish(f\\\"Found {len(backups)} backups\\\")\\n+\\n+print(section_header(\\\"Processing Backups\\\"))\\n+\\n+# Process each backup\\n+for idx, (backup_path, creation_time) in enumerate(backups):\\n+backup_name = os.path.basename(backup_path)\\n+creation_str = creation_time.strftime(\\\"%Y-%m-%d %H:%M\\\")\\n+\\n+print(f\\\"\\\\n{idx+1}/{len(backups)} \\u2022 {backup_name} (created {creation_str})\\\")\\n+self.stats[\\\"backups_processed\\\"] += 1\\n+\\n+if os.path.isfile(backup_path) and backup_path.endswith('.zip'):\\n+with spinner(f\\\"Processing zip backup\\\") as spin:\\n+try:\\n+freed = self._extract_and_clean_zip(backup_path)\\n+spin.finish(f\\\"Processed zip backup \\u2022 {self._format_size(freed)} freed\\\")\\n+except Exception as e:\\n+spin.finish(f\\\"Error processing zip backup\\\")\\n+logger.error(f\\\"Error details: {e}\\\")\\n+self.stats[\\\"errors\\\"] += 1\\n+elif os.path.isdir(backup_path):\\n+# For directories, we can show more granular progress\\n+logger.info(f\\\"Cleaning directory backup: {backup_path}\\\")\\n+freed = self._clean_backup_directory(backup_path)\\n+logger.info(f\\\"Cleaned directory \\u2022 {self._format_size(freed)} freed\\\")\\n+\\n+# Remove old backups if exceeding maximum\\n+if len(backups) > self.max_backups:\\n+print(\\\"\\\\n\\\" + section_header(\\\"Removing Old Backups\\\"))\\n+with spinner(f\\\"Removing {len(backups) - self.max_backups} old backups\\\") as spin:\\n+freed = self._remove_old_backups(backups)\\n+spin.finish(f\\\"Removed old backups \\u2022 {self._format_size(freed)} freed\\\")\\n+\\n+# Show final results\\n+print(\\\"\\\\n\\\" + section_header(\\\"Cleanup Results\\\"))\\n+\\n+results = [\\n+f\\\"Backups Processed: {self.stats['backups_processed']}\\\",\\n+f\\\"Backups Removed: {self.stats['backups_removed']}\\\",\\n+f\\\"Directories Cleaned: {self.stats['directories_cleaned']}\\\",\\n+f\\\"Files Removed: {self.stats['files_removed']}\\\",\\n+f\\\"Total Space Freed: {self._format_size(self.stats['bytes_freed'])}\\\",\\n+f\\\"Errors: {self.stats['errors']}\\\"\\n+]\\n+\\n+for result in results:\\n+logger.info(result)\\n+\\n+print(\\\"\\\\n\\\" + box_message(\\\"Cleanup Completed\\\",\\n+f\\\"Total Space {'Would be Freed' if self.dry_run else 'Freed'}: {self._format_size(self.stats['bytes_freed'])}\\\"))\\n+\\n+# Send notification\\n+error_count = self.stats[\\\"errors\\\"]\\n+title = \\\"EGOS Backup Cleanup Complete\\\"\\n+size_mb = self.stats[\\\"bytes_freed\\\"] / (1024 * 1024)\\n+message = f\\\"Removed {self.stats['backups_removed']} backups, {self.stats['directories_cleaned']} directories, and {self.stats['files_removed']} files ({size_mb:.2f} MB). \\\"\\n+if error_count > 0:\\n+message += f\\\"{error_count} errors encountered.\\\"\\n+else:\\n+message += \\\"No errors.\\\"\\n+self._show_notification(title, message)\\n+\\n+return self.stats\\n \\n \\n def main():\\n-    \\\"\\\"\\\"Main function for command-line execution.\\\"\\\"\\\"\\n-    parser = argparse.ArgumentParser(description=\\\"EGOS Backup Cleanup Script\\\")\\n-    parser.add_argument(\\\"--backup-dir\\\", default=\\\"./backups\\\", help=\\\"Directory containing backups to clean\\\")\\n-    parser.add_argument(\\\"--exclude\\\", nargs=\\\"+\\\", help=\\\"Additional patterns to exclude\\\")\\n-    parser.add_argument(\\\"--max-backups\\\", type=int, default=5, help=\\\"Maximum number of backups to keep\\\")\\n-    parser.add_argument(\\\"--log-file\\\", help=\\\"File to log operations\\\")\\n-    parser.add_argument(\\\"--dry-run\\\", action=\\\"store_true\\\", help=\\\"Show what would be done without actually doing it\\\")\\n-    parser.add_argument(\\\"--verbose\\\", action=\\\"store_true\\\", help=\\\"Show detailed logs\\\")\\n-    parser.add_argument(\\\"--output\\\", help=\\\"Path to save detailed cleanup report JSON\\\")\\n-    \\n-    args = parser.parse_args()\\n-    \\n-    # Configure log file if specified\\n-    if args.log_file:\\n-        file_handler = logging.FileHandler(args.log_file)\\n-        file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\\n-        file_handler.setFormatter(file_formatter)\\n-        logger.addHandler(file_handler)\\n-    \\n-    # Set log level based on verbosity\\n-    if args.verbose:\\n-        logger.setLevel(logging.DEBUG)\\n-    \\n-    try:\\n-        # Create backup cleaner\\n-        cleaner = BackupCleaner(\\n-            backup_dir=args.backup_dir,\\n-            exclusion_patterns=args.exclude,\\n-            dry_run=args.dry_run,\\n-            verbose=args.verbose,\\n-            log_file=args.log_file,\\n-            max_backups=args.max_backups\\n-        )\\n-        \\n-        # Execute cleanup\\n-        stats = cleaner.execute_cleanup()\\n-        \\n-        # Save detailed report if requested\\n-        if args.output:\\n-            report = {\\n-                \\\"timestamp\\\": datetime.datetime.now().isoformat(),\\n-                \\\"configuration\\\": {\\n-                    \\\"backup_dir\\\": args.backup_dir,\\n-                    \\\"max_backups\\\": args.max_backups,\\n-                    \\\"dry_run\\\": args.dry_run\\n-                },\\n-                \\\"results\\\": stats\\n-            }\\n-            \\n-            # Create directory for output file if it doesn't exist\\n-            output_dir = os.path.dirname(args.output)\\n-            if output_dir:\\n-                os.makedirs(output_dir, exist_ok=True)\\n-\\n-            with open(args.output, 'w', encoding='utf-8') as f:\\n-                json.dump(report, f, indent=2)\\n-                \\n-            logger.info(f\\\"Detailed report saved to {args.output}\\\")\\n-    \\n-    except KeyboardInterrupt:\\n-        print(\\\"\\\\n\\\\nOperation cancelled by user.\\\")\\n-        sys.exit(1)\\n-    except Exception as e:\\n-        logger.critical(f\\\"Unexpected error: {e}\\\")\\n-        sys.exit(1)\\n+\\\"\\\"\\\"Main function for command-line execution.\\\"\\\"\\\"\\n+parser = argparse.ArgumentParser(description=\\\"EGOS Backup Cleanup Script\\\")\\n+parser.add_argument(\\\"--backup-dir\\\", default=\\\"./backups\\\", help=\\\"Directory containing backups to clean\\\")\\n+parser.add_argument(\\\"--exclude\\\", nargs=\\\"+\\\", help=\\\"Additional patterns to exclude\\\")\\n+parser.add_argument(\\\"--max-backups\\\", type=int, default=5, help=\\\"Maximum number of backups to keep\\\")\\n+parser.add_argument(\\\"--log-file\\\", help=\\\"File to log operations\\\")\\n+parser.add_argument(\\\"--dry-run\\\", action=\\\"store_true\\\", help=\\\"Show what would be done without actually doing it\\\")\\n+parser.add_argument(\\\"--verbose\\\", action=\\\"store_true\\\", help=\\\"Show detailed logs\\\")\\n+parser.add_argument(\\\"--output\\\", help=\\\"Path to save detailed cleanup report JSON\\\")\\n+\\n+args = parser.parse_args()\\n+\\n+# Configure log file if specified\\n+if args.log_file:\\n+file_handler = logging.FileHandler(args.log_file)\\n+file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\\n+file_handler.setFormatter(file_formatter)\\n+logger.addHandler(file_handler)\\n+\\n+# Set log level based on verbosity\\n+if args.verbose:\\n+logger.setLevel(logging.DEBUG)\\n+\\n+try:\\n+# Create backup cleaner\\n+cleaner = BackupCleaner(\\n+backup_dir=args.backup_dir,\\n+exclusion_patterns=args.exclude,\\n+dry_run=args.dry_run,\\n+verbose=args.verbose,\\n+log_file=args.log_file,\\n+max_backups=args.max_backups\\n+)\\n+\\n+# Execute cleanup\\n+stats = cleaner.execute_cleanup()\\n+\\n+# Save detailed report if requested\\n+if args.output:\\n+report = {\\n+\\\"timestamp\\\": datetime.datetime.now().isoformat(),\\n+\\\"configuration\\\": {\\n+\\\"backup_dir\\\": args.backup_dir,\\n+\\\"max_backups\\\": args.max_backups,\\n+\\\"dry_run\\\": args.dry_run\\n+},\\n+\\\"results\\\": stats\\n+}\\n+\\n+# Create directory for output file if it doesn't exist\\n+output_dir = os.path.dirname(args.output)\\n+if output_dir:\\n+os.makedirs(output_dir, exist_ok=True)\\n+\\n+with open(args.output, 'w', encoding='utf-8') as f:\\n+json.dump(report, f, indent=2)\\n+\\n+logger.info(f\\\"Detailed report saved to {args.output}\\\")\\n+\\n+except KeyboardInterrupt:\\n+print(\\\"\\\\n\\\\nOperation cancelled by user.\\\")\\n+sys.exit(1)\\n+except Exception as e:\\n+logger.critical(f\\\"Unexpected error: {e}\\\")\\n+sys.exit(1)\\n \\n \\n if __name__ == \\\"__main__\\\":\\n-    main()+main()\\n+\\\"\\\"\\\"\""
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181428\\docs\\governance\\automated_docstring_fixing.md",
    "LineNumber": 130,
    "LineContent": "    Ref: PROC-KOIOS-003"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181435\\docs\\governance\\human_ai_collaboration_guidelines.md",
    "LineNumber": 58,
    "LineContent": "These guidelines enhance the EGOS operational framework (ref: `MQP`, `.windsurfrules`, `MEMORY[user_global]`, `MEMORY[user_17200193039781666577]`) with specific principles for effective human-AI collaboration, drawing from modern software engineering research (2024-2025)."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181538\\docs\\governance\\migrations\\processed\\mixed-pt-en\\Updates system EVA.md",
    "LineNumber": 1269,
    "LineContent": "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181541\\docs\\governance\\reports\\EGOS_Project_Diagnostic_Report.md",
    "LineNumber": 554,
    "LineContent": "- - **[CORUJA-DOCS-026]** Status: `OPEN`. Module docstring is incomplete (contains TODO placeholder) and does not conform to EGOS standard format (Ref: MEMORY[05e5435b...])."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181548\\docs\\governance\\roadmaps\\apps\\website\\ROADMAP.md",
    "LineNumber": 218,
    "LineContent": "- [x] VIS-INT-002: Fix node movement on hover while preserving zoom/pan. (Ref: `SystemGraph.tsx`)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181604\\docs\\governance\\roadmaps\\website\\ROADMAP.md",
    "LineNumber": 218,
    "LineContent": "- [x] VIS-INT-002: Fix node movement on hover while preserving zoom/pan. (Ref: `SystemGraph.tsx`)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181613\\docs\\markdown\\apps\\website\\WEBSITE_ROADMAP.md",
    "LineNumber": 218,
    "LineContent": "- [x] VIS-INT-002: Fix node movement on hover while preserving zoom/pan. (Ref: `SystemGraph.tsx`)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181619\\docs\\markdown\\docs\\apps\\DESIGN_GUIDE.md",
    "LineNumber": 147,
    "LineContent": "## 11. Content Strategy Notes (Ref: Task WCS-1)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181625\\docs\\markdown\\project_governance\\MQP.md",
    "LineNumber": 129,
    "LineContent": "* **Conscious Modularity:** Deep understanding of parts and whole (Ref: [[subsystem_boundaries.mdc](cci:7://file:///C:/EGOS/.cursor/rules/subsystem_boundaries.mdc:0:0-0:0)](../../..\\..\\.cursor\\rules\\subsystem_boundaries.mdc), [`file_modularity.mdc`](../../..\\..\\.cursor\\rules\\file_modularity.mdc))."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181638\\docs\\reference\\MQP.md",
    "LineNumber": 176,
    "LineContent": "* **Conscious Modularity:** Deep understanding of parts and whole (Ref: [[subsystem_boundaries.mdc](cci:7://file:///C:/EGOS/.cursor/rules/subsystem_boundaries.mdc:0:0-0:0)](../../..\\..\\.cursor\\rules\\subsystem_boundaries.mdc), [`file_modularity.mdc`](../../..\\..\\.cursor\\rules\\file_modularity.mdc))."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181648\\docs\\reference\\website\\DESIGN_GUIDE.md",
    "LineNumber": 152,
    "LineContent": "## 11. Content Strategy Notes (Ref: Task WCS-1)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\docs\\20250513_181713\\docs\\templates\\prompts\\strategic_analysis_prompt.md",
    "LineNumber": 67,
    "LineContent": "* **Technology & Innovation (including AI & Human-AI Collaboration):** Deep knowledge of tech trends, AI/LLM capabilities and limitations, practical applications, human-AI interaction patterns (ref: LLM/IDE Study), security/privacy/IP risks of AI-generated code, and opportunities."
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\_website_backup_20250520_011220\\website\\ROADMAP.md",
    "LineNumber": 174,
    "LineContent": "- [x] VIS-INT-002: Fix node movement on hover while preserving zoom/pan. (Ref: `SystemGraph.tsx`)"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\_website_backup_20250520_011220\\website\\scripts\\migration\\fix-components-structure.ps1",
    "LineNumber": 2,
    "LineContent": "# Following EGOS Conscious Modularity principles (Ref: MEMORY[user_global])"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\_website_backup_20250520_011220\\website\\scripts\\migration\\fix-website-structure.ps1",
    "LineNumber": 3,
    "LineContent": "# Ref: MEMORY[user_global], MEMORY[7e443f56...], MEMORY[d18c5768...]"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\legacy_root_backups\\_website_backup_20250520_011220\\website\\scripts\\migration\\migrate-en-content.ps1",
    "LineNumber": 2,
    "LineContent": "# Ref: KOIOS-CODE-001, CORUJA-MIGRATION-001"
  },
  {
    "File": "C:\\EGOS\\docs_egos\\zz_archive\\planning_records\\project_planning\\Work_2025-05-20_Docs_Optimization.md",
    "LineNumber": 71,
    "LineContent": "    *   Renamed `c:\\EGOS\\docs\\subsystems\\ATLAS\\ATL_description.md` to `c:\\EGOS\\docs\\subsystems\\ATLAS\\README.md`. Updated its frontmatter for clarity (Purpose: \"Systemic Cartography & Visualization\") and revised its overview section. (Ref: Step ID 1980s)"
  },
  {
    "File": "C:\\EGOS\\scripts\\cross_reference\\execute_inventory_scan.py",
    "LineNumber": 38,
    "LineContent": "    {\"query\": \"Ref:\", \"output_file\": \"grep_Ref_results.json\", \"case_insensitive\": True, \"category\": \"keyword\"},"
  },
  {
    "File": "C:\\EGOS\\scripts\\cross_reference\\file_reference_checker_optimized.py",
    "LineNumber": 358,
    "LineContent": "                                f.write(f\"  - **No references found - WARNING: File might be undocumented or orphaned.** (Ref: MEMORY[3bae44d3...])\\n\")"
  },
  {
    "File": "C:\\EGOS\\scripts\\cross_reference\\file_reference_report_20250519_132826.md",
    "LineNumber": 68,
    "LineContent": "  - **No references found - WARNING: File might be undocumented or orphaned.** (Ref: MEMORY[3bae44d3...])"
  },
  {
    "File": "C:\\EGOS\\scripts\\cross_reference\\purge_old_references.py",
    "LineNumber": 191,
    "LineContent": "    {\"name\": \"Ref\", \"pattern\": r\"Ref:\\s\", \"context_required\": False},"
  },
  {
    "File": "C:\\EGOS\\STRATEGIC_THINKING\\meta_prompts\\strategic_analysis_prompt_v2.0.md",
    "LineNumber": 13,
    "LineContent": "* **Technology & Innovation (including AI & Human-AI Collaboration):** Deep knowledge of tech trends, AI/LLM capabilities and limitations, practical applications, human-AI interaction patterns (ref: LLM/IDE Study), security/privacy/IP risks of AI-generated code, and opportunities."
  },
  {
    "File": "C:\\EGOS\\subsystems\\CORUJA\\core\\tool_registry.py",
    "LineNumber": 52,
    "LineContent": "        if not implementation_ref:"
  },
  {
    "File": "C:\\EGOS\\subsystems\\CORUJA\\docs\\ARCHITECTURE.md",
    "LineNumber": 225,
    "LineContent": "        implementation_ref: str"
  },
  {
    "File": "C:\\EGOS\\subsystems\\CORUJA\\schemas\\models.py",
    "LineNumber": 49,
    "LineContent": "    implementation_ref: str = Field(..., description=\"Reference to the tool's implementation (e.g., Python import path).\")"
  },
  {
    "File": "C:\\EGOS\\subsystems\\KOIOS\\schemas\\pdd_schema.py",
    "LineNumber": 97,
    "LineContent": "    bias_mitigation_ref: Optional[str] = Field("
  },
  {
    "File": "C:\\EGOS\\website\\DESIGN_GUIDE.md",
    "LineNumber": 94,
    "LineContent": "## 11. Content Strategy Notes (Ref: Task WCS-1)"
  },
  {
    "File": "C:\\EGOS\\website\\ROADMAP.md",
    "LineNumber": 174,
    "LineContent": "- [x] VIS-INT-002: Fix node movement on hover while preserving zoom/pan. (Ref: `SystemGraph.tsx`)"
  },
  {
    "File": "C:\\EGOS\\website\\scripts\\migration\\fix-components-structure.ps1",
    "LineNumber": 2,
    "LineContent": "# Following EGOS Conscious Modularity principles (Ref: MEMORY[user_global])"
  },
  {
    "File": "C:\\EGOS\\website\\scripts\\migration\\fix-website-structure.ps1",
    "LineNumber": 3,
    "LineContent": "# Ref: MEMORY[user_global], MEMORY[7e443f56...], MEMORY[d18c5768...]"
  },
  {
    "File": "C:\\EGOS\\website\\scripts\\migration\\migrate-en-content.ps1",
    "LineNumber": 2,
    "LineContent": "# Ref: KOIOS-CODE-001, CORUJA-MIGRATION-001"
  }
]